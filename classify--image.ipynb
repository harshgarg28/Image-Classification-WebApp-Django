{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "outputs": [],
      "metadata": {
        "id": "y5pu0JDUnE53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "oydttlPLcPcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                e_w = p.grad * scale\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and the `second_step`; see the documentation for more info.\")\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        p.grad.norm(p=2)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm"
      ],
      "outputs": [],
      "metadata": {
        "id": "StBSfEb1gw8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "lr = 0.1  #learning rate\n",
        "best_acc = 0\n",
        "num_epochs = 225"
      ],
      "outputs": [],
      "metadata": {
        "id": "M5XNf8N4cadH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "train_transforms = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ccdt50LTclIB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=0)\n",
        "\n",
        "test_data = torchvision.datasets.CIFAR100(root='./data', train = False, download=True, transform=test_transforms)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False, num_workers=0)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 37539810.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDGyH4sfdV6r",
        "outputId": "1cbceed8-f425-4a46-bc88-c12b732d55f3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "net = torchvision.models.resnet18(pretrained=True)\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs,100)\n",
        "\n",
        "net = net.to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 119MB/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "rpxirzUXeFi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf49be6a-c205-4cec-a9ce-a95ede730739"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "base_optimizer = torch.optim.SGD\n",
        "optimizer = SAM(net.parameters(), base_optimizer, rho=0.05, lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "fvH0d_96eFds"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    H = []\n",
        "    Y = []\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        Y.append(targets)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.mean().backward()\n",
        "        optimizer.first_step(zero_grad=True)\n",
        "\n",
        "        # second forward-backward step\n",
        "        criterion(net(inputs), targets).mean().backward()\n",
        "        optimizer.second_step(zero_grad=True)\n",
        "\n",
        "        train_loss += loss.mean().item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if(batch_idx % 50 == 0):\n",
        "              print(str(batch_idx)+\"/\"+str(len(train_loader)) +\"  Loss: \" + str(train_loss/(batch_idx+1)) +\"  Acc: \"+ str(100.*correct/total))\n",
        "\n",
        "\n",
        "        H.append(outputs.to('cpu'))\n",
        "\n",
        "    H = torch.cat(H,0).detach().numpy()\n",
        "    Y = torch.cat(Y,0).numpy()\n",
        "\n",
        "    return train_loss / len(train_loader), 100.*correct/total, H, Y\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "VLPRM3uGgKcW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    H = []\n",
        "    Y = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            Y.append(targets)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            # loss = smooth_crossentropy(outputs, targets)\n",
        "            loss = criterion(outputs, targets)\n",
        "            #loss = std_loss(outputs, targets)\n",
        "\n",
        "            test_loss += loss.mean().item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            if(batch_idx % 50 == 0):\n",
        "              print(str(batch_idx)+\"/\"+str(len(test_loader)) +\"  Loss: \" + str(test_loss/(batch_idx+1)) +\"  Acc: \"+ str(100.*correct/total))\n",
        "\n",
        "            H.append(outputs.to('cpu'))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "\n",
        "    H = torch.cat(H,0).detach().numpy()\n",
        "    Y = torch.cat(Y,0).numpy()\n",
        "\n",
        "    return test_loss / len(test_loader), 100.*correct/total, H, Y\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "9KOvqdZugWCR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "train_loss = []\n",
        "train_accuracy = []\n",
        "test_loss = []\n",
        "test_accuracy = []\n",
        "\n",
        "\n",
        "for epoch in range(0, num_epochs):\n",
        "    tr_loss, tr_acc, _, _ = train(epoch)\n",
        "    te_loss, te_acc, _, _ = test(epoch)\n",
        "\n",
        "    train_loss.append(tr_loss)\n",
        "    train_accuracy.append(tr_acc)\n",
        "    test_loss.append(te_loss)\n",
        "    test_accuracy.append(te_acc)\n",
        "    print(\"Train Accuracy:  \" , max(train_accuracy), \"% \", \"  Test Accuracy:  \" , max(test_accuracy), \"%\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "0/391  Loss: 4.899670124053955  Acc: 0.0\n",
            "50/391  Loss: 4.988009957706227  Acc: 2.2518382352941178\n",
            "100/391  Loss: 4.888335261014428  Acc: 2.4520420792079207\n",
            "150/391  Loss: 4.740847698110619  Acc: 2.8197433774834435\n",
            "200/391  Loss: 4.595420138752876  Acc: 3.626399253731343\n",
            "250/391  Loss: 4.48030491369179  Acc: 4.4135956175298805\n",
            "300/391  Loss: 4.38302342836247  Acc: 5.048276578073089\n",
            "350/391  Loss: 4.302273831136546  Acc: 5.669070512820513\n",
            "0/79  Loss: 3.6348345279693604  Acc: 13.28125\n",
            "50/79  Loss: 3.759814094094669  Acc: 10.814950980392156\n",
            "Saving..\n",
            "Train Accuracy:   6.112 %    Test Accuracy:   10.66 %\n",
            "\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/391  Loss: 3.7717888355255127  Acc: 7.8125\n",
            "50/391  Loss: 3.778241994334202  Acc: 10.217524509803921\n",
            "100/391  Loss: 3.7152663221453675  Acc: 11.270111386138614\n",
            "150/391  Loss: 3.6744755577567396  Acc: 11.863617549668874\n",
            "200/391  Loss: 3.641258246863066  Acc: 12.26290422885572\n",
            "250/391  Loss: 3.6096377885674102  Acc: 12.749003984063744\n",
            "300/391  Loss: 3.573472305785778  Acc: 13.44736295681063\n",
            "350/391  Loss: 3.5444630435389333  Acc: 13.913372507122507\n",
            "0/79  Loss: 3.14821195602417  Acc: 19.53125\n",
            "50/79  Loss: 3.292984780143289  Acc: 18.995098039215687\n",
            "Saving..\n",
            "Train Accuracy:   14.376 %    Test Accuracy:   18.9 %\n",
            "\n",
            "Epoch: 2\n",
            "0/391  Loss: 3.23058819770813  Acc: 21.875\n",
            "50/391  Loss: 3.2357575566160914  Acc: 20.159313725490197\n",
            "100/391  Loss: 3.327021889167257  Acc: 18.48700495049505\n",
            "150/391  Loss: 3.3651897165159514  Acc: 17.528973509933774\n",
            "200/391  Loss: 3.3440244162260595  Acc: 17.89101368159204\n",
            "250/391  Loss: 3.31732375118362  Acc: 18.236429282868524\n",
            "300/391  Loss: 3.2853705780054643  Acc: 18.78374169435216\n",
            "350/391  Loss: 3.254383182933188  Acc: 19.328703703703702\n",
            "0/79  Loss: 2.8471977710723877  Acc: 28.125\n",
            "50/79  Loss: 3.0698396551842784  Acc: 22.717524509803923\n",
            "Saving..\n",
            "Train Accuracy:   19.798 %    Test Accuracy:   22.82 %\n",
            "\n",
            "Epoch: 3\n",
            "0/391  Loss: 3.298265218734741  Acc: 19.53125\n",
            "50/391  Loss: 3.0125431546977923  Acc: 23.529411764705884\n",
            "100/391  Loss: 2.9703670822747865  Acc: 24.412128712871286\n",
            "150/391  Loss: 2.9559253588417507  Acc: 24.487789735099337\n",
            "200/391  Loss: 2.9342117855204872  Acc: 25.260416666666668\n",
            "250/391  Loss: 2.914496873004503  Acc: 25.56648406374502\n",
            "300/391  Loss: 2.9076495503270348  Acc: 25.708575581395348\n",
            "350/391  Loss: 2.9135215954902844  Acc: 25.65438034188034\n",
            "0/79  Loss: 2.6108734607696533  Acc: 33.59375\n",
            "50/79  Loss: 2.8252325899460735  Acc: 27.512254901960784\n",
            "Saving..\n",
            "Train Accuracy:   25.88 %    Test Accuracy:   27.33 %\n",
            "\n",
            "Epoch: 4\n",
            "0/391  Loss: 2.7520434856414795  Acc: 25.78125\n",
            "50/391  Loss: 2.783950445698757  Acc: 28.691789215686274\n",
            "100/391  Loss: 2.7500375001737387  Acc: 28.983601485148515\n",
            "150/391  Loss: 2.7570048831156546  Acc: 29.025248344370862\n",
            "200/391  Loss: 2.740577318182039  Acc: 29.45040422885572\n",
            "250/391  Loss: 2.731321797427903  Acc: 29.441608565737052\n",
            "300/391  Loss: 2.7292170271128913  Acc: 29.583679401993354\n",
            "350/391  Loss: 2.7219021510534476  Acc: 29.754273504273506\n",
            "0/79  Loss: 2.374940872192383  Acc: 34.375\n",
            "50/79  Loss: 2.4911568538815367  Acc: 34.83455882352941\n",
            "Saving..\n",
            "Train Accuracy:   29.842 %    Test Accuracy:   34.85 %\n",
            "\n",
            "Epoch: 5\n",
            "0/391  Loss: 2.4855575561523438  Acc: 33.59375\n",
            "50/391  Loss: 2.5984208770826753  Acc: 32.24571078431372\n",
            "100/391  Loss: 2.587151496717245  Acc: 32.804764851485146\n",
            "150/391  Loss: 2.593890782223632  Acc: 32.77628311258278\n",
            "200/391  Loss: 2.5898772721266865  Acc: 32.56374378109453\n",
            "250/391  Loss: 2.5829554601494533  Acc: 32.77514940239044\n",
            "300/391  Loss: 2.5788756224800187  Acc: 32.88517441860465\n",
            "350/391  Loss: 2.575094261740008  Acc: 32.84588675213675\n",
            "0/79  Loss: 2.355855941772461  Acc: 32.03125\n",
            "50/79  Loss: 2.4924532058192232  Acc: 35.2328431372549\n",
            "Saving..\n",
            "Train Accuracy:   32.828 %    Test Accuracy:   34.9 %\n",
            "\n",
            "Epoch: 6\n",
            "0/391  Loss: 2.4233906269073486  Acc: 42.1875\n",
            "50/391  Loss: 2.4590022096446917  Acc: 35.064338235294116\n",
            "100/391  Loss: 2.469875465525259  Acc: 35.00928217821782\n",
            "150/391  Loss: 2.486632175003456  Acc: 34.535389072847686\n",
            "200/391  Loss: 2.4878682141280293  Acc: 34.65873756218905\n",
            "250/391  Loss: 2.4875680203456803  Acc: 34.62089143426295\n",
            "300/391  Loss: 2.484270210677999  Acc: 34.63455149501661\n",
            "350/391  Loss: 2.4812631158747225  Acc: 34.8201566951567\n",
            "0/79  Loss: 2.264700412750244  Acc: 43.75\n",
            "50/79  Loss: 2.369744815078436  Acc: 36.87193627450981\n",
            "Saving..\n",
            "Train Accuracy:   34.858 %    Test Accuracy:   36.9 %\n",
            "\n",
            "Epoch: 7\n",
            "0/391  Loss: 2.6113550662994385  Acc: 28.90625\n",
            "50/391  Loss: 2.3863310580160104  Acc: 36.70343137254902\n",
            "100/391  Loss: 2.405081857549082  Acc: 36.32425742574257\n",
            "150/391  Loss: 2.4222309652543226  Acc: 36.082367549668874\n",
            "200/391  Loss: 2.414960721238929  Acc: 36.18625621890547\n",
            "250/391  Loss: 2.4168701836787374  Acc: 36.20517928286853\n",
            "300/391  Loss: 2.414973096594066  Acc: 36.37873754152824\n",
            "350/391  Loss: 2.409694182567107  Acc: 36.491720085470085\n",
            "0/79  Loss: 2.156951665878296  Acc: 47.65625\n",
            "50/79  Loss: 2.3558065143286013  Acc: 38.373161764705884\n",
            "Saving..\n",
            "Train Accuracy:   36.568 %    Test Accuracy:   37.81 %\n",
            "\n",
            "Epoch: 8\n",
            "0/391  Loss: 2.424879550933838  Acc: 33.59375\n",
            "50/391  Loss: 2.340053740669699  Acc: 37.607230392156865\n",
            "100/391  Loss: 2.34266995203377  Acc: 37.538675742574256\n",
            "150/391  Loss: 2.344475139845286  Acc: 37.80008278145695\n",
            "200/391  Loss: 2.3421711085447625  Acc: 37.82260572139303\n",
            "250/391  Loss: 2.3370355757109196  Acc: 37.8952938247012\n",
            "300/391  Loss: 2.33844060636438  Acc: 37.91268687707641\n",
            "350/391  Loss: 2.3377042411059734  Acc: 37.86502849002849\n",
            "0/79  Loss: 2.16780948638916  Acc: 42.1875\n",
            "50/79  Loss: 2.2459755458083808  Acc: 39.90502450980392\n",
            "Saving..\n",
            "Train Accuracy:   37.952 %    Test Accuracy:   39.87 %\n",
            "\n",
            "Epoch: 9\n",
            "0/391  Loss: 2.1667990684509277  Acc: 47.65625\n",
            "50/391  Loss: 2.2504039558709836  Acc: 39.78247549019608\n",
            "100/391  Loss: 2.2780585359818866  Acc: 39.74319306930693\n",
            "150/391  Loss: 2.2803808182280583  Acc: 39.66266556291391\n",
            "200/391  Loss: 2.2853077911025848  Acc: 39.66106965174129\n",
            "250/391  Loss: 2.2820491339580946  Acc: 39.653884462151396\n",
            "300/391  Loss: 2.283502100710061  Acc: 39.61015365448505\n",
            "350/391  Loss: 2.278701142028526  Acc: 39.62562321937322\n",
            "0/79  Loss: 2.1484792232513428  Acc: 44.53125\n",
            "50/79  Loss: 2.280516329933615  Acc: 39.705882352941174\n",
            "Train Accuracy:   39.606 %    Test Accuracy:   39.87 %\n",
            "\n",
            "Epoch: 10\n",
            "0/391  Loss: 2.287548303604126  Acc: 40.625\n",
            "50/391  Loss: 2.2040676065519746  Acc: 40.900735294117645\n",
            "100/391  Loss: 2.215306292666067  Acc: 41.08137376237624\n",
            "150/391  Loss: 2.2249099174082674  Acc: 40.67673841059602\n",
            "200/391  Loss: 2.2286027782592015  Acc: 40.636660447761194\n",
            "250/391  Loss: 2.223478105438658  Acc: 40.625\n",
            "300/391  Loss: 2.2288794172958677  Acc: 40.40957225913621\n",
            "350/391  Loss: 2.2353599794909487  Acc: 40.30893874643875\n",
            "0/79  Loss: 1.9166218042373657  Acc: 48.4375\n",
            "50/79  Loss: 2.2210741650824453  Acc: 40.79350490196079\n",
            "Saving..\n",
            "Train Accuracy:   40.164 %    Test Accuracy:   40.57 %\n",
            "\n",
            "Epoch: 11\n",
            "0/391  Loss: 2.2256195545196533  Acc: 39.84375\n",
            "50/391  Loss: 2.203679269435359  Acc: 40.79350490196079\n",
            "100/391  Loss: 2.1798217438235143  Acc: 41.9322400990099\n",
            "150/391  Loss: 2.1731924505423237  Acc: 41.85637417218543\n",
            "200/391  Loss: 2.177975877600523  Acc: 41.68610074626866\n",
            "250/391  Loss: 2.1717841174023085  Acc: 41.80776892430279\n",
            "300/391  Loss: 2.1758534706312163  Acc: 41.811150332225914\n",
            "350/391  Loss: 2.1784141990873547  Acc: 41.6355056980057\n",
            "0/79  Loss: 1.920033574104309  Acc: 48.4375\n",
            "50/79  Loss: 2.2026349516475903  Acc: 41.513480392156865\n",
            "Saving..\n",
            "Train Accuracy:   41.614 %    Test Accuracy:   41.52 %\n",
            "\n",
            "Epoch: 12\n",
            "0/391  Loss: 2.0486502647399902  Acc: 41.40625\n",
            "50/391  Loss: 2.12900069881888  Acc: 43.30575980392157\n",
            "100/391  Loss: 2.1397154697097176  Acc: 42.976485148514854\n",
            "150/391  Loss: 2.1365143873833663  Acc: 42.782491721854306\n",
            "200/391  Loss: 2.1360237414564067  Acc: 42.634483830845774\n",
            "250/391  Loss: 2.1376368581536282  Acc: 42.50186752988048\n",
            "300/391  Loss: 2.1404668488771814  Acc: 42.34582641196013\n",
            "350/391  Loss: 2.142975651640498  Acc: 42.19862891737892\n",
            "0/79  Loss: 2.0556507110595703  Acc: 46.09375\n",
            "50/79  Loss: 2.133186246834549  Acc: 42.8921568627451\n",
            "Saving..\n",
            "Train Accuracy:   42.16 %    Test Accuracy:   42.34 %\n",
            "\n",
            "Epoch: 13\n",
            "0/391  Loss: 1.8750416040420532  Acc: 49.21875\n",
            "50/391  Loss: 2.0821281578026567  Acc: 43.6734068627451\n",
            "100/391  Loss: 2.087807785166372  Acc: 43.40191831683168\n",
            "150/391  Loss: 2.0961995519549643  Acc: 43.030836092715234\n",
            "200/391  Loss: 2.10178063698669  Acc: 43.14365671641791\n",
            "250/391  Loss: 2.1068738076791345  Acc: 42.92517430278885\n",
            "300/391  Loss: 2.105887781345963  Acc: 43.01806478405316\n",
            "350/391  Loss: 2.110697409705898  Acc: 43.080039173789174\n",
            "0/79  Loss: 1.7981423139572144  Acc: 51.5625\n",
            "50/79  Loss: 2.120649260633132  Acc: 43.229166666666664\n",
            "Saving..\n",
            "Train Accuracy:   43.042 %    Test Accuracy:   42.66 %\n",
            "\n",
            "Epoch: 14\n",
            "0/391  Loss: 2.087947130203247  Acc: 44.53125\n",
            "50/391  Loss: 2.0646675497877833  Acc: 44.240196078431374\n",
            "100/391  Loss: 2.051321605644604  Acc: 44.647277227722775\n",
            "150/391  Loss: 2.062002782000611  Acc: 44.65024834437086\n",
            "200/391  Loss: 2.0620828979644017  Acc: 44.57400497512438\n",
            "250/391  Loss: 2.0788567051944504  Acc: 44.05191733067729\n",
            "300/391  Loss: 2.0836590647301403  Acc: 43.80969684385382\n",
            "350/391  Loss: 2.083515125122505  Acc: 43.830128205128204\n",
            "0/79  Loss: 1.9357210397720337  Acc: 47.65625\n",
            "50/79  Loss: 2.077882402083453  Acc: 44.34742647058823\n",
            "Saving..\n",
            "Train Accuracy:   43.74 %    Test Accuracy:   43.75 %\n",
            "\n",
            "Epoch: 15\n",
            "0/391  Loss: 2.103494167327881  Acc: 35.15625\n",
            "50/391  Loss: 2.027891091271943  Acc: 45.34313725490196\n",
            "100/391  Loss: 2.036107400856396  Acc: 45.26608910891089\n",
            "150/391  Loss: 2.0513178773273695  Acc: 44.64507450331126\n",
            "200/391  Loss: 2.0459651448833407  Acc: 44.632307213930346\n",
            "250/391  Loss: 2.0485923689200107  Acc: 44.51568725099602\n",
            "300/391  Loss: 2.052099491274634  Acc: 44.48193521594684\n",
            "350/391  Loss: 2.0634502009448843  Acc: 44.15286680911681\n",
            "0/79  Loss: 1.885550618171692  Acc: 52.34375\n",
            "50/79  Loss: 2.0585524591745115  Acc: 45.787377450980394\n",
            "Saving..\n",
            "Train Accuracy:   44.254 %    Test Accuracy:   45.09 %\n",
            "\n",
            "Epoch: 16\n",
            "0/391  Loss: 1.9828252792358398  Acc: 48.4375\n",
            "50/391  Loss: 1.9505156582477046  Acc: 46.30821078431372\n",
            "100/391  Loss: 1.9977749196609649  Acc: 45.81528465346535\n",
            "150/391  Loss: 2.0059998548583478  Acc: 45.60740894039735\n",
            "200/391  Loss: 2.0134640463549105  Acc: 45.42521766169154\n",
            "250/391  Loss: 2.0215655536765595  Acc: 45.23157370517928\n",
            "300/391  Loss: 2.0218510057443004  Acc: 45.26837624584718\n",
            "350/391  Loss: 2.022640808015807  Acc: 45.23682336182336\n",
            "0/79  Loss: 1.9423178434371948  Acc: 52.34375\n",
            "50/79  Loss: 2.1566978506013457  Acc: 42.61642156862745\n",
            "Train Accuracy:   45.064 %    Test Accuracy:   45.09 %\n",
            "\n",
            "Epoch: 17\n",
            "0/391  Loss: 1.802512288093567  Acc: 52.34375\n",
            "50/391  Loss: 1.9891927405899645  Acc: 46.30821078431372\n",
            "100/391  Loss: 1.9988297944021698  Acc: 45.598700495049506\n",
            "150/391  Loss: 1.9941019696115658  Acc: 45.57636589403973\n",
            "200/391  Loss: 1.9902886703832825  Acc: 45.806125621890544\n",
            "250/391  Loss: 1.997131859163839  Acc: 45.66110557768924\n",
            "300/391  Loss: 2.0034239874329676  Acc: 45.6421303986711\n",
            "350/391  Loss: 2.007034546968944  Acc: 45.559561965811966\n",
            "0/79  Loss: 1.8607515096664429  Acc: 50.78125\n",
            "50/79  Loss: 2.0749210984099147  Acc: 44.46997549019608\n",
            "Train Accuracy:   45.432 %    Test Accuracy:   45.09 %\n",
            "\n",
            "Epoch: 18\n",
            "0/391  Loss: 1.8763346672058105  Acc: 51.5625\n",
            "50/391  Loss: 1.8893547946331548  Acc: 48.63664215686274\n",
            "100/391  Loss: 1.9232566144206735  Acc: 47.772277227722775\n",
            "150/391  Loss: 1.9450135017862382  Acc: 47.04056291390729\n",
            "200/391  Loss: 1.9572209938248593  Acc: 46.727300995024876\n",
            "250/391  Loss: 1.9691124209369797  Acc: 46.467255976095615\n",
            "300/391  Loss: 1.9773188789817582  Acc: 46.29360465116279\n",
            "350/391  Loss: 1.9845602617644176  Acc: 46.067040598290596\n",
            "0/79  Loss: 1.928684115409851  Acc: 46.875\n",
            "50/79  Loss: 2.025411668945761  Acc: 45.37377450980392\n",
            "Saving..\n",
            "Train Accuracy:   45.908 %    Test Accuracy:   45.44 %\n",
            "\n",
            "Epoch: 19\n",
            "0/391  Loss: 2.1411330699920654  Acc: 38.28125\n",
            "50/391  Loss: 1.9492259212568694  Acc: 46.89031862745098\n",
            "100/391  Loss: 1.947133148070609  Acc: 47.17667079207921\n",
            "150/391  Loss: 1.9506539319524703  Acc: 47.09747516556291\n",
            "200/391  Loss: 1.965789517953028  Acc: 46.82058457711443\n",
            "250/391  Loss: 1.9675346779158391  Acc: 46.6820219123506\n",
            "300/391  Loss: 1.9696006652128657  Acc: 46.58170681063123\n",
            "350/391  Loss: 1.9733621870350635  Acc: 46.53445512820513\n",
            "0/79  Loss: 1.7514985799789429  Acc: 57.03125\n",
            "50/79  Loss: 2.0406539299908806  Acc: 45.48100490196079\n",
            "Train Accuracy:   46.446 %    Test Accuracy:   45.44 %\n",
            "\n",
            "Epoch: 20\n",
            "0/391  Loss: 1.9304757118225098  Acc: 46.875\n",
            "50/391  Loss: 1.9032249684427298  Acc: 46.9515931372549\n",
            "100/391  Loss: 1.9011315697490578  Acc: 47.60983910891089\n",
            "150/391  Loss: 1.8940400982534649  Acc: 47.95633278145695\n",
            "200/391  Loss: 1.9202098122876674  Acc: 47.19760572139303\n",
            "250/391  Loss: 1.9343386370822253  Acc: 47.008839641434264\n",
            "300/391  Loss: 1.941453566186848  Acc: 47.03073089700997\n",
            "350/391  Loss: 1.9447924978712685  Acc: 47.04415954415954\n",
            "0/79  Loss: 1.8012700080871582  Acc: 57.8125\n",
            "50/79  Loss: 2.0321217635098625  Acc: 45.2359068627451\n",
            "Train Accuracy:   46.778 %    Test Accuracy:   45.44 %\n",
            "\n",
            "Epoch: 21\n",
            "0/391  Loss: 1.8992891311645508  Acc: 49.21875\n",
            "50/391  Loss: 1.9060764826980292  Acc: 48.48345588235294\n",
            "100/391  Loss: 1.890736663695609  Acc: 48.53805693069307\n",
            "150/391  Loss: 1.9080606793725727  Acc: 47.86837748344371\n",
            "200/391  Loss: 1.9147859165324501  Acc: 47.60960820895522\n",
            "250/391  Loss: 1.9257341191113233  Acc: 47.447709163346616\n",
            "300/391  Loss: 1.9309428672853894  Acc: 47.339597176079735\n",
            "350/391  Loss: 1.9401726923097573  Acc: 47.16880341880342\n",
            "0/79  Loss: 1.9630342721939087  Acc: 48.4375\n",
            "50/79  Loss: 2.104424743091359  Acc: 44.025735294117645\n",
            "Train Accuracy:   47.014 %    Test Accuracy:   45.44 %\n",
            "\n",
            "Epoch: 22\n",
            "0/391  Loss: 1.8689944744110107  Acc: 46.875\n",
            "50/391  Loss: 1.8905872129926495  Acc: 47.411151960784316\n",
            "100/391  Loss: 1.8878421570995065  Acc: 47.71813118811881\n",
            "150/391  Loss: 1.901679141631979  Acc: 47.69246688741722\n",
            "200/391  Loss: 1.9068181692664303  Acc: 47.803949004975124\n",
            "250/391  Loss: 1.9166587101985735  Acc: 47.68115039840637\n",
            "300/391  Loss: 1.925146135776938  Acc: 47.51609219269103\n",
            "350/391  Loss: 1.9236818670886875  Acc: 47.524928774928775\n",
            "0/79  Loss: 1.8923643827438354  Acc: 53.125\n",
            "50/79  Loss: 2.04394836285535  Acc: 45.90992647058823\n",
            "Saving..\n",
            "Train Accuracy:   47.484 %    Test Accuracy:   45.99 %\n",
            "\n",
            "Epoch: 23\n",
            "0/391  Loss: 1.7886899709701538  Acc: 57.8125\n",
            "50/391  Loss: 1.8202308229371613  Acc: 49.693627450980394\n",
            "100/391  Loss: 1.8634053834594122  Acc: 49.087252475247524\n",
            "150/391  Loss: 1.8739243169494022  Acc: 48.72723509933775\n",
            "200/391  Loss: 1.8964165247494902  Acc: 48.11489427860697\n",
            "250/391  Loss: 1.9053977010734529  Acc: 47.864790836653384\n",
            "300/391  Loss: 1.9061996045698755  Acc: 47.93656561461794\n",
            "350/391  Loss: 1.9132176039904951  Acc: 47.72079772079772\n",
            "0/79  Loss: 1.8788506984710693  Acc: 50.78125\n",
            "50/79  Loss: 2.0022422870000205  Acc: 46.982230392156865\n",
            "Saving..\n",
            "Train Accuracy:   47.678 %    Test Accuracy:   46.76 %\n",
            "\n",
            "Epoch: 24\n",
            "0/391  Loss: 1.6752029657363892  Acc: 53.90625\n",
            "50/391  Loss: 1.8590164698806464  Acc: 49.356617647058826\n",
            "100/391  Loss: 1.8569196828521124  Acc: 49.56683168316832\n",
            "150/391  Loss: 1.8788335197019261  Acc: 48.94971026490066\n",
            "200/391  Loss: 1.8854436150830776  Acc: 48.643501243781095\n",
            "250/391  Loss: 1.8920536786911497  Acc: 48.51531374501992\n",
            "300/391  Loss: 1.8927009877968468  Acc: 48.40375830564784\n",
            "350/391  Loss: 1.903104736594393  Acc: 48.041310541310544\n",
            "0/79  Loss: 1.7534633874893188  Acc: 55.46875\n",
            "50/79  Loss: 1.9899524987912645  Acc: 46.614583333333336\n",
            "Train Accuracy:   47.85 %    Test Accuracy:   46.76 %\n",
            "\n",
            "Epoch: 25\n",
            "0/391  Loss: 1.6013211011886597  Acc: 52.34375\n",
            "50/391  Loss: 1.8145969802258062  Acc: 49.908088235294116\n",
            "100/391  Loss: 1.8499055935604738  Acc: 49.040841584158414\n",
            "150/391  Loss: 1.8547289253070653  Acc: 48.74275662251656\n",
            "200/391  Loss: 1.8636350305519294  Acc: 48.74455845771144\n",
            "250/391  Loss: 1.86892892354988  Acc: 48.70517928286853\n",
            "300/391  Loss: 1.8782135549177759  Acc: 48.51536544850498\n",
            "350/391  Loss: 1.8786017127186485  Acc: 48.506499287749286\n",
            "0/79  Loss: 1.7090853452682495  Acc: 55.46875\n",
            "50/79  Loss: 1.944354160159242  Acc: 47.135416666666664\n",
            "Saving..\n",
            "Train Accuracy:   48.308 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 26\n",
            "0/391  Loss: 1.7784106731414795  Acc: 53.90625\n",
            "50/391  Loss: 1.8221240277383841  Acc: 49.908088235294116\n",
            "100/391  Loss: 1.8290240257093222  Acc: 49.67512376237624\n",
            "150/391  Loss: 1.840663712545736  Acc: 49.65335264900662\n",
            "200/391  Loss: 1.8574800627741648  Acc: 49.20320273631841\n",
            "250/391  Loss: 1.860847928609506  Acc: 49.00398406374502\n",
            "300/391  Loss: 1.872278000033179  Acc: 48.72560215946844\n",
            "350/391  Loss: 1.874833044163522  Acc: 48.697916666666664\n",
            "0/79  Loss: 1.7960903644561768  Acc: 50.0\n",
            "50/79  Loss: 1.9479993020786959  Acc: 47.30392156862745\n",
            "Train Accuracy:   48.522 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 27\n",
            "0/391  Loss: 2.039684772491455  Acc: 43.75\n",
            "50/391  Loss: 1.8144144427542592  Acc: 50.16850490196079\n",
            "100/391  Loss: 1.8375921615279547  Acc: 49.620977722772274\n",
            "150/391  Loss: 1.8513712069846147  Acc: 49.33257450331126\n",
            "200/391  Loss: 1.8552363577173716  Acc: 49.20320273631841\n",
            "250/391  Loss: 1.8603898946982456  Acc: 49.16272410358566\n",
            "300/391  Loss: 1.8678595702909553  Acc: 48.91507475083056\n",
            "350/391  Loss: 1.867733450696679  Acc: 48.980591168091166\n",
            "0/79  Loss: 1.7813758850097656  Acc: 52.34375\n",
            "50/79  Loss: 1.9734826415192848  Acc: 46.997549019607845\n",
            "Train Accuracy:   48.834 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 28\n",
            "0/391  Loss: 1.7017978429794312  Acc: 53.90625\n",
            "50/391  Loss: 1.7911732921413346  Acc: 50.122549019607845\n",
            "100/391  Loss: 1.8141112398393084  Acc: 49.55909653465346\n",
            "150/391  Loss: 1.8223739586129093  Acc: 49.62748344370861\n",
            "200/391  Loss: 1.8353800044130923  Acc: 49.38976990049751\n",
            "250/391  Loss: 1.833322992362824  Acc: 49.35881474103586\n",
            "300/391  Loss: 1.8405704205218343  Acc: 49.10973837209303\n",
            "350/391  Loss: 1.8459606404997344  Acc: 49.12749287749288\n",
            "0/79  Loss: 1.9347926378250122  Acc: 50.0\n",
            "50/79  Loss: 2.0410079395069793  Acc: 44.96017156862745\n",
            "Train Accuracy:   48.952 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 29\n",
            "0/391  Loss: 1.6506925821304321  Acc: 53.90625\n",
            "50/391  Loss: 1.7707819588044111  Acc: 51.36335784313726\n",
            "100/391  Loss: 1.7923402927889682  Acc: 50.73483910891089\n",
            "150/391  Loss: 1.8061055394987398  Acc: 50.28973509933775\n",
            "200/391  Loss: 1.8275540082608883  Acc: 49.762904228855724\n",
            "250/391  Loss: 1.8460725379655085  Acc: 49.324576693227094\n",
            "300/391  Loss: 1.8546050549345554  Acc: 49.18760382059801\n",
            "350/391  Loss: 1.856001396804114  Acc: 49.15642806267806\n",
            "0/79  Loss: 1.8825210332870483  Acc: 47.65625\n",
            "50/79  Loss: 2.045799909853468  Acc: 46.44607843137255\n",
            "Train Accuracy:   49.15 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 30\n",
            "0/391  Loss: 1.8728744983673096  Acc: 48.4375\n",
            "50/391  Loss: 1.7709419142966176  Acc: 50.75061274509804\n",
            "100/391  Loss: 1.7966280160564008  Acc: 50.742574257425744\n",
            "150/391  Loss: 1.7965470291920844  Acc: 50.713990066225165\n",
            "200/391  Loss: 1.7951864075304858  Acc: 50.72294776119403\n",
            "250/391  Loss: 1.8142068424072872  Acc: 50.227216135458164\n",
            "300/391  Loss: 1.8254283579481005  Acc: 49.98961794019934\n",
            "350/391  Loss: 1.8298297203504121  Acc: 49.835292022792025\n",
            "0/79  Loss: 1.9074302911758423  Acc: 54.6875\n",
            "50/79  Loss: 1.9916276324029063  Acc: 46.12438725490196\n",
            "Train Accuracy:   49.716 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 31\n",
            "0/391  Loss: 1.8046361207962036  Acc: 49.21875\n",
            "50/391  Loss: 1.779155686789868  Acc: 50.82720588235294\n",
            "100/391  Loss: 1.786592691251547  Acc: 50.17017326732673\n",
            "150/391  Loss: 1.7987817502179682  Acc: 49.896523178807946\n",
            "200/391  Loss: 1.8036584367799522  Acc: 49.98056592039801\n",
            "250/391  Loss: 1.8114216650624675  Acc: 49.9968874501992\n",
            "300/391  Loss: 1.8173750904311374  Acc: 49.91694352159468\n",
            "350/391  Loss: 1.8186836602681042  Acc: 49.91096866096866\n",
            "0/79  Loss: 1.8073009252548218  Acc: 53.125\n",
            "50/79  Loss: 2.010421596321405  Acc: 46.216299019607845\n",
            "Train Accuracy:   49.788 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 32\n",
            "0/391  Loss: 1.633040428161621  Acc: 53.125\n",
            "50/391  Loss: 1.7488287687301636  Acc: 52.1139705882353\n",
            "100/391  Loss: 1.7667402347715775  Acc: 51.46194306930693\n",
            "150/391  Loss: 1.7733524437771728  Acc: 51.38141556291391\n",
            "200/391  Loss: 1.778482628698966  Acc: 51.29042288557214\n",
            "250/391  Loss: 1.788720722217484  Acc: 51.008466135458164\n",
            "300/391  Loss: 1.8065848635676691  Acc: 50.51391196013289\n",
            "350/391  Loss: 1.8097328939329185  Acc: 50.46741452991453\n",
            "0/79  Loss: 1.9523552656173706  Acc: 47.65625\n",
            "50/79  Loss: 2.0743205266840317  Acc: 44.85294117647059\n",
            "Train Accuracy:   50.232 %    Test Accuracy:   47.11 %\n",
            "\n",
            "Epoch: 33\n",
            "0/391  Loss: 1.3970892429351807  Acc: 60.9375\n",
            "50/391  Loss: 1.691741380037046  Acc: 52.818627450980394\n",
            "100/391  Loss: 1.7361401461138584  Acc: 51.895111386138616\n",
            "150/391  Loss: 1.7567023954644108  Acc: 51.453849337748345\n",
            "200/391  Loss: 1.7642890090373025  Acc: 51.23989427860697\n",
            "250/391  Loss: 1.7855545728805056  Acc: 50.73456175298805\n",
            "300/391  Loss: 1.7951496221694438  Acc: 50.4516196013289\n",
            "350/391  Loss: 1.8074202442440892  Acc: 50.193643162393165\n",
            "0/79  Loss: 1.7770071029663086  Acc: 54.6875\n",
            "50/79  Loss: 1.8802183445762186  Acc: 49.509803921568626\n",
            "Saving..\n",
            "Train Accuracy:   50.232 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 34\n",
            "0/391  Loss: 1.5715255737304688  Acc: 56.25\n",
            "50/391  Loss: 1.7030308994592405  Acc: 52.54289215686274\n",
            "100/391  Loss: 1.7555623526620392  Acc: 51.5779702970297\n",
            "150/391  Loss: 1.775069252544681  Acc: 51.07098509933775\n",
            "200/391  Loss: 1.7696317997737903  Acc: 51.11940298507463\n",
            "250/391  Loss: 1.785874306443203  Acc: 50.73144920318725\n",
            "300/391  Loss: 1.7894307374954224  Acc: 50.59956395348837\n",
            "350/391  Loss: 1.7923139129948413  Acc: 50.56980056980057\n",
            "0/79  Loss: 1.7091352939605713  Acc: 55.46875\n",
            "50/79  Loss: 1.9498872616711784  Acc: 48.544730392156865\n",
            "Train Accuracy:   50.43 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 35\n",
            "0/391  Loss: 1.5324875116348267  Acc: 54.6875\n",
            "50/391  Loss: 1.6832725253759646  Acc: 52.91053921568628\n",
            "100/391  Loss: 1.7109676198203965  Acc: 52.506188118811885\n",
            "150/391  Loss: 1.7486574389287177  Acc: 51.671150662251655\n",
            "200/391  Loss: 1.7508842945098877  Acc: 51.50808457711443\n",
            "250/391  Loss: 1.7683012514000396  Acc: 51.1765438247012\n",
            "300/391  Loss: 1.7771741254781173  Acc: 50.97850913621262\n",
            "350/391  Loss: 1.7890306938747396  Acc: 50.62099358974359\n",
            "0/79  Loss: 1.8422333002090454  Acc: 48.4375\n",
            "50/79  Loss: 2.008578515520283  Acc: 46.93627450980392\n",
            "Train Accuracy:   50.522 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 36\n",
            "0/391  Loss: 1.6868836879730225  Acc: 59.375\n",
            "50/391  Loss: 1.691849762318181  Acc: 53.07904411764706\n",
            "100/391  Loss: 1.7220937757208796  Acc: 52.274133663366335\n",
            "150/391  Loss: 1.7218418784488905  Acc: 52.03331953642384\n",
            "200/391  Loss: 1.7447548185414936  Acc: 51.59748134328358\n",
            "250/391  Loss: 1.7576612452586808  Acc: 51.40687250996016\n",
            "300/391  Loss: 1.7637459209987096  Acc: 51.29516196013289\n",
            "350/391  Loss: 1.7806217955727863  Acc: 50.90366809116809\n",
            "0/79  Loss: 1.756497859954834  Acc: 53.125\n",
            "50/79  Loss: 1.9441153278537826  Acc: 47.61029411764706\n",
            "Train Accuracy:   50.726 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 37\n",
            "0/391  Loss: 1.5907762050628662  Acc: 57.8125\n",
            "50/391  Loss: 1.7071550149543613  Acc: 52.80330882352941\n",
            "100/391  Loss: 1.7193522075615306  Acc: 52.57580445544554\n",
            "150/391  Loss: 1.7344902722251336  Acc: 52.41618377483444\n",
            "200/391  Loss: 1.7491044345779798  Acc: 51.958955223880594\n",
            "250/391  Loss: 1.7602353589943205  Acc: 51.50647410358566\n",
            "300/391  Loss: 1.772910237708361  Acc: 51.152408637873755\n",
            "350/391  Loss: 1.7760791581580442  Acc: 51.12847222222222\n",
            "0/79  Loss: 1.5684211254119873  Acc: 53.90625\n",
            "50/79  Loss: 1.9781447368509628  Acc: 46.966911764705884\n",
            "Train Accuracy:   50.99 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 38\n",
            "0/391  Loss: 1.7491170167922974  Acc: 48.4375\n",
            "50/391  Loss: 1.6973030052933038  Acc: 52.54289215686274\n",
            "100/391  Loss: 1.7034237077920744  Acc: 52.60674504950495\n",
            "150/391  Loss: 1.7332296110936347  Acc: 51.90397350993378\n",
            "200/391  Loss: 1.7474526166915894  Acc: 51.632462686567166\n",
            "250/391  Loss: 1.7558134438032174  Acc: 51.48468625498008\n",
            "300/391  Loss: 1.7628030163109105  Acc: 51.30813953488372\n",
            "350/391  Loss: 1.7709890053822444  Acc: 51.09953703703704\n",
            "0/79  Loss: 1.7190561294555664  Acc: 56.25\n",
            "50/79  Loss: 1.9073363706177355  Acc: 48.5140931372549\n",
            "Train Accuracy:   51.066 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 39\n",
            "0/391  Loss: 1.8352571725845337  Acc: 50.0\n",
            "50/391  Loss: 1.6927716100917143  Acc: 53.232230392156865\n",
            "100/391  Loss: 1.7078953119787839  Acc: 53.07858910891089\n",
            "150/391  Loss: 1.7112827672074173  Acc: 53.01117549668874\n",
            "200/391  Loss: 1.7332464569243626  Acc: 52.51476990049751\n",
            "250/391  Loss: 1.7458098955838328  Acc: 52.0480577689243\n",
            "300/391  Loss: 1.7494310219818572  Acc: 52.006333056478404\n",
            "350/391  Loss: 1.7535435603215144  Acc: 51.88746438746439\n",
            "0/79  Loss: 1.7386029958724976  Acc: 55.46875\n",
            "50/79  Loss: 1.9013590251698214  Acc: 48.697916666666664\n",
            "Train Accuracy:   51.698 %    Test Accuracy:   48.86 %\n",
            "\n",
            "Epoch: 40\n",
            "0/391  Loss: 1.717482566833496  Acc: 53.90625\n",
            "50/391  Loss: 1.70170497193056  Acc: 53.2015931372549\n",
            "100/391  Loss: 1.713489298773284  Acc: 52.82332920792079\n",
            "150/391  Loss: 1.7102730329463025  Acc: 52.799048013245034\n",
            "200/391  Loss: 1.7270699846210764  Acc: 52.347636815920396\n",
            "250/391  Loss: 1.7322936352505627  Acc: 52.15699701195219\n",
            "300/391  Loss: 1.74418129952643  Acc: 51.93106312292359\n",
            "350/391  Loss: 1.7485835562404404  Acc: 51.79843304843305\n",
            "0/79  Loss: 1.6721802949905396  Acc: 52.34375\n",
            "50/79  Loss: 1.9102264221976786  Acc: 49.03492647058823\n",
            "Saving..\n",
            "Train Accuracy:   51.706 %    Test Accuracy:   48.92 %\n",
            "\n",
            "Epoch: 41\n",
            "0/391  Loss: 1.7260075807571411  Acc: 53.90625\n",
            "50/391  Loss: 1.6716972402497834  Acc: 53.30882352941177\n",
            "100/391  Loss: 1.6761283933526219  Acc: 53.07858910891089\n",
            "150/391  Loss: 1.6870940963164072  Acc: 52.86113410596027\n",
            "200/391  Loss: 1.705270390605452  Acc: 52.55363805970149\n",
            "250/391  Loss: 1.7080345642994124  Acc: 52.542953187250994\n",
            "300/391  Loss: 1.7246333439880828  Acc: 52.22695182724252\n",
            "350/391  Loss: 1.7340920932612187  Acc: 52.04326923076923\n",
            "0/79  Loss: 1.7468771934509277  Acc: 54.6875\n",
            "50/79  Loss: 1.9821588899575027  Acc: 46.81372549019608\n",
            "Train Accuracy:   51.846 %    Test Accuracy:   48.92 %\n",
            "\n",
            "Epoch: 42\n",
            "0/391  Loss: 1.7033588886260986  Acc: 56.25\n",
            "50/391  Loss: 1.6753371159235637  Acc: 53.75306372549019\n",
            "100/391  Loss: 1.6969165388900456  Acc: 52.87747524752475\n",
            "150/391  Loss: 1.6991312598550556  Acc: 52.85596026490066\n",
            "200/391  Loss: 1.7114705120153095  Acc: 52.49922263681592\n",
            "250/391  Loss: 1.7230215979762287  Acc: 52.26593625498008\n",
            "300/391  Loss: 1.7309689018813479  Acc: 52.01931063122924\n",
            "350/391  Loss: 1.735297709109097  Acc: 52.02546296296296\n",
            "0/79  Loss: 1.7556836605072021  Acc: 53.125\n",
            "50/79  Loss: 1.9549130364960314  Acc: 47.45710784313726\n",
            "Train Accuracy:   51.904 %    Test Accuracy:   48.92 %\n",
            "\n",
            "Epoch: 43\n",
            "0/391  Loss: 1.648884654045105  Acc: 57.8125\n",
            "50/391  Loss: 1.6931800187802781  Acc: 53.232230392156865\n",
            "100/391  Loss: 1.6879854615372005  Acc: 52.98576732673267\n",
            "150/391  Loss: 1.7013039667874772  Acc: 52.63348509933775\n",
            "200/391  Loss: 1.7143554800185399  Acc: 52.3515236318408\n",
            "250/391  Loss: 1.7108637445951362  Acc: 52.43401394422311\n",
            "300/391  Loss: 1.7192152429656729  Acc: 52.252906976744185\n",
            "350/391  Loss: 1.7230046783757007  Acc: 52.20575142450142\n",
            "0/79  Loss: 1.627365231513977  Acc: 57.03125\n",
            "50/79  Loss: 1.8747137878455369  Acc: 48.912377450980394\n",
            "Train Accuracy:   52.088 %    Test Accuracy:   48.92 %\n",
            "\n",
            "Epoch: 44\n",
            "0/391  Loss: 1.376471757888794  Acc: 57.8125\n",
            "50/391  Loss: 1.6478564131493663  Acc: 53.75306372549019\n",
            "100/391  Loss: 1.6729731996460717  Acc: 53.20235148514851\n",
            "150/391  Loss: 1.6936639776293017  Acc: 52.80939569536424\n",
            "200/391  Loss: 1.6995380642402231  Acc: 52.895677860696516\n",
            "250/391  Loss: 1.7112869057522353  Acc: 52.524277888446214\n",
            "300/391  Loss: 1.7138921664006685  Acc: 52.4968853820598\n",
            "350/391  Loss: 1.7191621696847117  Acc: 52.39494301994302\n",
            "0/79  Loss: 1.6503660678863525  Acc: 57.8125\n",
            "50/79  Loss: 1.937319933199415  Acc: 47.962622549019606\n",
            "Train Accuracy:   52.254 %    Test Accuracy:   48.92 %\n",
            "\n",
            "Epoch: 45\n",
            "0/391  Loss: 1.61491060256958  Acc: 51.5625\n",
            "50/391  Loss: 1.6847254341723872  Acc: 52.51225490196079\n",
            "100/391  Loss: 1.6506227797800952  Acc: 53.681930693069305\n",
            "150/391  Loss: 1.665732318991857  Acc: 53.43543046357616\n",
            "200/391  Loss: 1.6767132673690568  Acc: 53.26881218905473\n",
            "250/391  Loss: 1.681108309453227  Acc: 53.05341135458168\n",
            "300/391  Loss: 1.6975694395775018  Acc: 52.62406561461794\n",
            "350/391  Loss: 1.7098216986724115  Acc: 52.39494301994302\n",
            "0/79  Loss: 1.8375076055526733  Acc: 50.0\n",
            "50/79  Loss: 2.005089184817146  Acc: 46.15502450980392\n",
            "Train Accuracy:   52.282 %    Test Accuracy:   48.92 %\n",
            "\n",
            "Epoch: 46\n",
            "0/391  Loss: 1.6142555475234985  Acc: 54.6875\n",
            "50/391  Loss: 1.6349468441570507  Acc: 54.564950980392155\n",
            "100/391  Loss: 1.6529929460865436  Acc: 54.04548267326733\n",
            "150/391  Loss: 1.6588820755876452  Acc: 53.8959023178808\n",
            "200/391  Loss: 1.6760225278228076  Acc: 53.41651119402985\n",
            "250/391  Loss: 1.687444889687922  Acc: 52.86665836653386\n",
            "300/391  Loss: 1.6884412674412774  Acc: 52.945909468438536\n",
            "350/391  Loss: 1.6992682911391952  Acc: 52.731036324786324\n",
            "0/79  Loss: 1.725788950920105  Acc: 59.375\n",
            "50/79  Loss: 1.8844488017699297  Acc: 50.19914215686274\n",
            "Saving..\n",
            "Train Accuracy:   52.678 %    Test Accuracy:   49.64 %\n",
            "\n",
            "Epoch: 47\n",
            "0/391  Loss: 1.5174126625061035  Acc: 57.03125\n",
            "50/391  Loss: 1.6447307292152853  Acc: 54.1360294117647\n",
            "100/391  Loss: 1.6530376885196951  Acc: 54.38582920792079\n",
            "150/391  Loss: 1.6611553785816724  Acc: 53.82864238410596\n",
            "200/391  Loss: 1.6633738344581566  Acc: 53.743003731343286\n",
            "250/391  Loss: 1.6786746997757263  Acc: 53.34910358565737\n",
            "300/391  Loss: 1.685337323683045  Acc: 53.35080980066445\n",
            "350/391  Loss: 1.6882011605124188  Acc: 53.300836894586894\n",
            "0/79  Loss: 1.6059479713439941  Acc: 56.25\n",
            "50/79  Loss: 1.908548112009086  Acc: 48.62132352941177\n",
            "Train Accuracy:   53.098 %    Test Accuracy:   49.64 %\n",
            "\n",
            "Epoch: 48\n",
            "0/391  Loss: 1.6500056982040405  Acc: 53.90625\n",
            "50/391  Loss: 1.6024320242451686  Acc: 55.08578431372549\n",
            "100/391  Loss: 1.6455783041396943  Acc: 54.18471534653465\n",
            "150/391  Loss: 1.6496554344695136  Acc: 53.99420529801324\n",
            "200/391  Loss: 1.659675608226909  Acc: 53.758550995024876\n",
            "250/391  Loss: 1.6676544972149974  Acc: 53.57320717131474\n",
            "300/391  Loss: 1.6743157500048413  Acc: 53.45203488372093\n",
            "350/391  Loss: 1.682072176213278  Acc: 53.24964387464387\n",
            "0/79  Loss: 1.922351360321045  Acc: 50.78125\n",
            "50/79  Loss: 1.9270887234631706  Acc: 48.682598039215684\n",
            "Train Accuracy:   53.11 %    Test Accuracy:   49.64 %\n",
            "\n",
            "Epoch: 49\n",
            "0/391  Loss: 1.7500097751617432  Acc: 51.5625\n",
            "50/391  Loss: 1.6183829751669192  Acc: 54.22794117647059\n",
            "100/391  Loss: 1.6118272436727392  Acc: 54.7029702970297\n",
            "150/391  Loss: 1.6342628539003283  Acc: 54.20633278145695\n",
            "200/391  Loss: 1.6470294473183096  Acc: 53.94123134328358\n",
            "250/391  Loss: 1.6622897073092213  Acc: 53.498505976095615\n",
            "300/391  Loss: 1.6658913004041906  Acc: 53.42607973421927\n",
            "350/391  Loss: 1.6746612573281312  Acc: 53.24519230769231\n",
            "0/79  Loss: 1.7619848251342773  Acc: 48.4375\n",
            "50/79  Loss: 1.9797107916252286  Acc: 46.997549019607845\n",
            "Train Accuracy:   53.11 %    Test Accuracy:   49.64 %\n",
            "\n",
            "Epoch: 50\n",
            "0/391  Loss: 1.6909679174423218  Acc: 53.125\n",
            "50/391  Loss: 1.605715929293165  Acc: 55.13174019607843\n",
            "100/391  Loss: 1.6244417783057337  Acc: 54.818997524752476\n",
            "150/391  Loss: 1.6296914650114955  Acc: 54.56332781456954\n",
            "200/391  Loss: 1.6486961924614598  Acc: 54.05006218905473\n",
            "250/391  Loss: 1.6605954892131911  Acc: 53.78797310756972\n",
            "300/391  Loss: 1.6641227296024461  Acc: 53.65188953488372\n",
            "350/391  Loss: 1.6686516419435158  Acc: 53.51451210826211\n",
            "0/79  Loss: 1.685342788696289  Acc: 53.90625\n",
            "50/79  Loss: 1.8472709422018014  Acc: 49.40257352941177\n",
            "Train Accuracy:   53.382 %    Test Accuracy:   49.64 %\n",
            "\n",
            "Epoch: 51\n",
            "0/391  Loss: 1.6796926259994507  Acc: 51.5625\n",
            "50/391  Loss: 1.5754136711943383  Acc: 55.88235294117647\n",
            "100/391  Loss: 1.6123793373013486  Acc: 54.7029702970297\n",
            "150/391  Loss: 1.6201079318065517  Acc: 54.527110927152314\n",
            "200/391  Loss: 1.633070553120096  Acc: 54.25217661691542\n",
            "250/391  Loss: 1.6417393342432274  Acc: 54.13657868525896\n",
            "300/391  Loss: 1.6525066500882373  Acc: 53.89067691029901\n",
            "350/391  Loss: 1.6568196645149817  Acc: 53.75712250712251\n",
            "0/79  Loss: 1.7493599653244019  Acc: 54.6875\n",
            "50/79  Loss: 1.8762162012212418  Acc: 49.708946078431374\n",
            "Saving..\n",
            "Train Accuracy:   53.656 %    Test Accuracy:   49.98 %\n",
            "\n",
            "Epoch: 52\n",
            "0/391  Loss: 1.756491780281067  Acc: 46.875\n",
            "50/391  Loss: 1.5962092502444398  Acc: 55.376838235294116\n",
            "100/391  Loss: 1.5991791510345912  Acc: 55.020111386138616\n",
            "150/391  Loss: 1.6069197978404974  Acc: 54.94619205298013\n",
            "200/391  Loss: 1.6091696777153963  Acc: 54.975124378109456\n",
            "250/391  Loss: 1.6213197337678704  Acc: 54.7871015936255\n",
            "300/391  Loss: 1.6394356065414277  Acc: 54.37863372093023\n",
            "350/391  Loss: 1.649787998607016  Acc: 54.28685897435897\n",
            "0/79  Loss: 1.5992380380630493  Acc: 58.59375\n",
            "50/79  Loss: 1.857201480397991  Acc: 50.091911764705884\n",
            "Train Accuracy:   54.232 %    Test Accuracy:   49.98 %\n",
            "\n",
            "Epoch: 53\n",
            "0/391  Loss: 1.507691740989685  Acc: 55.46875\n",
            "50/391  Loss: 1.5557614144156962  Acc: 55.759803921568626\n",
            "100/391  Loss: 1.565990387803257  Acc: 55.507425742574256\n",
            "150/391  Loss: 1.5942106160107037  Acc: 54.97723509933775\n",
            "200/391  Loss: 1.6044992949832138  Acc: 54.83131218905473\n",
            "250/391  Loss: 1.607256492295588  Acc: 54.83690239043825\n",
            "300/391  Loss: 1.6278233219222769  Acc: 54.42016196013289\n",
            "350/391  Loss: 1.639399267669417  Acc: 54.070957977207975\n",
            "0/79  Loss: 1.6529314517974854  Acc: 60.15625\n",
            "50/79  Loss: 1.9526192136839324  Acc: 48.23835784313726\n",
            "Train Accuracy:   54.232 %    Test Accuracy:   49.98 %\n",
            "\n",
            "Epoch: 54\n",
            "0/391  Loss: 1.5635682344436646  Acc: 59.375\n",
            "50/391  Loss: 1.599601988698922  Acc: 54.794730392156865\n",
            "100/391  Loss: 1.598047661309195  Acc: 54.911819306930695\n",
            "150/391  Loss: 1.6006808604625677  Acc: 54.99275662251656\n",
            "200/391  Loss: 1.615832123590346  Acc: 54.788557213930346\n",
            "250/391  Loss: 1.6289963465762802  Acc: 54.45405876494024\n",
            "300/391  Loss: 1.6337435629676742  Acc: 54.30076827242525\n",
            "350/391  Loss: 1.637875025428598  Acc: 54.17779558404558\n",
            "0/79  Loss: 1.5983737707138062  Acc: 57.8125\n",
            "50/79  Loss: 1.8929586457271201  Acc: 49.05024509803921\n",
            "Train Accuracy:   54.232 %    Test Accuracy:   49.98 %\n",
            "\n",
            "Epoch: 55\n",
            "0/391  Loss: 1.5612514019012451  Acc: 56.25\n",
            "50/391  Loss: 1.537751183790319  Acc: 56.60232843137255\n",
            "100/391  Loss: 1.5515326981497284  Acc: 56.59808168316832\n",
            "150/391  Loss: 1.583048506288339  Acc: 55.52566225165563\n",
            "200/391  Loss: 1.591820993233676  Acc: 55.441542288557216\n",
            "250/391  Loss: 1.6097646913680423  Acc: 55.032993027888445\n",
            "300/391  Loss: 1.6166175865255716  Acc: 54.892545681063126\n",
            "350/391  Loss: 1.6250152149771013  Acc: 54.66301638176638\n",
            "0/79  Loss: 1.546971082687378  Acc: 57.03125\n",
            "50/79  Loss: 1.8493098488041  Acc: 49.908088235294116\n",
            "Train Accuracy:   54.452 %    Test Accuracy:   49.98 %\n",
            "\n",
            "Epoch: 56\n",
            "0/391  Loss: 1.4524867534637451  Acc: 61.71875\n",
            "50/391  Loss: 1.5406392134872138  Acc: 56.46446078431372\n",
            "100/391  Loss: 1.5692918300628662  Acc: 55.84003712871287\n",
            "150/391  Loss: 1.5834125998793849  Acc: 55.58774834437086\n",
            "200/391  Loss: 1.5898009331072147  Acc: 55.554259950248756\n",
            "250/391  Loss: 1.5940050228658424  Acc: 55.23219621513944\n",
            "300/391  Loss: 1.6038618067966348  Acc: 54.98338870431893\n",
            "350/391  Loss: 1.6123095553824704  Acc: 54.80991809116809\n",
            "0/79  Loss: 1.773757815361023  Acc: 57.03125\n",
            "50/79  Loss: 1.8324488050797407  Acc: 51.087622549019606\n",
            "Saving..\n",
            "Train Accuracy:   54.716 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 57\n",
            "0/391  Loss: 1.404405951499939  Acc: 67.96875\n",
            "50/391  Loss: 1.5265183916278915  Acc: 56.908700980392155\n",
            "100/391  Loss: 1.5554016946565987  Acc: 56.28094059405941\n",
            "150/391  Loss: 1.5720515132739843  Acc: 55.62396523178808\n",
            "200/391  Loss: 1.5791424702649093  Acc: 55.53482587064676\n",
            "250/391  Loss: 1.590386440554463  Acc: 55.35047310756972\n",
            "300/391  Loss: 1.6001512857766642  Acc: 55.12354651162791\n",
            "350/391  Loss: 1.606393975070399  Acc: 54.90562678062678\n",
            "0/79  Loss: 1.5647358894348145  Acc: 56.25\n",
            "50/79  Loss: 1.9268299738566081  Acc: 49.03492647058823\n",
            "Train Accuracy:   54.822 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 58\n",
            "0/391  Loss: 1.4300185441970825  Acc: 57.03125\n",
            "50/391  Loss: 1.5568066438039143  Acc: 55.744485294117645\n",
            "100/391  Loss: 1.5611707786522289  Acc: 55.98700495049505\n",
            "150/391  Loss: 1.568336262608206  Acc: 56.022350993377486\n",
            "200/391  Loss: 1.5782440539023177  Acc: 55.68641169154229\n",
            "250/391  Loss: 1.5799183313590122  Acc: 55.79868027888446\n",
            "300/391  Loss: 1.5900032167814895  Acc: 55.57516611295681\n",
            "350/391  Loss: 1.5951844202487218  Acc: 55.53107193732194\n",
            "0/79  Loss: 1.5899158716201782  Acc: 59.375\n",
            "50/79  Loss: 1.7958814162834018  Acc: 50.96507352941177\n",
            "Train Accuracy:   55.354 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 59\n",
            "0/391  Loss: 1.4609075784683228  Acc: 57.8125\n",
            "50/391  Loss: 1.5318839690264534  Acc: 57.06188725490196\n",
            "100/391  Loss: 1.5445504519018796  Acc: 56.629022277227726\n",
            "150/391  Loss: 1.5588698087149109  Acc: 56.19308774834437\n",
            "200/391  Loss: 1.5784925281704956  Acc: 55.73305348258707\n",
            "250/391  Loss: 1.5773686949475354  Acc: 55.67106573705179\n",
            "300/391  Loss: 1.5902312990042855  Acc: 55.40126661129568\n",
            "350/391  Loss: 1.5995894381802986  Acc: 55.19275284900285\n",
            "0/79  Loss: 1.4477735757827759  Acc: 62.5\n",
            "50/79  Loss: 1.7860143348282458  Acc: 51.317401960784316\n",
            "Train Accuracy:   55.354 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 60\n",
            "0/391  Loss: 1.719383716583252  Acc: 53.125\n",
            "50/391  Loss: 1.4966943591248756  Acc: 57.8890931372549\n",
            "100/391  Loss: 1.5099874215550941  Acc: 57.479888613861384\n",
            "150/391  Loss: 1.5368479213967228  Acc: 56.77773178807947\n",
            "200/391  Loss: 1.5424602215562886  Acc: 56.68532338308458\n",
            "250/391  Loss: 1.562844963662653  Acc: 56.12861055776892\n",
            "300/391  Loss: 1.5713120342489098  Acc: 55.951515780730894\n",
            "350/391  Loss: 1.5769375311343419  Acc: 55.960648148148145\n",
            "0/79  Loss: 1.5247297286987305  Acc: 57.03125\n",
            "50/79  Loss: 1.8608249122021245  Acc: 50.275735294117645\n",
            "Train Accuracy:   55.74 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 61\n",
            "0/391  Loss: 1.5374258756637573  Acc: 59.375\n",
            "50/391  Loss: 1.5038206577301025  Acc: 57.61335784313726\n",
            "100/391  Loss: 1.5115503155358947  Acc: 57.21689356435643\n",
            "150/391  Loss: 1.542370955675643  Acc: 56.53973509933775\n",
            "200/391  Loss: 1.5546160408513463  Acc: 56.31607587064676\n",
            "250/391  Loss: 1.570803636098763  Acc: 55.9574203187251\n",
            "300/391  Loss: 1.583381746298451  Acc: 55.603716777408636\n",
            "350/391  Loss: 1.5899941401603894  Acc: 55.44871794871795\n",
            "0/79  Loss: 1.6212137937545776  Acc: 58.59375\n",
            "50/79  Loss: 1.875401772704779  Acc: 50.38296568627451\n",
            "Train Accuracy:   55.74 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 62\n",
            "0/391  Loss: 1.485982060432434  Acc: 57.03125\n",
            "50/391  Loss: 1.539356928245694  Acc: 56.004901960784316\n",
            "100/391  Loss: 1.546522597275158  Acc: 56.32735148514851\n",
            "150/391  Loss: 1.5406413496724818  Acc: 56.25\n",
            "200/391  Loss: 1.552642556565318  Acc: 56.020677860696516\n",
            "250/391  Loss: 1.5645345724911328  Acc: 55.75510458167331\n",
            "300/391  Loss: 1.5719088628838624  Acc: 55.697155315614616\n",
            "350/391  Loss: 1.5764619838817846  Acc: 55.64903846153846\n",
            "0/79  Loss: 1.7222908735275269  Acc: 53.125\n",
            "50/79  Loss: 1.920564525267657  Acc: 48.63664215686274\n",
            "Train Accuracy:   55.76 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 63\n",
            "0/391  Loss: 1.6027042865753174  Acc: 47.65625\n",
            "50/391  Loss: 1.4461944991467046  Acc: 59.083946078431374\n",
            "100/391  Loss: 1.471314517578276  Acc: 58.11417079207921\n",
            "150/391  Loss: 1.4951991213867997  Acc: 57.78145695364238\n",
            "200/391  Loss: 1.523077617830305  Acc: 57.05068407960199\n",
            "250/391  Loss: 1.5359667178644127  Acc: 56.713769920318725\n",
            "300/391  Loss: 1.547721266746521  Acc: 56.506955980066444\n",
            "350/391  Loss: 1.5565628424668922  Acc: 56.27448361823362\n",
            "0/79  Loss: 1.6600165367126465  Acc: 55.46875\n",
            "50/79  Loss: 1.8514619902068494  Acc: 50.36764705882353\n",
            "Train Accuracy:   56.088 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 64\n",
            "0/391  Loss: 1.5725269317626953  Acc: 57.03125\n",
            "50/391  Loss: 1.5096502117082184  Acc: 57.75122549019608\n",
            "100/391  Loss: 1.518534025343338  Acc: 57.38706683168317\n",
            "150/391  Loss: 1.5257135487550142  Acc: 57.093336092715234\n",
            "200/391  Loss: 1.5281320828110425  Acc: 56.98072139303483\n",
            "250/391  Loss: 1.5373707862489252  Acc: 56.71688247011952\n",
            "300/391  Loss: 1.550192177889751  Acc: 56.4031353820598\n",
            "350/391  Loss: 1.5632484196937322  Acc: 56.149839743589745\n",
            "0/79  Loss: 1.612441897392273  Acc: 59.375\n",
            "50/79  Loss: 1.8517368541044348  Acc: 50.275735294117645\n",
            "Train Accuracy:   56.094 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 65\n",
            "0/391  Loss: 1.5501168966293335  Acc: 57.8125\n",
            "50/391  Loss: 1.4929450820474064  Acc: 57.75122549019608\n",
            "100/391  Loss: 1.517266574472484  Acc: 57.116336633663366\n",
            "150/391  Loss: 1.5289281796145913  Acc: 56.963990066225165\n",
            "200/391  Loss: 1.5317834265789583  Acc: 56.94962686567164\n",
            "250/391  Loss: 1.5345884832253018  Acc: 56.94721115537848\n",
            "300/391  Loss: 1.542544911470128  Acc: 56.81582225913621\n",
            "350/391  Loss: 1.5471298229320776  Acc: 56.55715811965812\n",
            "0/79  Loss: 1.6009927988052368  Acc: 58.59375\n",
            "50/79  Loss: 1.8481678635466332  Acc: 50.35232843137255\n",
            "Train Accuracy:   56.472 %    Test Accuracy:   51.12 %\n",
            "\n",
            "Epoch: 66\n",
            "0/391  Loss: 1.5562764406204224  Acc: 58.59375\n",
            "50/391  Loss: 1.4723866523480882  Acc: 58.54779411764706\n",
            "100/391  Loss: 1.489964716505296  Acc: 58.20699257425743\n",
            "150/391  Loss: 1.4994747322916195  Acc: 57.972889072847686\n",
            "200/391  Loss: 1.5142880156265563  Acc: 57.6181592039801\n",
            "250/391  Loss: 1.5251805563846907  Acc: 57.29892928286853\n",
            "300/391  Loss: 1.5297820267091162  Acc: 57.142857142857146\n",
            "350/391  Loss: 1.5376125428072067  Acc: 56.9377670940171\n",
            "0/79  Loss: 1.6099406480789185  Acc: 55.46875\n",
            "50/79  Loss: 1.8211129436305924  Acc: 51.76164215686274\n",
            "Saving..\n",
            "Train Accuracy:   56.794 %    Test Accuracy:   51.78 %\n",
            "\n",
            "Epoch: 67\n",
            "0/391  Loss: 1.3744072914123535  Acc: 63.28125\n",
            "50/391  Loss: 1.4737771655998977  Acc: 58.53247549019608\n",
            "100/391  Loss: 1.4863441698621995  Acc: 58.13737623762376\n",
            "150/391  Loss: 1.4980659098025189  Acc: 57.74006622516556\n",
            "200/391  Loss: 1.5095631156987812  Acc: 57.416044776119406\n",
            "250/391  Loss: 1.50895724781006  Acc: 57.39853087649402\n",
            "300/391  Loss: 1.5174197160524387  Acc: 57.09354235880399\n",
            "350/391  Loss: 1.5257071657398147  Acc: 56.8019943019943\n",
            "0/79  Loss: 1.720120906829834  Acc: 55.46875\n",
            "50/79  Loss: 1.8297632628796148  Acc: 51.685049019607845\n",
            "Train Accuracy:   56.794 %    Test Accuracy:   51.78 %\n",
            "\n",
            "Epoch: 68\n",
            "0/391  Loss: 1.3392322063446045  Acc: 64.84375\n",
            "50/391  Loss: 1.4245304476981069  Acc: 59.283088235294116\n",
            "100/391  Loss: 1.4409359162396724  Acc: 59.088799504950494\n",
            "150/391  Loss: 1.4598228978794932  Acc: 58.59375\n",
            "200/391  Loss: 1.4829626961133966  Acc: 58.03016169154229\n",
            "250/391  Loss: 1.4987769430852032  Acc: 57.669322709163346\n",
            "300/391  Loss: 1.5124952365393654  Acc: 57.31935215946844\n",
            "350/391  Loss: 1.5133221448316874  Acc: 57.24269943019943\n",
            "0/79  Loss: 1.5961319208145142  Acc: 58.59375\n",
            "50/79  Loss: 1.824343064252068  Acc: 51.34803921568628\n",
            "Train Accuracy:   57.108 %    Test Accuracy:   51.78 %\n",
            "\n",
            "Epoch: 69\n",
            "0/391  Loss: 1.3682509660720825  Acc: 61.71875\n",
            "50/391  Loss: 1.4157386948080624  Acc: 59.283088235294116\n",
            "100/391  Loss: 1.4333775857887645  Acc: 59.14294554455446\n",
            "150/391  Loss: 1.447336203215138  Acc: 58.86796357615894\n",
            "200/391  Loss: 1.462474490279582  Acc: 58.57431592039801\n",
            "250/391  Loss: 1.4820576887206727  Acc: 58.01481573705179\n",
            "300/391  Loss: 1.49241737234236  Acc: 57.716465946843854\n",
            "350/391  Loss: 1.5023901724068187  Acc: 57.50979344729345\n",
            "0/79  Loss: 1.3912986516952515  Acc: 67.1875\n",
            "50/79  Loss: 1.7505285155539418  Acc: 52.97181372549019\n",
            "Saving..\n",
            "Train Accuracy:   57.324 %    Test Accuracy:   52.92 %\n",
            "\n",
            "Epoch: 70\n",
            "0/391  Loss: 1.1362839937210083  Acc: 69.53125\n",
            "50/391  Loss: 1.4308794386246626  Acc: 59.34436274509804\n",
            "100/391  Loss: 1.440587069728587  Acc: 58.879950495049506\n",
            "150/391  Loss: 1.4638062478690748  Acc: 58.28849337748344\n",
            "200/391  Loss: 1.4763851723267665  Acc: 58.11178482587065\n",
            "250/391  Loss: 1.4890796212086164  Acc: 57.89342629482072\n",
            "300/391  Loss: 1.491403226044487  Acc: 57.85143272425249\n",
            "350/391  Loss: 1.498891099226101  Acc: 57.63443732193732\n",
            "0/79  Loss: 1.6867611408233643  Acc: 59.375\n",
            "50/79  Loss: 1.872020396531797  Acc: 50.21446078431372\n",
            "Train Accuracy:   57.466 %    Test Accuracy:   52.92 %\n",
            "\n",
            "Epoch: 71\n",
            "0/391  Loss: 1.4993305206298828  Acc: 57.03125\n",
            "50/391  Loss: 1.4669091070399565  Acc: 58.25674019607843\n",
            "100/391  Loss: 1.4509097373131479  Acc: 58.50092821782178\n",
            "150/391  Loss: 1.464294825168635  Acc: 58.366100993377486\n",
            "200/391  Loss: 1.4731624992332648  Acc: 58.09235074626866\n",
            "250/391  Loss: 1.487057617461064  Acc: 57.87475099601593\n",
            "300/391  Loss: 1.4946648001274794  Acc: 57.65417358803987\n",
            "350/391  Loss: 1.4946034097263956  Acc: 57.65669515669516\n",
            "0/79  Loss: 1.821661114692688  Acc: 53.125\n",
            "50/79  Loss: 1.7887453448538686  Acc: 52.068014705882355\n",
            "Train Accuracy:   57.582 %    Test Accuracy:   52.92 %\n",
            "\n",
            "Epoch: 72\n",
            "0/391  Loss: 1.375993251800537  Acc: 60.15625\n",
            "50/391  Loss: 1.3636014438142963  Acc: 61.38174019607843\n",
            "100/391  Loss: 1.3946655764438138  Acc: 60.62809405940594\n",
            "150/391  Loss: 1.409437322458684  Acc: 59.98551324503311\n",
            "200/391  Loss: 1.4300805531924043  Acc: 59.375\n",
            "250/391  Loss: 1.4480623956695495  Acc: 58.82407868525896\n",
            "300/391  Loss: 1.462445664247405  Acc: 58.53924418604651\n",
            "350/391  Loss: 1.4749125911299659  Acc: 58.282140313390315\n",
            "0/79  Loss: 1.6748400926589966  Acc: 55.46875\n",
            "50/79  Loss: 1.796871587341907  Acc: 51.59313725490196\n",
            "Train Accuracy:   58.084 %    Test Accuracy:   52.92 %\n",
            "\n",
            "Epoch: 73\n",
            "0/391  Loss: 1.209742784500122  Acc: 62.5\n",
            "50/391  Loss: 1.4036314580954758  Acc: 60.14093137254902\n",
            "100/391  Loss: 1.4115493628058102  Acc: 59.993811881188115\n",
            "150/391  Loss: 1.4235979176514986  Acc: 59.63369205298013\n",
            "200/391  Loss: 1.439067229109617  Acc: 59.184546019900495\n",
            "250/391  Loss: 1.4552741772625075  Acc: 58.81785358565737\n",
            "300/391  Loss: 1.462970330469632  Acc: 58.596345514950166\n",
            "350/391  Loss: 1.474300430371211  Acc: 58.29104344729345\n",
            "0/79  Loss: 1.6477398872375488  Acc: 57.8125\n",
            "50/79  Loss: 1.838991363843282  Acc: 51.37867647058823\n",
            "Train Accuracy:   58.27 %    Test Accuracy:   52.92 %\n",
            "\n",
            "Epoch: 74\n",
            "0/391  Loss: 1.507876992225647  Acc: 58.59375\n",
            "50/391  Loss: 1.4015872945972518  Acc: 59.83455882352941\n",
            "100/391  Loss: 1.4281012858494673  Acc: 59.28217821782178\n",
            "150/391  Loss: 1.4269913821820392  Acc: 59.15769867549669\n",
            "200/391  Loss: 1.4360508408712511  Acc: 59.08348880597015\n",
            "250/391  Loss: 1.4448381339411336  Acc: 58.90500498007968\n",
            "300/391  Loss: 1.4537220864596954  Acc: 58.739098837209305\n",
            "350/391  Loss: 1.46660813137337  Acc: 58.45352564102564\n",
            "0/79  Loss: 1.5775871276855469  Acc: 57.8125\n",
            "50/79  Loss: 1.7972232781204522  Acc: 51.5625\n",
            "Train Accuracy:   58.402 %    Test Accuracy:   52.92 %\n",
            "\n",
            "Epoch: 75\n",
            "0/391  Loss: 1.312469720840454  Acc: 62.5\n",
            "50/391  Loss: 1.4068363414091223  Acc: 60.692401960784316\n",
            "100/391  Loss: 1.4026225793479692  Acc: 60.194925742574256\n",
            "150/391  Loss: 1.407666114781866  Acc: 60.052773178807946\n",
            "200/391  Loss: 1.4270804343531973  Acc: 59.67039800995025\n",
            "250/391  Loss: 1.4352177220036784  Acc: 59.38433764940239\n",
            "300/391  Loss: 1.4425894936849906  Acc: 59.18033637873754\n",
            "350/391  Loss: 1.451118005306972  Acc: 58.936520655270655\n",
            "0/79  Loss: 1.5562983751296997  Acc: 58.59375\n",
            "50/79  Loss: 1.7404409740485398  Acc: 52.80330882352941\n",
            "Saving..\n",
            "Train Accuracy:   58.718 %    Test Accuracy:   53.06 %\n",
            "\n",
            "Epoch: 76\n",
            "0/391  Loss: 1.419958233833313  Acc: 60.15625\n",
            "50/391  Loss: 1.3307122155731799  Acc: 61.90257352941177\n",
            "100/391  Loss: 1.3628079973825133  Acc: 61.223700495049506\n",
            "150/391  Loss: 1.3801494481547778  Acc: 60.60637417218543\n",
            "200/391  Loss: 1.4027179948133022  Acc: 59.94636194029851\n",
            "250/391  Loss: 1.4177031255813235  Acc: 59.555527888446214\n",
            "300/391  Loss: 1.4322626079831804  Acc: 59.16216777408638\n",
            "350/391  Loss: 1.441114698380147  Acc: 58.97658475783476\n",
            "0/79  Loss: 1.5876941680908203  Acc: 63.28125\n",
            "50/79  Loss: 1.802197117431491  Acc: 51.51654411764706\n",
            "Train Accuracy:   58.822 %    Test Accuracy:   53.06 %\n",
            "\n",
            "Epoch: 77\n",
            "0/391  Loss: 1.3670483827590942  Acc: 60.15625\n",
            "50/391  Loss: 1.3549623629626106  Acc: 61.70343137254902\n",
            "100/391  Loss: 1.3615068681169264  Acc: 61.25464108910891\n",
            "150/391  Loss: 1.3888970802951333  Acc: 60.40459437086093\n",
            "200/391  Loss: 1.4032464519662051  Acc: 60.10572139303483\n",
            "250/391  Loss: 1.4094669334442018  Acc: 59.929033864541836\n",
            "300/391  Loss: 1.4221906119406817  Acc: 59.603405315614616\n",
            "350/391  Loss: 1.4333943821426132  Acc: 59.328258547008545\n",
            "0/79  Loss: 1.8349875211715698  Acc: 50.0\n",
            "50/79  Loss: 1.8500262078116922  Acc: 51.19485294117647\n",
            "Train Accuracy:   59.12 %    Test Accuracy:   53.06 %\n",
            "\n",
            "Epoch: 78\n",
            "0/391  Loss: 1.205881118774414  Acc: 66.40625\n",
            "50/391  Loss: 1.3247918708651674  Acc: 62.2702205882353\n",
            "100/391  Loss: 1.3462095838962216  Acc: 61.448019801980195\n",
            "150/391  Loss: 1.3711488602177198  Acc: 60.89093543046358\n",
            "200/391  Loss: 1.387032926082611  Acc: 60.482742537313435\n",
            "250/391  Loss: 1.3960752665284146  Acc: 60.17181274900398\n",
            "300/391  Loss: 1.4033580675869686  Acc: 60.02387873754153\n",
            "350/391  Loss: 1.412167449792226  Acc: 59.77564102564103\n",
            "0/79  Loss: 1.6952970027923584  Acc: 57.03125\n",
            "50/79  Loss: 1.8471648552838493  Acc: 50.903799019607845\n",
            "Train Accuracy:   59.686 %    Test Accuracy:   53.06 %\n",
            "\n",
            "Epoch: 79\n",
            "0/391  Loss: 1.3949216604232788  Acc: 60.15625\n",
            "50/391  Loss: 1.3390846579682594  Acc: 61.78002450980392\n",
            "100/391  Loss: 1.348387067860896  Acc: 61.40160891089109\n",
            "150/391  Loss: 1.374298908063118  Acc: 60.45633278145695\n",
            "200/391  Loss: 1.3817946074613885  Acc: 60.49440298507463\n",
            "250/391  Loss: 1.3917825939170867  Acc: 60.21850099601593\n",
            "300/391  Loss: 1.3995155369324541  Acc: 59.99273255813954\n",
            "350/391  Loss: 1.4090451728924047  Acc: 59.71331908831909\n",
            "0/79  Loss: 1.4167418479919434  Acc: 63.28125\n",
            "50/79  Loss: 1.7709347664141188  Acc: 52.68075980392157\n",
            "Train Accuracy:   59.686 %    Test Accuracy:   53.06 %\n",
            "\n",
            "Epoch: 80\n",
            "0/391  Loss: 1.260133981704712  Acc: 58.59375\n",
            "50/391  Loss: 1.3101187009437412  Acc: 62.408088235294116\n",
            "100/391  Loss: 1.3518152484799375  Acc: 61.20049504950495\n",
            "150/391  Loss: 1.361794037534701  Acc: 60.97371688741722\n",
            "200/391  Loss: 1.3771156499634927  Acc: 60.62266791044776\n",
            "250/391  Loss: 1.3775311319001642  Acc: 60.73207171314741\n",
            "300/391  Loss: 1.3864134451083567  Acc: 60.55336378737542\n",
            "350/391  Loss: 1.3975844369654642  Acc: 60.160701566951566\n",
            "0/79  Loss: 1.6851900815963745  Acc: 59.375\n",
            "50/79  Loss: 1.7456449457243377  Acc: 53.14031862745098\n",
            "Saving..\n",
            "Train Accuracy:   59.972 %    Test Accuracy:   53.15 %\n",
            "\n",
            "Epoch: 81\n",
            "0/391  Loss: 1.1780660152435303  Acc: 66.40625\n",
            "50/391  Loss: 1.2748288442106808  Acc: 63.174019607843135\n",
            "100/391  Loss: 1.294238812262469  Acc: 62.275680693069305\n",
            "150/391  Loss: 1.3247890397412887  Acc: 61.76531456953642\n",
            "200/391  Loss: 1.3416610721925004  Acc: 61.36893656716418\n",
            "250/391  Loss: 1.3589105769932508  Acc: 61.08690239043825\n",
            "300/391  Loss: 1.3734882382855462  Acc: 60.76360049833887\n",
            "350/391  Loss: 1.3807959371482545  Acc: 60.581374643874646\n",
            "0/79  Loss: 1.6262354850769043  Acc: 58.59375\n",
            "50/79  Loss: 1.7431999351464065  Acc: 52.634803921568626\n",
            "Train Accuracy:   60.362 %    Test Accuracy:   53.15 %\n",
            "\n",
            "Epoch: 82\n",
            "0/391  Loss: 1.2351927757263184  Acc: 67.96875\n",
            "50/391  Loss: 1.2948508262634277  Acc: 63.158700980392155\n",
            "100/391  Loss: 1.3080074952380492  Acc: 62.13644801980198\n",
            "150/391  Loss: 1.3254592008148598  Acc: 61.7135761589404\n",
            "200/391  Loss: 1.347637862708438  Acc: 61.25233208955224\n",
            "250/391  Loss: 1.3539345729873475  Acc: 61.080677290836654\n",
            "300/391  Loss: 1.3636015404102415  Acc: 60.97643272425249\n",
            "350/391  Loss: 1.3751972224298026  Acc: 60.68376068376068\n",
            "0/79  Loss: 1.4463504552841187  Acc: 62.5\n",
            "50/79  Loss: 1.774990345917496  Acc: 51.62377450980392\n",
            "Train Accuracy:   60.51 %    Test Accuracy:   53.15 %\n",
            "\n",
            "Epoch: 83\n",
            "0/391  Loss: 1.410306453704834  Acc: 64.84375\n",
            "50/391  Loss: 1.322352598695194  Acc: 62.13235294117647\n",
            "100/391  Loss: 1.3243762337335265  Acc: 61.86571782178218\n",
            "150/391  Loss: 1.3342807103466514  Acc: 61.7290976821192\n",
            "200/391  Loss: 1.3475644950249894  Acc: 61.5166355721393\n",
            "250/391  Loss: 1.366257318937446  Acc: 60.94995019920319\n",
            "300/391  Loss: 1.3687819389013753  Acc: 60.81551079734219\n",
            "350/391  Loss: 1.3705655394116698  Acc: 60.761663105413106\n",
            "0/79  Loss: 1.4245918989181519  Acc: 64.0625\n",
            "50/79  Loss: 1.762963379130644  Acc: 52.619485294117645\n",
            "Train Accuracy:   60.648 %    Test Accuracy:   53.15 %\n",
            "\n",
            "Epoch: 84\n",
            "0/391  Loss: 1.5849277973175049  Acc: 59.375\n",
            "50/391  Loss: 1.2635492962949417  Acc: 63.771446078431374\n",
            "100/391  Loss: 1.2773567219771962  Acc: 63.165222772277225\n",
            "150/391  Loss: 1.2955288733078154  Acc: 62.62934602649007\n",
            "200/391  Loss: 1.3125066374664875  Acc: 62.08411069651741\n",
            "250/391  Loss: 1.3258683170930323  Acc: 61.69384960159363\n",
            "300/391  Loss: 1.343999626034518  Acc: 61.326827242524914\n",
            "350/391  Loss: 1.356012114262649  Acc: 61.037660256410255\n",
            "0/79  Loss: 1.5357297658920288  Acc: 58.59375\n",
            "50/79  Loss: 1.7594759861628215  Acc: 52.78799019607843\n",
            "Train Accuracy:   60.818 %    Test Accuracy:   53.15 %\n",
            "\n",
            "Epoch: 85\n",
            "0/391  Loss: 1.1002466678619385  Acc: 66.40625\n",
            "50/391  Loss: 1.2726628324564766  Acc: 63.373161764705884\n",
            "100/391  Loss: 1.2764119340641664  Acc: 63.08787128712871\n",
            "150/391  Loss: 1.292671164929472  Acc: 62.94495033112583\n",
            "200/391  Loss: 1.3101529683639754  Acc: 62.45724502487562\n",
            "250/391  Loss: 1.3202351626172009  Acc: 62.182519920318725\n",
            "300/391  Loss: 1.334643422171127  Acc: 61.76806478405316\n",
            "350/391  Loss: 1.3426346646414862  Acc: 61.46723646723647\n",
            "0/79  Loss: 1.6489269733428955  Acc: 57.03125\n",
            "50/79  Loss: 1.7589047258975459  Acc: 53.15563725490196\n",
            "Saving..\n",
            "Train Accuracy:   61.394 %    Test Accuracy:   53.4 %\n",
            "\n",
            "Epoch: 86\n",
            "0/391  Loss: 1.1936602592468262  Acc: 67.1875\n",
            "50/391  Loss: 1.2558075960944681  Acc: 63.84803921568628\n",
            "100/391  Loss: 1.280578372502091  Acc: 63.32766089108911\n",
            "150/391  Loss: 1.2911473520544192  Acc: 63.02255794701987\n",
            "200/391  Loss: 1.3022689327078671  Acc: 62.706001243781095\n",
            "250/391  Loss: 1.317058357584524  Acc: 62.366160358565736\n",
            "300/391  Loss: 1.323469008322174  Acc: 62.13143687707641\n",
            "350/391  Loss: 1.331259544418748  Acc: 61.881232193732195\n",
            "0/79  Loss: 1.3819005489349365  Acc: 62.5\n",
            "50/79  Loss: 1.7118220703274596  Acc: 54.73345588235294\n",
            "Saving..\n",
            "Train Accuracy:   61.72 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 87\n",
            "0/391  Loss: 1.0353373289108276  Acc: 71.875\n",
            "50/391  Loss: 1.2423107740925807  Acc: 64.49142156862744\n",
            "100/391  Loss: 1.250773316562766  Acc: 63.90779702970297\n",
            "150/391  Loss: 1.283610216829161  Acc: 63.136382450331126\n",
            "200/391  Loss: 1.2954176931238885  Acc: 62.83037935323383\n",
            "250/391  Loss: 1.3021750901324816  Acc: 62.643177290836654\n",
            "300/391  Loss: 1.3083795546693264  Acc: 62.37281976744186\n",
            "350/391  Loss: 1.3267492466842348  Acc: 61.925747863247864\n",
            "0/79  Loss: 1.521488904953003  Acc: 58.59375\n",
            "50/79  Loss: 1.694968723783306  Acc: 54.044117647058826\n",
            "Train Accuracy:   61.784 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 88\n",
            "0/391  Loss: 1.274640440940857  Acc: 64.0625\n",
            "50/391  Loss: 1.2004671424042945  Acc: 65.42585784313725\n",
            "100/391  Loss: 1.2269757478544028  Acc: 64.84375\n",
            "150/391  Loss: 1.2453467695918303  Acc: 64.42466887417218\n",
            "200/391  Loss: 1.2620799932906877  Acc: 63.840951492537314\n",
            "250/391  Loss: 1.2868177572569524  Acc: 63.09760956175299\n",
            "300/391  Loss: 1.2948256675587144  Acc: 62.871158637873755\n",
            "350/391  Loss: 1.3050272603999515  Acc: 62.65357905982906\n",
            "0/79  Loss: 1.4821217060089111  Acc: 60.9375\n",
            "50/79  Loss: 1.784488266589595  Acc: 52.37438725490196\n",
            "Train Accuracy:   62.504 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 89\n",
            "0/391  Loss: 1.1846802234649658  Acc: 60.9375\n",
            "50/391  Loss: 1.2476626868341483  Acc: 63.87867647058823\n",
            "100/391  Loss: 1.270020856715665  Acc: 63.73762376237624\n",
            "150/391  Loss: 1.2656961482092244  Acc: 63.777938741721854\n",
            "200/391  Loss: 1.27209154646195  Acc: 63.63106343283582\n",
            "250/391  Loss: 1.271114587308876  Acc: 63.50224103585657\n",
            "300/391  Loss: 1.2808658035094556  Acc: 63.16445182724252\n",
            "350/391  Loss: 1.293613790786504  Acc: 62.778222934472936\n",
            "0/79  Loss: 1.7370682954788208  Acc: 51.5625\n",
            "50/79  Loss: 1.8901648053935929  Acc: 50.873161764705884\n",
            "Train Accuracy:   62.504 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 90\n",
            "0/391  Loss: 1.2425413131713867  Acc: 65.625\n",
            "50/391  Loss: 1.206575396014195  Acc: 65.15012254901961\n",
            "100/391  Loss: 1.2174651988662115  Acc: 64.6194306930693\n",
            "150/391  Loss: 1.2410541950472143  Acc: 63.75724337748344\n",
            "200/391  Loss: 1.2576089032846898  Acc: 63.471703980099505\n",
            "250/391  Loss: 1.2756769778244048  Acc: 62.99800796812749\n",
            "300/391  Loss: 1.278947617524486  Acc: 62.84001245847176\n",
            "350/391  Loss: 1.2864018580173155  Acc: 62.64912749287749\n",
            "0/79  Loss: 1.4862167835235596  Acc: 56.25\n",
            "50/79  Loss: 1.7077251929862827  Acc: 53.8296568627451\n",
            "Train Accuracy:   62.504 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 91\n",
            "0/391  Loss: 1.364412546157837  Acc: 64.84375\n",
            "50/391  Loss: 1.1773632098646725  Acc: 65.97732843137256\n",
            "100/391  Loss: 1.1963067214087684  Acc: 65.52444306930693\n",
            "150/391  Loss: 1.2191022433192524  Acc: 64.62127483443709\n",
            "200/391  Loss: 1.2392264809181441  Acc: 64.08970771144278\n",
            "250/391  Loss: 1.2471801652851333  Acc: 63.92554780876494\n",
            "300/391  Loss: 1.2648232507943316  Acc: 63.43957641196013\n",
            "350/391  Loss: 1.2756658240600869  Acc: 63.21002492877493\n",
            "0/79  Loss: 1.5633244514465332  Acc: 57.8125\n",
            "50/79  Loss: 1.765169571427738  Acc: 53.033088235294116\n",
            "Train Accuracy:   63.084 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 92\n",
            "0/391  Loss: 1.0470346212387085  Acc: 71.875\n",
            "50/391  Loss: 1.1642457831139659  Acc: 66.51348039215686\n",
            "100/391  Loss: 1.1994385648481916  Acc: 65.39294554455445\n",
            "150/391  Loss: 1.2194673857152067  Acc: 64.98344370860927\n",
            "200/391  Loss: 1.2301109293800088  Acc: 64.64163557213931\n",
            "250/391  Loss: 1.2414693547435016  Acc: 64.30839143426294\n",
            "300/391  Loss: 1.2498091649375485  Acc: 63.93791528239203\n",
            "350/391  Loss: 1.256051958795966  Acc: 63.824341168091166\n",
            "0/79  Loss: 1.3388956785202026  Acc: 60.9375\n",
            "50/79  Loss: 1.6920051224091475  Acc: 54.427083333333336\n",
            "Train Accuracy:   63.678 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 93\n",
            "0/391  Loss: 1.177026629447937  Acc: 65.625\n",
            "50/391  Loss: 1.128673417895448  Acc: 67.67769607843137\n",
            "100/391  Loss: 1.16220621130254  Acc: 66.46813118811882\n",
            "150/391  Loss: 1.1861982373212347  Acc: 65.74399834437087\n",
            "200/391  Loss: 1.2072432877412482  Acc: 65.13914800995025\n",
            "250/391  Loss: 1.222164048379161  Acc: 64.64143426294821\n",
            "300/391  Loss: 1.231737539023656  Acc: 64.44923172757476\n",
            "350/391  Loss: 1.2422161610038192  Acc: 64.17378917378917\n",
            "0/79  Loss: 1.4432909488677979  Acc: 58.59375\n",
            "50/79  Loss: 1.76901487976897  Acc: 52.77267156862745\n",
            "Train Accuracy:   63.826 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 94\n",
            "0/391  Loss: 1.126881718635559  Acc: 70.3125\n",
            "50/391  Loss: 1.1776804035785151  Acc: 66.16115196078431\n",
            "100/391  Loss: 1.1722134139278146  Acc: 65.94214108910892\n",
            "150/391  Loss: 1.1904269670019088  Acc: 65.37665562913908\n",
            "200/391  Loss: 1.2071898063616966  Acc: 64.73880597014926\n",
            "250/391  Loss: 1.2145059464937187  Acc: 64.54805776892431\n",
            "300/391  Loss: 1.2206343463885032  Acc: 64.3376245847176\n",
            "350/391  Loss: 1.2308512504963454  Acc: 64.16488603988604\n",
            "0/79  Loss: 1.6589382886886597  Acc: 54.6875\n",
            "50/79  Loss: 1.7705997205248065  Acc: 53.370098039215684\n",
            "Train Accuracy:   63.862 %    Test Accuracy:   54.63 %\n",
            "\n",
            "Epoch: 95\n",
            "0/391  Loss: 1.165700078010559  Acc: 61.71875\n",
            "50/391  Loss: 1.132866476096359  Acc: 67.0343137254902\n",
            "100/391  Loss: 1.1544021332618033  Acc: 66.55321782178218\n",
            "150/391  Loss: 1.176364709209922  Acc: 65.98716887417218\n",
            "200/391  Loss: 1.1821081300279987  Acc: 65.63666044776119\n",
            "250/391  Loss: 1.2020744042567524  Acc: 65.16434262948208\n",
            "300/391  Loss: 1.2078329421753107  Acc: 65.05917774086379\n",
            "350/391  Loss: 1.2177917527676987  Acc: 64.78587962962963\n",
            "0/79  Loss: 1.5154329538345337  Acc: 56.25\n",
            "50/79  Loss: 1.6723662170709348  Acc: 55.00919117647059\n",
            "Saving..\n",
            "Train Accuracy:   64.608 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 96\n",
            "0/391  Loss: 0.9659412503242493  Acc: 76.5625\n",
            "50/391  Loss: 1.130466666876101  Acc: 68.13725490196079\n",
            "100/391  Loss: 1.148253318696919  Acc: 67.07147277227723\n",
            "150/391  Loss: 1.1651450928473315  Acc: 66.50455298013244\n",
            "200/391  Loss: 1.1769847511059015  Acc: 66.11862562189054\n",
            "250/391  Loss: 1.192195411697327  Acc: 65.78685258964144\n",
            "300/391  Loss: 1.2029365879356664  Acc: 65.37323504983388\n",
            "350/391  Loss: 1.2122837053744542  Acc: 65.07523148148148\n",
            "0/79  Loss: 1.6840434074401855  Acc: 56.25\n",
            "50/79  Loss: 1.7395912829567404  Acc: 53.46200980392157\n",
            "Train Accuracy:   64.992 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 97\n",
            "0/391  Loss: 1.000598430633545  Acc: 72.65625\n",
            "50/391  Loss: 1.132948476894229  Acc: 67.46323529411765\n",
            "100/391  Loss: 1.1611173619138133  Acc: 66.25928217821782\n",
            "150/391  Loss: 1.1492568534731076  Acc: 66.71668046357615\n",
            "200/391  Loss: 1.1595080954518484  Acc: 66.18470149253731\n",
            "250/391  Loss: 1.1711728883929462  Acc: 65.90512948207171\n",
            "300/391  Loss: 1.1834779985719346  Acc: 65.56530315614619\n",
            "350/391  Loss: 1.1938252938099396  Acc: 65.29780982905983\n",
            "0/79  Loss: 1.6283323764801025  Acc: 55.46875\n",
            "50/79  Loss: 1.7187476765875722  Acc: 54.427083333333336\n",
            "Train Accuracy:   65.08 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 98\n",
            "0/391  Loss: 1.2722541093826294  Acc: 61.71875\n",
            "50/391  Loss: 1.1326010110331517  Acc: 66.80453431372548\n",
            "100/391  Loss: 1.1178441236514856  Acc: 67.06373762376238\n",
            "150/391  Loss: 1.1333038897703815  Acc: 66.64942052980132\n",
            "200/391  Loss: 1.150428748545955  Acc: 66.29741915422886\n",
            "250/391  Loss: 1.1641607265548402  Acc: 65.93002988047809\n",
            "300/391  Loss: 1.1702704952404746  Acc: 65.86638289036544\n",
            "350/391  Loss: 1.181321999116501  Acc: 65.62945156695157\n",
            "0/79  Loss: 1.469045877456665  Acc: 61.71875\n",
            "50/79  Loss: 1.7356510559717815  Acc: 54.05943627450981\n",
            "Train Accuracy:   65.372 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 99\n",
            "0/391  Loss: 0.9331580996513367  Acc: 72.65625\n",
            "50/391  Loss: 1.1061869707762026  Acc: 67.8921568627451\n",
            "100/391  Loss: 1.1252344156255816  Acc: 66.9012995049505\n",
            "150/391  Loss: 1.1395638241673147  Acc: 66.65976821192054\n",
            "200/391  Loss: 1.1507062375248962  Acc: 66.33240049751244\n",
            "250/391  Loss: 1.1616684058272981  Acc: 66.24128486055777\n",
            "300/391  Loss: 1.1633126309939794  Acc: 66.22196843853821\n",
            "350/391  Loss: 1.1694753524924275  Acc: 66.04567307692308\n",
            "0/79  Loss: 1.5670301914215088  Acc: 60.15625\n",
            "50/79  Loss: 1.7584030558081234  Acc: 53.615196078431374\n",
            "Train Accuracy:   65.858 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 100\n",
            "0/391  Loss: 1.228419303894043  Acc: 61.71875\n",
            "50/391  Loss: 1.0836421777220333  Acc: 68.61213235294117\n",
            "100/391  Loss: 1.074128767051319  Acc: 68.76547029702971\n",
            "150/391  Loss: 1.097912891811093  Acc: 67.92735927152317\n",
            "200/391  Loss: 1.1140951876023515  Acc: 67.47123756218906\n",
            "250/391  Loss: 1.1277529298071842  Acc: 67.10968625498008\n",
            "300/391  Loss: 1.1432349212541928  Acc: 66.61129568106313\n",
            "350/391  Loss: 1.1521674668007766  Acc: 66.43073361823362\n",
            "0/79  Loss: 1.5746963024139404  Acc: 59.375\n",
            "50/79  Loss: 1.709479002391591  Acc: 54.64154411764706\n",
            "Train Accuracy:   66.148 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 101\n",
            "0/391  Loss: 1.0059616565704346  Acc: 69.53125\n",
            "50/391  Loss: 1.074096421400706  Acc: 68.81127450980392\n",
            "100/391  Loss: 1.078732445098386  Acc: 68.54888613861387\n",
            "150/391  Loss: 1.1067043345495564  Acc: 67.90149006622516\n",
            "200/391  Loss: 1.1082923284810573  Acc: 67.88324004975124\n",
            "250/391  Loss: 1.1179061565266188  Acc: 67.41160358565737\n",
            "300/391  Loss: 1.1296134131691384  Acc: 67.13039867109634\n",
            "350/391  Loss: 1.1402657408999581  Acc: 66.76682692307692\n",
            "0/79  Loss: 1.335300326347351  Acc: 62.5\n",
            "50/79  Loss: 1.716724952061971  Acc: 54.64154411764706\n",
            "Train Accuracy:   66.462 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 102\n",
            "0/391  Loss: 1.1094461679458618  Acc: 68.75\n",
            "50/391  Loss: 1.0594682518173666  Acc: 69.27083333333333\n",
            "100/391  Loss: 1.0780825372969751  Acc: 68.7190594059406\n",
            "150/391  Loss: 1.09214707162996  Acc: 68.06187913907284\n",
            "200/391  Loss: 1.1002952690741317  Acc: 67.7860696517413\n",
            "250/391  Loss: 1.1195957606057247  Acc: 67.14703685258964\n",
            "300/391  Loss: 1.1297890153834194  Acc: 66.83710548172758\n",
            "350/391  Loss: 1.1382449175897147  Acc: 66.60434472934473\n",
            "0/79  Loss: 1.4236249923706055  Acc: 61.71875\n",
            "50/79  Loss: 1.743848634701149  Acc: 53.73774509803921\n",
            "Train Accuracy:   66.558 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 103\n",
            "0/391  Loss: 1.0026723146438599  Acc: 72.65625\n",
            "50/391  Loss: 1.0575470445202846  Acc: 68.91850490196079\n",
            "100/391  Loss: 1.0419153925215845  Acc: 69.59313118811882\n",
            "150/391  Loss: 1.058073940656043  Acc: 69.03973509933775\n",
            "200/391  Loss: 1.0688550012621714  Acc: 68.58675373134328\n",
            "250/391  Loss: 1.085775415498422  Acc: 68.08391434262948\n",
            "300/391  Loss: 1.0952051277968575  Acc: 67.73515365448505\n",
            "350/391  Loss: 1.1130686847233024  Acc: 67.2542735042735\n",
            "0/79  Loss: 1.3753588199615479  Acc: 67.1875\n",
            "50/79  Loss: 1.7266558852850222  Acc: 54.4577205882353\n",
            "Train Accuracy:   67.026 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 104\n",
            "0/391  Loss: 1.1231024265289307  Acc: 65.625\n",
            "50/391  Loss: 1.0435246451228273  Acc: 69.53125\n",
            "100/391  Loss: 1.050732430845204  Acc: 69.22957920792079\n",
            "150/391  Loss: 1.0516636411085825  Acc: 69.25186258278146\n",
            "200/391  Loss: 1.0688512212601466  Acc: 68.71890547263682\n",
            "250/391  Loss: 1.0855047968754254  Acc: 68.1741782868526\n",
            "300/391  Loss: 1.0947960438918434  Acc: 67.84416528239203\n",
            "350/391  Loss: 1.1018971693821442  Acc: 67.75730056980056\n",
            "0/79  Loss: 1.530612826347351  Acc: 64.84375\n",
            "50/79  Loss: 1.7280473849352669  Acc: 54.9172794117647\n",
            "Train Accuracy:   67.58 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 105\n",
            "0/391  Loss: 0.9696868062019348  Acc: 70.3125\n",
            "50/391  Loss: 1.0390572582974154  Acc: 69.48529411764706\n",
            "100/391  Loss: 1.0202064891852956  Acc: 69.87159653465346\n",
            "150/391  Loss: 1.0383988543851486  Acc: 69.5364238410596\n",
            "200/391  Loss: 1.0575203895568848  Acc: 68.87437810945273\n",
            "250/391  Loss: 1.0646576653438735  Acc: 68.70642430278885\n",
            "300/391  Loss: 1.0773248100201553  Acc: 68.43334717607974\n",
            "350/391  Loss: 1.0880016971517492  Acc: 68.09561965811966\n",
            "0/79  Loss: 1.4482359886169434  Acc: 67.1875\n",
            "50/79  Loss: 1.7208692419762706  Acc: 54.580269607843135\n",
            "Train Accuracy:   67.832 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 106\n",
            "0/391  Loss: 1.115412950515747  Acc: 64.0625\n",
            "50/391  Loss: 1.0087796718466515  Acc: 69.92953431372548\n",
            "100/391  Loss: 1.0330103594477813  Acc: 69.63180693069307\n",
            "150/391  Loss: 1.0385878326877063  Acc: 69.7226821192053\n",
            "200/391  Loss: 1.0446671695258487  Acc: 69.49238184079601\n",
            "250/391  Loss: 1.0557337605145822  Acc: 69.10171812749005\n",
            "300/391  Loss: 1.0646679789520974  Acc: 68.85122508305648\n",
            "350/391  Loss: 1.0770675575631297  Acc: 68.50071225071225\n",
            "0/79  Loss: 1.521180510520935  Acc: 63.28125\n",
            "50/79  Loss: 1.7181356934940113  Acc: 54.28921568627451\n",
            "Train Accuracy:   68.376 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 107\n",
            "0/391  Loss: 1.183530330657959  Acc: 67.1875\n",
            "50/391  Loss: 0.989210725999346  Acc: 70.69546568627452\n",
            "100/391  Loss: 0.9763161156437185  Acc: 71.07054455445545\n",
            "150/391  Loss: 0.9870867050246687  Acc: 70.9126655629139\n",
            "200/391  Loss: 0.9993345784903759  Acc: 70.27751865671642\n",
            "250/391  Loss: 1.0173525511031132  Acc: 69.75224103585657\n",
            "300/391  Loss: 1.0305467905396244  Acc: 69.3547549833887\n",
            "350/391  Loss: 1.0454146400136486  Acc: 68.98370726495726\n",
            "0/79  Loss: 1.4034358263015747  Acc: 64.0625\n",
            "50/79  Loss: 1.7287890607235479  Acc: 54.87132352941177\n",
            "Train Accuracy:   68.796 %    Test Accuracy:   54.92 %\n",
            "\n",
            "Epoch: 108\n",
            "0/391  Loss: 0.9396217465400696  Acc: 68.75\n",
            "50/391  Loss: 0.9870150895679698  Acc: 71.2469362745098\n",
            "100/391  Loss: 0.9746046308243629  Acc: 71.55785891089108\n",
            "150/391  Loss: 0.9899518418785752  Acc: 70.8919701986755\n",
            "200/391  Loss: 1.0046581198920066  Acc: 70.59235074626865\n",
            "250/391  Loss: 1.0193571322467698  Acc: 70.15687250996017\n",
            "300/391  Loss: 1.0323475264631632  Acc: 69.88164451827242\n",
            "350/391  Loss: 1.0417969719297187  Acc: 69.55350783475784\n",
            "0/79  Loss: 1.375523328781128  Acc: 60.9375\n",
            "50/79  Loss: 1.678149702502232  Acc: 55.759803921568626\n",
            "Saving..\n",
            "Train Accuracy:   69.374 %    Test Accuracy:   55.56 %\n",
            "\n",
            "Epoch: 109\n",
            "0/391  Loss: 1.0722389221191406  Acc: 67.1875\n",
            "50/391  Loss: 0.9681260667595208  Acc: 71.20098039215686\n",
            "100/391  Loss: 0.9770511872697585  Acc: 71.20977722772277\n",
            "150/391  Loss: 0.979742367536027  Acc: 70.91783940397352\n",
            "200/391  Loss: 0.9814487547423709  Acc: 70.98880597014926\n",
            "250/391  Loss: 0.994978161922014  Acc: 70.61752988047809\n",
            "300/391  Loss: 1.0094483209606817  Acc: 70.24242109634551\n",
            "350/391  Loss: 1.020737013416073  Acc: 69.8962784900285\n",
            "0/79  Loss: 1.471112847328186  Acc: 60.9375\n",
            "50/79  Loss: 1.7179938040527643  Acc: 55.3921568627451\n",
            "Train Accuracy:   69.59 %    Test Accuracy:   55.56 %\n",
            "\n",
            "Epoch: 110\n",
            "0/391  Loss: 1.0634021759033203  Acc: 69.53125\n",
            "50/391  Loss: 0.9253461407680138  Acc: 73.5140931372549\n",
            "100/391  Loss: 0.9382847346881829  Acc: 72.57889851485149\n",
            "150/391  Loss: 0.9499324299641793  Acc: 72.12851821192054\n",
            "200/391  Loss: 0.9690328880922118  Acc: 71.49409203980099\n",
            "250/391  Loss: 0.987763822791111  Acc: 70.83540836653387\n",
            "300/391  Loss: 1.000472526019594  Acc: 70.53571428571429\n",
            "350/391  Loss: 1.0104478404053256  Acc: 70.16114672364672\n",
            "0/79  Loss: 1.3393014669418335  Acc: 64.0625\n",
            "50/79  Loss: 1.6982178127064425  Acc: 55.07046568627451\n",
            "Train Accuracy:   69.978 %    Test Accuracy:   55.56 %\n",
            "\n",
            "Epoch: 111\n",
            "0/391  Loss: 0.9075690507888794  Acc: 73.4375\n",
            "50/391  Loss: 0.9097691821117028  Acc: 73.03921568627452\n",
            "100/391  Loss: 0.9160184335000444  Acc: 73.12809405940594\n",
            "150/391  Loss: 0.9306660049798473  Acc: 72.4699917218543\n",
            "200/391  Loss: 0.9408085482630564  Acc: 72.02269900497512\n",
            "250/391  Loss: 0.9554591694201131  Acc: 71.65712151394422\n",
            "300/391  Loss: 0.9688548697189635  Acc: 71.16123338870432\n",
            "350/391  Loss: 0.9872526499280903  Acc: 70.71314102564102\n",
            "0/79  Loss: 1.4494566917419434  Acc: 64.0625\n",
            "50/79  Loss: 1.7530280842500574  Acc: 54.10539215686274\n",
            "Train Accuracy:   70.416 %    Test Accuracy:   55.56 %\n",
            "\n",
            "Epoch: 112\n",
            "0/391  Loss: 0.9363225698471069  Acc: 72.65625\n",
            "50/391  Loss: 0.9189558075923546  Acc: 72.39583333333333\n",
            "100/391  Loss: 0.9274851266700442  Acc: 72.0993193069307\n",
            "150/391  Loss: 0.9340239439579036  Acc: 72.09747516556291\n",
            "200/391  Loss: 0.9474417018653148  Acc: 71.77782960199005\n",
            "250/391  Loss: 0.95639220533143  Acc: 71.58864541832669\n",
            "300/391  Loss: 0.9707760998973023  Acc: 71.1015365448505\n",
            "350/391  Loss: 0.9777666055239164  Acc: 70.96465455840456\n",
            "0/79  Loss: 1.4036065340042114  Acc: 61.71875\n",
            "50/79  Loss: 1.628030258066514  Acc: 56.67892156862745\n",
            "Saving..\n",
            "Train Accuracy:   70.824 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 113\n",
            "0/391  Loss: 0.8480409383773804  Acc: 79.6875\n",
            "50/391  Loss: 0.89787178179797  Acc: 73.85110294117646\n",
            "100/391  Loss: 0.8870777115963473  Acc: 73.63861386138613\n",
            "150/391  Loss: 0.9082600000678309  Acc: 72.82698675496688\n",
            "200/391  Loss: 0.914710868650408  Acc: 72.77285447761194\n",
            "250/391  Loss: 0.9275403279232314  Acc: 72.57843625498008\n",
            "300/391  Loss: 0.943085849879192  Acc: 72.14493355481727\n",
            "350/391  Loss: 0.9550823574392204  Acc: 71.66800213675214\n",
            "0/79  Loss: 1.5696133375167847  Acc: 59.375\n",
            "50/79  Loss: 1.6551945139380062  Acc: 56.03553921568628\n",
            "Train Accuracy:   71.444 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 114\n",
            "0/391  Loss: 0.9212148189544678  Acc: 69.53125\n",
            "50/391  Loss: 0.8713799270929075  Acc: 74.61703431372548\n",
            "100/391  Loss: 0.8851219645821222  Acc: 73.75464108910892\n",
            "150/391  Loss: 0.8965161099339163  Acc: 73.35471854304636\n",
            "200/391  Loss: 0.9082060156770013  Acc: 73.10712064676616\n",
            "250/391  Loss: 0.9166694967395281  Acc: 72.88657868525897\n",
            "300/391  Loss: 0.9288033977695478  Acc: 72.5264742524917\n",
            "350/391  Loss: 0.9395377056890744  Acc: 72.15322293447294\n",
            "0/79  Loss: 1.525407075881958  Acc: 60.15625\n",
            "50/79  Loss: 1.6995345167085236  Acc: 55.80575980392157\n",
            "Train Accuracy:   71.868 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 115\n",
            "0/391  Loss: 0.6718471646308899  Acc: 78.90625\n",
            "50/391  Loss: 0.8429051497403313  Acc: 74.80085784313725\n",
            "100/391  Loss: 0.8642385926577124  Acc: 74.17233910891089\n",
            "150/391  Loss: 0.8678939614864375  Acc: 74.18770695364239\n",
            "200/391  Loss: 0.882036043340294  Acc: 73.75233208955224\n",
            "250/391  Loss: 0.8919896440676959  Acc: 73.48107569721115\n",
            "300/391  Loss: 0.9065742270891056  Acc: 73.0092400332226\n",
            "350/391  Loss: 0.922267633455771  Acc: 72.56499287749288\n",
            "0/79  Loss: 1.6009830236434937  Acc: 60.15625\n",
            "50/79  Loss: 1.6774469917895747  Acc: 56.41850490196079\n",
            "Train Accuracy:   72.284 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 116\n",
            "0/391  Loss: 0.8735471367835999  Acc: 77.34375\n",
            "50/391  Loss: 0.8281680322160908  Acc: 75.29105392156863\n",
            "100/391  Loss: 0.816675335463911  Acc: 76.0055693069307\n",
            "150/391  Loss: 0.8376033625855351  Acc: 75.09830298013244\n",
            "200/391  Loss: 0.8611991871055679  Acc: 74.49082711442786\n",
            "250/391  Loss: 0.8845119037001257  Acc: 73.64604083665338\n",
            "300/391  Loss: 0.9007054187134651  Acc: 73.24543189368771\n",
            "350/391  Loss: 0.914424044114572  Acc: 72.78534544159544\n",
            "0/79  Loss: 1.5123108625411987  Acc: 60.15625\n",
            "50/79  Loss: 1.6778516255173028  Acc: 55.974264705882355\n",
            "Train Accuracy:   72.78 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 117\n",
            "0/391  Loss: 0.9005365371704102  Acc: 73.4375\n",
            "50/391  Loss: 0.8148911829088249  Acc: 75.94975490196079\n",
            "100/391  Loss: 0.8272988389034083  Acc: 75.40996287128714\n",
            "150/391  Loss: 0.8421339965024531  Acc: 74.87065397350993\n",
            "200/391  Loss: 0.8533558534152472  Acc: 74.4286380597015\n",
            "250/391  Loss: 0.8652645627340948  Acc: 73.99153386454184\n",
            "300/391  Loss: 0.8744756017016414  Acc: 73.86316445182725\n",
            "350/391  Loss: 0.8872575371014086  Acc: 73.4775641025641\n",
            "0/79  Loss: 1.3622968196868896  Acc: 63.28125\n",
            "50/79  Loss: 1.671874018276439  Acc: 56.0202205882353\n",
            "Train Accuracy:   73.258 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 118\n",
            "0/391  Loss: 0.6464517116546631  Acc: 75.0\n",
            "50/391  Loss: 0.8006746172904968  Acc: 76.05698529411765\n",
            "100/391  Loss: 0.8073896222775525  Acc: 75.48731435643565\n",
            "150/391  Loss: 0.814809008544644  Acc: 75.66225165562913\n",
            "200/391  Loss: 0.8383072789628707  Acc: 75.09328358208955\n",
            "250/391  Loss: 0.8484626424265098  Acc: 74.75410856573706\n",
            "300/391  Loss: 0.8581242274208323  Acc: 74.61067275747509\n",
            "350/391  Loss: 0.8700230045196338  Acc: 74.15865384615384\n",
            "0/79  Loss: 1.619472622871399  Acc: 57.8125\n",
            "50/79  Loss: 1.70777245128856  Acc: 55.606617647058826\n",
            "Train Accuracy:   73.918 %    Test Accuracy:   56.74 %\n",
            "\n",
            "Epoch: 119\n",
            "0/391  Loss: 0.6400148868560791  Acc: 79.6875\n",
            "50/391  Loss: 0.7684891843328289  Acc: 76.74632352941177\n",
            "100/391  Loss: 0.7964800525419783  Acc: 76.15253712871286\n",
            "150/391  Loss: 0.8003651243171944  Acc: 76.02442052980132\n",
            "200/391  Loss: 0.8103211400520742  Acc: 75.75792910447761\n",
            "250/391  Loss: 0.8263196679225481  Acc: 75.16496513944223\n",
            "300/391  Loss: 0.8337581264616247  Acc: 74.92992109634551\n",
            "350/391  Loss: 0.8462147695726139  Acc: 74.57264957264957\n",
            "0/79  Loss: 1.3612951040267944  Acc: 61.71875\n",
            "50/79  Loss: 1.628743195066265  Acc: 56.740196078431374\n",
            "Saving..\n",
            "Train Accuracy:   74.364 %    Test Accuracy:   56.99 %\n",
            "\n",
            "Epoch: 120\n",
            "0/391  Loss: 0.6161167621612549  Acc: 81.25\n",
            "50/391  Loss: 0.7720181158944672  Acc: 77.11397058823529\n",
            "100/391  Loss: 0.7687860455843482  Acc: 77.32827970297029\n",
            "150/391  Loss: 0.7802884795807845  Acc: 76.89879966887418\n",
            "200/391  Loss: 0.7968809770707467  Acc: 76.22823383084577\n",
            "250/391  Loss: 0.8072871470831305  Acc: 75.79370019920319\n",
            "300/391  Loss: 0.8211483747460121  Acc: 75.38673172757476\n",
            "350/391  Loss: 0.8364701715969293  Acc: 74.97551638176638\n",
            "0/79  Loss: 1.3054113388061523  Acc: 63.28125\n",
            "50/79  Loss: 1.659190341538074  Acc: 57.123161764705884\n",
            "Saving..\n",
            "Train Accuracy:   74.742 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 121\n",
            "0/391  Loss: 0.7385408878326416  Acc: 77.34375\n",
            "50/391  Loss: 0.7394631125178992  Acc: 78.32414215686275\n",
            "100/391  Loss: 0.7412509006438869  Acc: 78.27196782178218\n",
            "150/391  Loss: 0.7598684793276502  Acc: 77.49896523178808\n",
            "200/391  Loss: 0.775452949810977  Acc: 76.87733208955224\n",
            "250/391  Loss: 0.7886110171378846  Acc: 76.39442231075697\n",
            "300/391  Loss: 0.8027380790029254  Acc: 75.93698089700996\n",
            "350/391  Loss: 0.8159973149286036  Acc: 75.61209045584046\n",
            "0/79  Loss: 1.5250747203826904  Acc: 59.375\n",
            "50/79  Loss: 1.7530203122718662  Acc: 54.85600490196079\n",
            "Train Accuracy:   75.292 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 122\n",
            "0/391  Loss: 0.7937169075012207  Acc: 74.21875\n",
            "50/391  Loss: 0.7242704384467181  Acc: 78.96752450980392\n",
            "100/391  Loss: 0.7304004513391174  Acc: 78.75154702970298\n",
            "150/391  Loss: 0.7360943686093716  Acc: 78.42508278145695\n",
            "200/391  Loss: 0.7535943166533513  Acc: 77.92288557213931\n",
            "250/391  Loss: 0.7720302067904833  Acc: 77.22858565737052\n",
            "300/391  Loss: 0.7878237230041099  Acc: 76.67929817275747\n",
            "350/391  Loss: 0.7976351942431893  Acc: 76.31988960113961\n",
            "0/79  Loss: 1.4123955965042114  Acc: 61.71875\n",
            "50/79  Loss: 1.7561850968529196  Acc: 55.346200980392155\n",
            "Train Accuracy:   76.042 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 123\n",
            "0/391  Loss: 0.7235987186431885  Acc: 78.90625\n",
            "50/391  Loss: 0.711827952487796  Acc: 78.56924019607843\n",
            "100/391  Loss: 0.7326591463372258  Acc: 77.84653465346534\n",
            "150/391  Loss: 0.7405897126292551  Acc: 78.01634933774834\n",
            "200/391  Loss: 0.7516899099990503  Acc: 77.62748756218906\n",
            "250/391  Loss: 0.7607015962619705  Acc: 77.31884960159363\n",
            "300/391  Loss: 0.7679251826482754  Acc: 77.03488372093024\n",
            "350/391  Loss: 0.7777490661694453  Acc: 76.73611111111111\n",
            "0/79  Loss: 1.5029375553131104  Acc: 57.8125\n",
            "50/79  Loss: 1.7087646816291062  Acc: 56.25\n",
            "Train Accuracy:   76.502 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 124\n",
            "0/391  Loss: 0.8017132878303528  Acc: 75.78125\n",
            "50/391  Loss: 0.6960076491038004  Acc: 79.64154411764706\n",
            "100/391  Loss: 0.6924355720529461  Acc: 79.82673267326733\n",
            "150/391  Loss: 0.7162908063029612  Acc: 79.06663907284768\n",
            "200/391  Loss: 0.7291923194975403  Acc: 78.54477611940298\n",
            "250/391  Loss: 0.7388829425036669  Acc: 78.19036354581674\n",
            "300/391  Loss: 0.7480906941169916  Acc: 77.93552740863787\n",
            "350/391  Loss: 0.7557770674384897  Acc: 77.6397792022792\n",
            "0/79  Loss: 1.5778969526290894  Acc: 59.375\n",
            "50/79  Loss: 1.699779655419144  Acc: 56.127450980392155\n",
            "Train Accuracy:   77.27 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 125\n",
            "0/391  Loss: 0.6984390020370483  Acc: 79.6875\n",
            "50/391  Loss: 0.6645111780540616  Acc: 80.5453431372549\n",
            "100/391  Loss: 0.6807477745679346  Acc: 79.74164603960396\n",
            "150/391  Loss: 0.6870556860570086  Acc: 79.40811258278146\n",
            "200/391  Loss: 0.7061542871579602  Acc: 78.74300373134328\n",
            "250/391  Loss: 0.7140560456481113  Acc: 78.55453187250995\n",
            "300/391  Loss: 0.726915665045133  Acc: 78.17950581395348\n",
            "350/391  Loss: 0.7367830357999883  Acc: 77.9380341880342\n",
            "0/79  Loss: 1.4460399150848389  Acc: 64.0625\n",
            "50/79  Loss: 1.679371106858347  Acc: 57.00061274509804\n",
            "Train Accuracy:   77.672 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 126\n",
            "0/391  Loss: 0.6114567518234253  Acc: 82.8125\n",
            "50/391  Loss: 0.6744936611138138  Acc: 80.49938725490196\n",
            "100/391  Loss: 0.6766237179831703  Acc: 80.32951732673267\n",
            "150/391  Loss: 0.6739654691014069  Acc: 80.24110099337749\n",
            "200/391  Loss: 0.6872864350750671  Acc: 79.76134950248756\n",
            "250/391  Loss: 0.7003733971679353  Acc: 79.27664342629483\n",
            "300/391  Loss: 0.7142461368015834  Acc: 78.85174418604652\n",
            "350/391  Loss: 0.7238541160553609  Acc: 78.4477386039886\n",
            "0/79  Loss: 1.4541528224945068  Acc: 64.0625\n",
            "50/79  Loss: 1.7296146972506654  Acc: 55.943627450980394\n",
            "Train Accuracy:   78.236 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 127\n",
            "0/391  Loss: 0.6540606021881104  Acc: 81.25\n",
            "50/391  Loss: 0.6551021857588899  Acc: 81.20404411764706\n",
            "100/391  Loss: 0.6532456567972014  Acc: 80.76268564356435\n",
            "150/391  Loss: 0.6534066737092883  Acc: 80.55670529801324\n",
            "200/391  Loss: 0.662795679486213  Acc: 80.19667288557214\n",
            "250/391  Loss: 0.6739714152784462  Acc: 79.72173804780877\n",
            "300/391  Loss: 0.6916491965518837  Acc: 79.11648671096346\n",
            "350/391  Loss: 0.7010477857711988  Acc: 78.83279914529915\n",
            "0/79  Loss: 1.4935638904571533  Acc: 57.03125\n",
            "50/79  Loss: 1.7108652685202805  Acc: 56.3265931372549\n",
            "Train Accuracy:   78.548 %    Test Accuracy:   57.25 %\n",
            "\n",
            "Epoch: 128\n",
            "0/391  Loss: 0.5855384469032288  Acc: 80.46875\n",
            "50/391  Loss: 0.6220363285027298  Acc: 81.83210784313725\n",
            "100/391  Loss: 0.6264853200109879  Acc: 81.48205445544555\n",
            "150/391  Loss: 0.6339980906603352  Acc: 81.2551738410596\n",
            "200/391  Loss: 0.6477856428468999  Acc: 80.81078980099502\n",
            "250/391  Loss: 0.6619813736691418  Acc: 80.22908366533865\n",
            "300/391  Loss: 0.6751220905503561  Acc: 79.8094892026578\n",
            "350/391  Loss: 0.6837234821414676  Acc: 79.60514601139602\n",
            "0/79  Loss: 1.3801343441009521  Acc: 62.5\n",
            "50/79  Loss: 1.6811857831244374  Acc: 57.41421568627451\n",
            "Saving..\n",
            "Train Accuracy:   79.444 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 129\n",
            "0/391  Loss: 0.5587363243103027  Acc: 84.375\n",
            "50/391  Loss: 0.5963817209589715  Acc: 82.64399509803921\n",
            "100/391  Loss: 0.5972073989929539  Acc: 82.48762376237623\n",
            "150/391  Loss: 0.6097751378223596  Acc: 81.89155629139073\n",
            "200/391  Loss: 0.6157368058292427  Acc: 81.66200248756219\n",
            "250/391  Loss: 0.6294878461208951  Acc: 81.2468874501992\n",
            "300/391  Loss: 0.6379750104639618  Acc: 80.98525747508306\n",
            "350/391  Loss: 0.6511115719619979  Acc: 80.55778133903134\n",
            "0/79  Loss: 1.380355954170227  Acc: 60.15625\n",
            "50/79  Loss: 1.7264176116270178  Acc: 56.57169117647059\n",
            "Train Accuracy:   80.332 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 130\n",
            "0/391  Loss: 0.6523743867874146  Acc: 82.03125\n",
            "50/391  Loss: 0.5983820160230001  Acc: 82.33762254901961\n",
            "100/391  Loss: 0.5946525078598816  Acc: 82.36386138613861\n",
            "150/391  Loss: 0.6006314101203388  Acc: 82.2123344370861\n",
            "200/391  Loss: 0.6153884810298237  Acc: 81.65034203980099\n",
            "250/391  Loss: 0.6242244557080516  Acc: 81.37761454183267\n",
            "300/391  Loss: 0.6314977260919108  Acc: 81.15137043189368\n",
            "350/391  Loss: 0.6414689805432942  Acc: 80.81597222222223\n",
            "0/79  Loss: 1.5863232612609863  Acc: 61.71875\n",
            "50/79  Loss: 1.7280309363907458  Acc: 57.138480392156865\n",
            "Train Accuracy:   80.574 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 131\n",
            "0/391  Loss: 0.5102906227111816  Acc: 89.84375\n",
            "50/391  Loss: 0.5866787001198414  Acc: 83.19546568627452\n",
            "100/391  Loss: 0.5734207538684996  Acc: 83.38490099009901\n",
            "150/391  Loss: 0.581861853402182  Acc: 83.12293046357615\n",
            "200/391  Loss: 0.5890055985296544  Acc: 82.77751865671642\n",
            "250/391  Loss: 0.6013865349777191  Acc: 82.30204183266932\n",
            "300/391  Loss: 0.614123167191629  Acc: 81.72757475083057\n",
            "350/391  Loss: 0.6216614528259321  Acc: 81.47480413105413\n",
            "0/79  Loss: 1.460336685180664  Acc: 62.5\n",
            "50/79  Loss: 1.6886293186860926  Acc: 57.5827205882353\n",
            "Train Accuracy:   81.282 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 132\n",
            "0/391  Loss: 0.5742641091346741  Acc: 80.46875\n",
            "50/391  Loss: 0.5292951663335165  Acc: 84.26776960784314\n",
            "100/391  Loss: 0.5470678407957058  Acc: 83.87221534653466\n",
            "150/391  Loss: 0.5561073975057791  Acc: 83.54718543046357\n",
            "200/391  Loss: 0.5655909874545995  Acc: 83.26725746268657\n",
            "250/391  Loss: 0.5761694128057396  Acc: 82.91832669322709\n",
            "300/391  Loss: 0.5896669766831636  Acc: 82.44134136212625\n",
            "350/391  Loss: 0.6003819638677472  Acc: 82.08689458689459\n",
            "0/79  Loss: 1.4728615283966064  Acc: 62.5\n",
            "50/79  Loss: 1.6866680500554103  Acc: 57.138480392156865\n",
            "Train Accuracy:   81.882 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 133\n",
            "0/391  Loss: 0.5150852799415588  Acc: 86.71875\n",
            "50/391  Loss: 0.5191954017854205  Acc: 85.21752450980392\n",
            "100/391  Loss: 0.526140453496782  Acc: 84.72308168316832\n",
            "150/391  Loss: 0.5298707161518122  Acc: 84.60264900662251\n",
            "200/391  Loss: 0.5448241410267294  Acc: 84.09514925373135\n",
            "250/391  Loss: 0.5571368834174487  Acc: 83.65288844621514\n",
            "300/391  Loss: 0.569149353773491  Acc: 83.20182724252491\n",
            "350/391  Loss: 0.57693509600441  Acc: 82.91488603988604\n",
            "0/79  Loss: 1.6025748252868652  Acc: 61.71875\n",
            "50/79  Loss: 1.7283607674579995  Acc: 57.1078431372549\n",
            "Train Accuracy:   82.724 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 134\n",
            "0/391  Loss: 0.42965245246887207  Acc: 90.625\n",
            "50/391  Loss: 0.5166130480813045  Acc: 85.0030637254902\n",
            "100/391  Loss: 0.5205248378881133  Acc: 84.8855198019802\n",
            "150/391  Loss: 0.5302466377122512  Acc: 84.3905215231788\n",
            "200/391  Loss: 0.5418368266589606  Acc: 83.92412935323384\n",
            "250/391  Loss: 0.5473402168646276  Acc: 83.75249003984064\n",
            "300/391  Loss: 0.5583913438145901  Acc: 83.33679401993355\n",
            "350/391  Loss: 0.5652418147634577  Acc: 83.16639957264957\n",
            "0/79  Loss: 1.741964340209961  Acc: 60.9375\n",
            "50/79  Loss: 1.7048008699043125  Acc: 56.64828431372549\n",
            "Train Accuracy:   83.0 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 135\n",
            "0/391  Loss: 0.4365486800670624  Acc: 85.15625\n",
            "50/391  Loss: 0.48730118368186204  Acc: 85.83026960784314\n",
            "100/391  Loss: 0.48519878399254074  Acc: 85.79826732673267\n",
            "150/391  Loss: 0.4936365960844305  Acc: 85.67363410596026\n",
            "200/391  Loss: 0.5010804189971431  Acc: 85.38945895522389\n",
            "250/391  Loss: 0.5096482804334496  Acc: 85.09088645418326\n",
            "300/391  Loss: 0.5239249604484963  Acc: 84.63714700996678\n",
            "350/391  Loss: 0.5332487275627603  Acc: 84.31490384615384\n",
            "0/79  Loss: 1.449014663696289  Acc: 61.71875\n",
            "50/79  Loss: 1.7411528381646848  Acc: 57.49080882352941\n",
            "Train Accuracy:   83.978 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 136\n",
            "0/391  Loss: 0.4079224169254303  Acc: 91.40625\n",
            "50/391  Loss: 0.4618837599660836  Acc: 87.0404411764706\n",
            "100/391  Loss: 0.46712727328338244  Acc: 86.7496905940594\n",
            "150/391  Loss: 0.47705295129327585  Acc: 86.17032284768212\n",
            "200/391  Loss: 0.4872348458316196  Acc: 85.80534825870647\n",
            "250/391  Loss: 0.4990144344677488  Acc: 85.355453187251\n",
            "300/391  Loss: 0.5077095304018635  Acc: 85.07838455149502\n",
            "350/391  Loss: 0.515395042223808  Acc: 84.83128561253561\n",
            "0/79  Loss: 1.6670414209365845  Acc: 60.15625\n",
            "50/79  Loss: 1.7505560318628948  Acc: 57.23039215686274\n",
            "Train Accuracy:   84.568 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 137\n",
            "0/391  Loss: 0.49584853649139404  Acc: 85.15625\n",
            "50/391  Loss: 0.4649559306163414  Acc: 86.45833333333333\n",
            "100/391  Loss: 0.46525901615029513  Acc: 86.60272277227723\n",
            "150/391  Loss: 0.4670907148462258  Acc: 86.47557947019868\n",
            "200/391  Loss: 0.4737544030099366  Acc: 86.26010572139303\n",
            "250/391  Loss: 0.4840631889869492  Acc: 85.76319721115537\n",
            "300/391  Loss: 0.4965433332809182  Acc: 85.40801495016612\n",
            "350/391  Loss: 0.5071740952821878  Acc: 85.0471866096866\n",
            "0/79  Loss: 1.511550784111023  Acc: 63.28125\n",
            "50/79  Loss: 1.7336324406605141  Acc: 57.169117647058826\n",
            "Train Accuracy:   84.786 %    Test Accuracy:   57.71 %\n",
            "\n",
            "Epoch: 138\n",
            "0/391  Loss: 0.5669239163398743  Acc: 83.59375\n",
            "50/391  Loss: 0.4263151533463422  Acc: 87.71446078431373\n",
            "100/391  Loss: 0.4277222929614605  Acc: 87.83261138613861\n",
            "150/391  Loss: 0.4322603629124875  Acc: 87.60347682119205\n",
            "200/391  Loss: 0.44464618026913694  Acc: 87.16573383084577\n",
            "250/391  Loss: 0.4553309125729291  Acc: 86.74987549800797\n",
            "300/391  Loss: 0.4651114084197833  Acc: 86.39431063122923\n",
            "350/391  Loss: 0.4746768197782359  Acc: 86.05546652421653\n",
            "0/79  Loss: 1.5157568454742432  Acc: 62.5\n",
            "50/79  Loss: 1.7241887508654128  Acc: 57.935049019607845\n",
            "Saving..\n",
            "Train Accuracy:   85.868 %    Test Accuracy:   57.85 %\n",
            "\n",
            "Epoch: 139\n",
            "0/391  Loss: 0.4152880012989044  Acc: 87.5\n",
            "50/391  Loss: 0.41232703187886405  Acc: 88.46507352941177\n",
            "100/391  Loss: 0.41817413786850355  Acc: 88.49009900990099\n",
            "150/391  Loss: 0.42649127848890445  Acc: 88.06912251655629\n",
            "200/391  Loss: 0.43824095364233745  Acc: 87.56996268656717\n",
            "250/391  Loss: 0.448406830489398  Acc: 87.23543326693228\n",
            "300/391  Loss: 0.45693184093779504  Acc: 86.87707641196013\n",
            "350/391  Loss: 0.46776111377270474  Acc: 86.46501068376068\n",
            "0/79  Loss: 1.5819718837738037  Acc: 62.5\n",
            "50/79  Loss: 1.7251974998735915  Acc: 57.44485294117647\n",
            "Train Accuracy:   86.186 %    Test Accuracy:   57.85 %\n",
            "\n",
            "Epoch: 140\n",
            "0/391  Loss: 0.32178327441215515  Acc: 91.40625\n",
            "50/391  Loss: 0.40104734839177597  Acc: 88.69485294117646\n",
            "100/391  Loss: 0.40091633354083145  Acc: 88.52103960396039\n",
            "150/391  Loss: 0.4108411703283424  Acc: 88.17259933774834\n",
            "200/391  Loss: 0.4210487451422867  Acc: 87.70600124378109\n",
            "250/391  Loss: 0.42919026522997367  Acc: 87.45019920318725\n",
            "300/391  Loss: 0.4360468171759697  Acc: 87.27159468438538\n",
            "350/391  Loss: 0.44775473935651644  Acc: 86.96581196581197\n",
            "0/79  Loss: 1.4872690439224243  Acc: 65.625\n",
            "50/79  Loss: 1.6980769494000603  Acc: 58.22610294117647\n",
            "Saving..\n",
            "Train Accuracy:   86.8 %    Test Accuracy:   58.78 %\n",
            "\n",
            "Epoch: 141\n",
            "0/391  Loss: 0.36052268743515015  Acc: 89.84375\n",
            "50/391  Loss: 0.3760182062784831  Acc: 89.24632352941177\n",
            "100/391  Loss: 0.38011245149196965  Acc: 89.21720297029702\n",
            "150/391  Loss: 0.3917583789651757  Acc: 88.78311258278146\n",
            "200/391  Loss: 0.40065748507703713  Acc: 88.52223258706468\n",
            "250/391  Loss: 0.4081021200613197  Acc: 88.32793824701196\n",
            "300/391  Loss: 0.41381037086743455  Acc: 88.0813953488372\n",
            "350/391  Loss: 0.42186395021585316  Acc: 87.77377136752136\n",
            "0/79  Loss: 1.5145694017410278  Acc: 61.71875\n",
            "50/79  Loss: 1.7068026907303755  Acc: 58.854166666666664\n",
            "Saving..\n",
            "Train Accuracy:   87.434 %    Test Accuracy:   58.82 %\n",
            "\n",
            "Epoch: 142\n",
            "0/391  Loss: 0.2811250388622284  Acc: 92.1875\n",
            "50/391  Loss: 0.36535026133060455  Acc: 89.67524509803921\n",
            "100/391  Loss: 0.36572700517602486  Acc: 89.64263613861387\n",
            "150/391  Loss: 0.37135029085819293  Acc: 89.53331953642385\n",
            "200/391  Loss: 0.3792660657446183  Acc: 89.2451803482587\n",
            "250/391  Loss: 0.3864811410704457  Acc: 88.95044820717132\n",
            "300/391  Loss: 0.39334549787036605  Acc: 88.73546511627907\n",
            "350/391  Loss: 0.4010717469742495  Acc: 88.48379629629629\n",
            "0/79  Loss: 1.5500118732452393  Acc: 61.71875\n",
            "50/79  Loss: 1.7097152845532286  Acc: 57.96568627450981\n",
            "Train Accuracy:   88.34 %    Test Accuracy:   58.82 %\n",
            "\n",
            "Epoch: 143\n",
            "0/391  Loss: 0.4247928857803345  Acc: 87.5\n",
            "50/391  Loss: 0.35243650978686764  Acc: 90.60968137254902\n",
            "100/391  Loss: 0.35398128215629276  Acc: 90.28465346534654\n",
            "150/391  Loss: 0.3604021942773402  Acc: 90.06105132450331\n",
            "200/391  Loss: 0.36560216768464043  Acc: 89.85541044776119\n",
            "250/391  Loss: 0.3727107025475141  Acc: 89.56984561752988\n",
            "300/391  Loss: 0.37559309631487064  Acc: 89.45961378737542\n",
            "350/391  Loss: 0.38263935147866907  Acc: 89.24724002849003\n",
            "0/79  Loss: 1.4193191528320312  Acc: 64.84375\n",
            "50/79  Loss: 1.7100888794543696  Acc: 58.103553921568626\n",
            "Train Accuracy:   89.06 %    Test Accuracy:   58.82 %\n",
            "\n",
            "Epoch: 144\n",
            "0/391  Loss: 0.35090723633766174  Acc: 88.28125\n",
            "50/391  Loss: 0.32415831994776634  Acc: 91.5594362745098\n",
            "100/391  Loss: 0.3239226291085234  Acc: 91.15872524752476\n",
            "150/391  Loss: 0.3314174358418446  Acc: 91.01821192052981\n",
            "200/391  Loss: 0.3421592817674229  Acc: 90.55503731343283\n",
            "250/391  Loss: 0.34986598498792765  Acc: 90.40712151394422\n",
            "300/391  Loss: 0.35700155373823605  Acc: 90.11108803986711\n",
            "350/391  Loss: 0.36380531322582493  Acc: 89.88603988603988\n",
            "0/79  Loss: 1.5722864866256714  Acc: 64.84375\n",
            "50/79  Loss: 1.7461963915357404  Acc: 57.82781862745098\n",
            "Train Accuracy:   89.66 %    Test Accuracy:   58.82 %\n",
            "\n",
            "Epoch: 145\n",
            "0/391  Loss: 0.33430081605911255  Acc: 92.1875\n",
            "50/391  Loss: 0.33253784653018503  Acc: 91.00796568627452\n",
            "100/391  Loss: 0.3189801092785184  Acc: 91.38304455445545\n",
            "150/391  Loss: 0.32023391275611146  Acc: 91.27690397350993\n",
            "200/391  Loss: 0.3252087191711018  Acc: 91.12251243781094\n",
            "250/391  Loss: 0.3318463971771567  Acc: 90.93002988047809\n",
            "300/391  Loss: 0.3383713872032704  Acc: 90.67691029900332\n",
            "350/391  Loss: 0.34484395321108335  Acc: 90.4358084045584\n",
            "0/79  Loss: 1.5167841911315918  Acc: 64.0625\n",
            "50/79  Loss: 1.7390860295763202  Acc: 58.93075980392157\n",
            "Saving..\n",
            "Train Accuracy:   90.172 %    Test Accuracy:   59.01 %\n",
            "\n",
            "Epoch: 146\n",
            "0/391  Loss: 0.2335493266582489  Acc: 95.3125\n",
            "50/391  Loss: 0.29833585637457233  Acc: 92.17218137254902\n",
            "100/391  Loss: 0.30042616698411434  Acc: 91.94771039603961\n",
            "150/391  Loss: 0.31034848054513237  Acc: 91.61320364238411\n",
            "200/391  Loss: 0.31411210228851183  Acc: 91.49564676616916\n",
            "250/391  Loss: 0.3169281765877963  Acc: 91.34711155378486\n",
            "300/391  Loss: 0.3225816946687096  Acc: 91.13372093023256\n",
            "350/391  Loss: 0.3294757761252232  Acc: 90.8943198005698\n",
            "0/79  Loss: 1.485439658164978  Acc: 60.15625\n",
            "50/79  Loss: 1.731459189863766  Acc: 57.99632352941177\n",
            "Train Accuracy:   90.764 %    Test Accuracy:   59.01 %\n",
            "\n",
            "Epoch: 147\n",
            "0/391  Loss: 0.3321017026901245  Acc: 90.625\n",
            "50/391  Loss: 0.2858578504300585  Acc: 92.8921568627451\n",
            "100/391  Loss: 0.2826506054342383  Acc: 92.81404702970298\n",
            "150/391  Loss: 0.2864051132604776  Acc: 92.62210264900662\n",
            "200/391  Loss: 0.29498039882871047  Acc: 92.17972636815921\n",
            "250/391  Loss: 0.29844347491444817  Acc: 92.06611055776892\n",
            "300/391  Loss: 0.3059903062954297  Acc: 91.76962209302326\n",
            "350/391  Loss: 0.31258227644313097  Acc: 91.47747507122507\n",
            "0/79  Loss: 1.5533560514450073  Acc: 57.8125\n",
            "50/79  Loss: 1.7042842635921402  Acc: 58.716299019607845\n",
            "Saving..\n",
            "Train Accuracy:   91.406 %    Test Accuracy:   59.21 %\n",
            "\n",
            "Epoch: 148\n",
            "0/391  Loss: 0.2550320327281952  Acc: 94.53125\n",
            "50/391  Loss: 0.27499647350872264  Acc: 92.87683823529412\n",
            "100/391  Loss: 0.27664152864772495  Acc: 93.04610148514851\n",
            "150/391  Loss: 0.2804055354058348  Acc: 92.89114238410596\n",
            "200/391  Loss: 0.28289262008904226  Acc: 92.7744092039801\n",
            "250/391  Loss: 0.2874544938484511  Acc: 92.62325697211155\n",
            "300/391  Loss: 0.29420242282837333  Acc: 92.31468023255815\n",
            "350/391  Loss: 0.2998955345731176  Acc: 92.09179131054131\n",
            "0/79  Loss: 1.453292727470398  Acc: 64.0625\n",
            "50/79  Loss: 1.7032382301255768  Acc: 59.083946078431374\n",
            "Train Accuracy:   91.928 %    Test Accuracy:   59.21 %\n",
            "\n",
            "Epoch: 149\n",
            "0/391  Loss: 0.23600563406944275  Acc: 94.53125\n",
            "50/391  Loss: 0.25503261124386506  Acc: 93.47426470588235\n",
            "100/391  Loss: 0.25055256781011526  Acc: 93.5566212871287\n",
            "150/391  Loss: 0.2583266950600984  Acc: 93.43439569536424\n",
            "200/391  Loss: 0.26299421607854945  Acc: 93.27580845771145\n",
            "250/391  Loss: 0.26757246879229984  Acc: 93.12749003984064\n",
            "300/391  Loss: 0.2717522701551748  Acc: 92.95577242524917\n",
            "350/391  Loss: 0.27548392112778125  Acc: 92.84188034188034\n",
            "0/79  Loss: 1.4823336601257324  Acc: 61.71875\n",
            "50/79  Loss: 1.703865273326051  Acc: 58.501838235294116\n",
            "Train Accuracy:   92.716 %    Test Accuracy:   59.21 %\n",
            "\n",
            "Epoch: 150\n",
            "0/391  Loss: 0.22259192168712616  Acc: 92.1875\n",
            "50/391  Loss: 0.24494782791418188  Acc: 93.96446078431373\n",
            "100/391  Loss: 0.24260674003917393  Acc: 94.0207301980198\n",
            "150/391  Loss: 0.2446372208018966  Acc: 93.86382450331126\n",
            "200/391  Loss: 0.25101630500893096  Acc: 93.64505597014926\n",
            "250/391  Loss: 0.2542340808299433  Acc: 93.49788346613546\n",
            "300/391  Loss: 0.25864033594083946  Acc: 93.27761627906976\n",
            "350/391  Loss: 0.26494332383840513  Acc: 93.05778133903134\n",
            "0/79  Loss: 1.3709648847579956  Acc: 64.84375\n",
            "50/79  Loss: 1.7057720025380452  Acc: 59.620098039215684\n",
            "Saving..\n",
            "Train Accuracy:   92.858 %    Test Accuracy:   59.64 %\n",
            "\n",
            "Epoch: 151\n",
            "0/391  Loss: 0.21506740152835846  Acc: 95.3125\n",
            "50/391  Loss: 0.23596761653236314  Acc: 94.14828431372548\n",
            "100/391  Loss: 0.2310857470377837  Acc: 94.43842821782178\n",
            "150/391  Loss: 0.23067654718626415  Acc: 94.36568708609272\n",
            "200/391  Loss: 0.23499573922868985  Acc: 94.23973880597015\n",
            "250/391  Loss: 0.24151956946013933  Acc: 93.98966633466135\n",
            "300/391  Loss: 0.24703393277535804  Acc: 93.79931478405315\n",
            "350/391  Loss: 0.2506972241435635  Acc: 93.64983974358974\n",
            "0/79  Loss: 1.4327138662338257  Acc: 64.84375\n",
            "50/79  Loss: 1.7163129717695946  Acc: 59.666053921568626\n",
            "Saving..\n",
            "Train Accuracy:   93.512 %    Test Accuracy:   59.99 %\n",
            "\n",
            "Epoch: 152\n",
            "0/391  Loss: 0.1960728019475937  Acc: 96.875\n",
            "50/391  Loss: 0.21754911980208227  Acc: 94.83762254901961\n",
            "100/391  Loss: 0.2166392186490616  Acc: 94.83292079207921\n",
            "150/391  Loss: 0.21846494236529268  Acc: 94.75889900662251\n",
            "200/391  Loss: 0.21796511210019315  Acc: 94.78000621890547\n",
            "250/391  Loss: 0.22378430013875086  Acc: 94.64018924302789\n",
            "300/391  Loss: 0.2260921788572077  Acc: 94.60132890365449\n",
            "350/391  Loss: 0.23158847623401219  Acc: 94.37989672364672\n",
            "0/79  Loss: 1.5260531902313232  Acc: 66.40625\n",
            "50/79  Loss: 1.729322643841014  Acc: 58.93075980392157\n",
            "Train Accuracy:   94.212 %    Test Accuracy:   59.99 %\n",
            "\n",
            "Epoch: 153\n",
            "0/391  Loss: 0.2040610909461975  Acc: 93.75\n",
            "50/391  Loss: 0.21241165025561465  Acc: 95.17463235294117\n",
            "100/391  Loss: 0.21205226449978234  Acc: 95.09591584158416\n",
            "150/391  Loss: 0.21022779257684354  Acc: 95.1210678807947\n",
            "200/391  Loss: 0.21162782101637095  Acc: 95.08706467661692\n",
            "250/391  Loss: 0.21363225938433195  Acc: 94.94833167330677\n",
            "300/391  Loss: 0.21692965036115774  Acc: 94.78301495016612\n",
            "350/391  Loss: 0.22191701135319522  Acc: 94.62250712250712\n",
            "0/79  Loss: 1.5313702821731567  Acc: 61.71875\n",
            "50/79  Loss: 1.7204327863805435  Acc: 59.466911764705884\n",
            "Train Accuracy:   94.504 %    Test Accuracy:   59.99 %\n",
            "\n",
            "Epoch: 154\n",
            "0/391  Loss: 0.10635114461183548  Acc: 99.21875\n",
            "50/391  Loss: 0.19004981599602044  Acc: 95.75674019607843\n",
            "100/391  Loss: 0.1968468242500088  Acc: 95.43626237623762\n",
            "150/391  Loss: 0.20006138051779854  Acc: 95.3331953642384\n",
            "200/391  Loss: 0.20387172524757052  Acc: 95.22310323383084\n",
            "250/391  Loss: 0.20597460696777975  Acc: 95.18799800796813\n",
            "300/391  Loss: 0.20755795107232375  Acc: 95.11783637873754\n",
            "350/391  Loss: 0.20896934182980123  Acc: 95.0298254985755\n",
            "0/79  Loss: 1.5505917072296143  Acc: 60.9375\n",
            "50/79  Loss: 1.7235681917153152  Acc: 58.93075980392157\n",
            "Train Accuracy:   94.948 %    Test Accuracy:   59.99 %\n",
            "\n",
            "Epoch: 155\n",
            "0/391  Loss: 0.16815127432346344  Acc: 96.875\n",
            "50/391  Loss: 0.17827279076856725  Acc: 96.06311274509804\n",
            "100/391  Loss: 0.17845766216811568  Acc: 96.00866336633663\n",
            "150/391  Loss: 0.18145107127578053  Acc: 95.9126655629139\n",
            "200/391  Loss: 0.1861322281965569  Acc: 95.72838930348259\n",
            "250/391  Loss: 0.1899207076465941  Acc: 95.55527888446215\n",
            "300/391  Loss: 0.19280947749796895  Acc: 95.46044435215947\n",
            "350/391  Loss: 0.19603584948767963  Acc: 95.35033831908832\n",
            "0/79  Loss: 1.5608863830566406  Acc: 63.28125\n",
            "50/79  Loss: 1.7012008615568572  Acc: 59.81924019607843\n",
            "Train Accuracy:   95.216 %    Test Accuracy:   59.99 %\n",
            "\n",
            "Epoch: 156\n",
            "0/391  Loss: 0.16736558079719543  Acc: 96.875\n",
            "50/391  Loss: 0.1698978335834017  Acc: 96.62990196078431\n",
            "100/391  Loss: 0.1662170647984684  Acc: 96.75123762376238\n",
            "150/391  Loss: 0.1642777274874662  Acc: 96.6732201986755\n",
            "200/391  Loss: 0.1693877183382784  Acc: 96.50575248756219\n",
            "250/391  Loss: 0.17538621312237357  Acc: 96.21202689243027\n",
            "300/391  Loss: 0.17784008797121997  Acc: 96.140469269103\n",
            "350/391  Loss: 0.1808499765404609  Acc: 96.02029914529915\n",
            "0/79  Loss: 1.5562732219696045  Acc: 67.1875\n",
            "50/79  Loss: 1.7013636640473908  Acc: 60.30943627450981\n",
            "Saving..\n",
            "Train Accuracy:   95.958 %    Test Accuracy:   60.23 %\n",
            "\n",
            "Epoch: 157\n",
            "0/391  Loss: 0.12909738719463348  Acc: 97.65625\n",
            "50/391  Loss: 0.15740418872412512  Acc: 96.69117647058823\n",
            "100/391  Loss: 0.16183856328820237  Acc: 96.57332920792079\n",
            "150/391  Loss: 0.16059319793388543  Acc: 96.62148178807946\n",
            "200/391  Loss: 0.16301754969566024  Acc: 96.55628109452736\n",
            "250/391  Loss: 0.164582128959348  Acc: 96.4797061752988\n",
            "300/391  Loss: 0.1676127965317612  Acc: 96.38963870431894\n",
            "350/391  Loss: 0.17119462261682222  Acc: 96.23397435897436\n",
            "0/79  Loss: 1.4102758169174194  Acc: 65.625\n",
            "50/79  Loss: 1.7124683202481736  Acc: 59.512867647058826\n",
            "Train Accuracy:   96.11 %    Test Accuracy:   60.23 %\n",
            "\n",
            "Epoch: 158\n",
            "0/391  Loss: 0.1376086175441742  Acc: 96.09375\n",
            "50/391  Loss: 0.14790159025613  Acc: 97.01286764705883\n",
            "100/391  Loss: 0.1456063920908635  Acc: 97.04517326732673\n",
            "150/391  Loss: 0.14939796623607346  Acc: 96.875\n",
            "200/391  Loss: 0.15112010697227213  Acc: 96.79337686567165\n",
            "250/391  Loss: 0.15341181824525513  Acc: 96.71937250996017\n",
            "300/391  Loss: 0.15603734770684544  Acc: 96.64140365448505\n",
            "350/391  Loss: 0.15851733139437488  Acc: 96.59009971509971\n",
            "0/79  Loss: 1.4810521602630615  Acc: 62.5\n",
            "50/79  Loss: 1.6764728298374252  Acc: 60.56985294117647\n",
            "Saving..\n",
            "Train Accuracy:   96.54 %    Test Accuracy:   60.6 %\n",
            "\n",
            "Epoch: 159\n",
            "0/391  Loss: 0.14710785448551178  Acc: 97.65625\n",
            "50/391  Loss: 0.13907810826511943  Acc: 97.10477941176471\n",
            "100/391  Loss: 0.13913208142955705  Acc: 97.15346534653466\n",
            "150/391  Loss: 0.1388565381335107  Acc: 97.16473509933775\n",
            "200/391  Loss: 0.13949165233776936  Acc: 97.17817164179104\n",
            "250/391  Loss: 0.14272036317335182  Acc: 97.10532868525897\n",
            "300/391  Loss: 0.14507057342022361  Acc: 97.0047757475083\n",
            "350/391  Loss: 0.14719217361887635  Acc: 96.95735398860398\n",
            "0/79  Loss: 1.4323362112045288  Acc: 65.625\n",
            "50/79  Loss: 1.697090784708659  Acc: 59.880514705882355\n",
            "Train Accuracy:   96.948 %    Test Accuracy:   60.6 %\n",
            "\n",
            "Epoch: 160\n",
            "0/391  Loss: 0.13862600922584534  Acc: 96.09375\n",
            "50/391  Loss: 0.13787348889837078  Acc: 97.28860294117646\n",
            "100/391  Loss: 0.13361452601038584  Acc: 97.33910891089108\n",
            "150/391  Loss: 0.1333981663580762  Acc: 97.34064569536424\n",
            "200/391  Loss: 0.13185129018120506  Acc: 97.42692786069652\n",
            "250/391  Loss: 0.13419886874368941  Acc: 97.37923306772909\n",
            "300/391  Loss: 0.13578926633263744  Acc: 97.36295681063123\n",
            "350/391  Loss: 0.13830508066718056  Acc: 97.25560897435898\n",
            "0/79  Loss: 1.5592682361602783  Acc: 63.28125\n",
            "50/79  Loss: 1.6644597450892131  Acc: 61.10600490196079\n",
            "Saving..\n",
            "Train Accuracy:   97.2 %    Test Accuracy:   60.95 %\n",
            "\n",
            "Epoch: 161\n",
            "0/391  Loss: 0.09482012689113617  Acc: 96.875\n",
            "50/391  Loss: 0.11617363112814286  Acc: 97.68688725490196\n",
            "100/391  Loss: 0.1194163248828142  Acc: 97.64851485148515\n",
            "150/391  Loss: 0.12087616225741557  Acc: 97.6407284768212\n",
            "200/391  Loss: 0.12278453853741214  Acc: 97.55907960199005\n",
            "250/391  Loss: 0.12478101476136431  Acc: 97.51929780876495\n",
            "300/391  Loss: 0.1269322556267149  Acc: 97.50051910299004\n",
            "350/391  Loss: 0.1283650613068855  Acc: 97.48486467236468\n",
            "0/79  Loss: 1.4984842538833618  Acc: 66.40625\n",
            "50/79  Loss: 1.6939023639641555  Acc: 60.47794117647059\n",
            "Train Accuracy:   97.43 %    Test Accuracy:   60.95 %\n",
            "\n",
            "Epoch: 162\n",
            "0/391  Loss: 0.09891973435878754  Acc: 98.4375\n",
            "50/391  Loss: 0.11930816635197285  Acc: 97.74816176470588\n",
            "100/391  Loss: 0.11617078484580068  Acc: 97.87283415841584\n",
            "150/391  Loss: 0.11602471240901  Acc: 97.8580298013245\n",
            "200/391  Loss: 0.11731870490964966  Acc: 97.78840174129353\n",
            "250/391  Loss: 0.11697827635770774  Acc: 97.82121513944223\n",
            "300/391  Loss: 0.11812813018148524  Acc: 97.81198089700996\n",
            "350/391  Loss: 0.1193216945922952  Acc: 97.83431267806267\n",
            "0/79  Loss: 1.4312756061553955  Acc: 67.96875\n",
            "50/79  Loss: 1.6756658109964109  Acc: 60.8609068627451\n",
            "Saving..\n",
            "Train Accuracy:   97.806 %    Test Accuracy:   61.04 %\n",
            "\n",
            "Epoch: 163\n",
            "0/391  Loss: 0.12046917527914047  Acc: 96.09375\n",
            "50/391  Loss: 0.11280799233445934  Acc: 97.85539215686275\n",
            "100/391  Loss: 0.10986920158461769  Acc: 98.01980198019803\n",
            "150/391  Loss: 0.10863419612314527  Acc: 98.07015728476821\n",
            "200/391  Loss: 0.10925204110382801  Acc: 98.08379975124379\n",
            "250/391  Loss: 0.10889277473626384  Acc: 98.12313247011951\n",
            "300/391  Loss: 0.10871889125370109  Acc: 98.13642026578073\n",
            "350/391  Loss: 0.10929832454675283  Acc: 98.13034188034187\n",
            "0/79  Loss: 1.4746856689453125  Acc: 66.40625\n",
            "50/79  Loss: 1.6682606397890578  Acc: 60.61580882352941\n",
            "Train Accuracy:   98.116 %    Test Accuracy:   61.04 %\n",
            "\n",
            "Epoch: 164\n",
            "0/391  Loss: 0.10169791430234909  Acc: 98.4375\n",
            "50/391  Loss: 0.10273151800912969  Acc: 98.23835784313725\n",
            "100/391  Loss: 0.09910216182470322  Acc: 98.36014851485149\n",
            "150/391  Loss: 0.09940405530447992  Acc: 98.38058774834437\n",
            "200/391  Loss: 0.09964624855352279  Acc: 98.37919776119404\n",
            "250/391  Loss: 0.10212384758005104  Acc: 98.28809760956176\n",
            "300/391  Loss: 0.10291545246526648  Acc: 98.24802740863787\n",
            "350/391  Loss: 0.1036133434188332  Acc: 98.23272792022792\n",
            "0/79  Loss: 1.4512139558792114  Acc: 66.40625\n",
            "50/79  Loss: 1.655525618908452  Acc: 61.53492647058823\n",
            "Saving..\n",
            "Train Accuracy:   98.232 %    Test Accuracy:   61.56 %\n",
            "\n",
            "Epoch: 165\n",
            "0/391  Loss: 0.08507126569747925  Acc: 98.4375\n",
            "50/391  Loss: 0.08748652201657202  Acc: 98.71323529411765\n",
            "100/391  Loss: 0.08828458466594762  Acc: 98.76237623762377\n",
            "150/391  Loss: 0.08992679439337048  Acc: 98.74793046357615\n",
            "200/391  Loss: 0.0905585679322926  Acc: 98.71735074626865\n",
            "250/391  Loss: 0.0922444728592715  Acc: 98.636703187251\n",
            "300/391  Loss: 0.09255279693443118  Acc: 98.61659053156146\n",
            "350/391  Loss: 0.09335627494926466  Acc: 98.60665954415954\n",
            "0/79  Loss: 1.4764214754104614  Acc: 64.84375\n",
            "50/79  Loss: 1.6509089353037816  Acc: 61.13664215686274\n",
            "Train Accuracy:   98.574 %    Test Accuracy:   61.56 %\n",
            "\n",
            "Epoch: 166\n",
            "0/391  Loss: 0.0885794535279274  Acc: 97.65625\n",
            "50/391  Loss: 0.08208957248750855  Acc: 98.8970588235294\n",
            "100/391  Loss: 0.08401645067157132  Acc: 98.86293316831683\n",
            "150/391  Loss: 0.08423056443598097  Acc: 98.82036423841059\n",
            "200/391  Loss: 0.08613254530216331  Acc: 98.72901119402985\n",
            "250/391  Loss: 0.08740231509049576  Acc: 98.67405378486056\n",
            "300/391  Loss: 0.08754071422490567  Acc: 98.68407392026577\n",
            "350/391  Loss: 0.08928863528022739  Acc: 98.64672364672364\n",
            "0/79  Loss: 1.4714210033416748  Acc: 68.75\n",
            "50/79  Loss: 1.6447587340485816  Acc: 60.830269607843135\n",
            "Train Accuracy:   98.626 %    Test Accuracy:   61.56 %\n",
            "\n",
            "Epoch: 167\n",
            "0/391  Loss: 0.054825082421302795  Acc: 100.0\n",
            "50/391  Loss: 0.078735159016123  Acc: 98.8970588235294\n",
            "100/391  Loss: 0.07909719638600207  Acc: 98.89387376237623\n",
            "150/391  Loss: 0.07844580820952819  Acc: 98.94971026490066\n",
            "200/391  Loss: 0.07876929976231423  Acc: 98.91946517412936\n",
            "250/391  Loss: 0.08033490715273824  Acc: 98.88570717131473\n",
            "300/391  Loss: 0.08145702865432664  Acc: 98.88392857142857\n",
            "350/391  Loss: 0.08211727948737281  Acc: 98.83368945868946\n",
            "0/79  Loss: 1.478793740272522  Acc: 67.1875\n",
            "50/79  Loss: 1.6369986183503096  Acc: 61.10600490196079\n",
            "Train Accuracy:   98.828 %    Test Accuracy:   61.56 %\n",
            "\n",
            "Epoch: 168\n",
            "0/391  Loss: 0.07893001288175583  Acc: 99.21875\n",
            "50/391  Loss: 0.06632571733173202  Acc: 99.24938725490196\n",
            "100/391  Loss: 0.06783719984169054  Acc: 99.28063118811882\n",
            "150/391  Loss: 0.07035471333671879  Acc: 99.14631622516556\n",
            "200/391  Loss: 0.07219778442412467  Acc: 99.12157960199005\n",
            "250/391  Loss: 0.07332710083203012  Acc: 99.10047310756973\n",
            "300/391  Loss: 0.07438078455849739  Acc: 99.10195182724253\n",
            "350/391  Loss: 0.0744881118592034  Acc: 99.12304131054131\n",
            "0/79  Loss: 1.5186954736709595  Acc: 66.40625\n",
            "50/79  Loss: 1.6391047519796036  Acc: 61.27450980392157\n",
            "Saving..\n",
            "Train Accuracy:   99.1 %    Test Accuracy:   61.73 %\n",
            "\n",
            "Epoch: 169\n",
            "0/391  Loss: 0.07235611230134964  Acc: 100.0\n",
            "50/391  Loss: 0.06815947482691091  Acc: 99.09620098039215\n",
            "100/391  Loss: 0.06798782472563263  Acc: 99.16460396039604\n",
            "150/391  Loss: 0.06888471084912091  Acc: 99.11527317880795\n",
            "200/391  Loss: 0.06962508378337272  Acc: 99.06716417910448\n",
            "250/391  Loss: 0.07048715106341469  Acc: 99.05067231075697\n",
            "300/391  Loss: 0.07112794515500433  Acc: 99.05263704318936\n",
            "350/391  Loss: 0.07163013933900415  Acc: 99.05404202279202\n",
            "0/79  Loss: 1.4812153577804565  Acc: 64.84375\n",
            "50/79  Loss: 1.633101645637961  Acc: 61.67279411764706\n",
            "Saving..\n",
            "Train Accuracy:   99.1 %    Test Accuracy:   61.88 %\n",
            "\n",
            "Epoch: 170\n",
            "0/391  Loss: 0.06369251012802124  Acc: 98.4375\n",
            "50/391  Loss: 0.06440928472461653  Acc: 99.15747549019608\n",
            "100/391  Loss: 0.06297055969365162  Acc: 99.2496905940594\n",
            "150/391  Loss: 0.06421199543339921  Acc: 99.2239238410596\n",
            "200/391  Loss: 0.06351908643855088  Acc: 99.31980721393035\n",
            "250/391  Loss: 0.06451947898770946  Acc: 99.29967629482071\n",
            "300/391  Loss: 0.06519899208507071  Acc: 99.265469269103\n",
            "350/391  Loss: 0.065276964803749  Acc: 99.2988782051282\n",
            "0/79  Loss: 1.4262865781784058  Acc: 68.75\n",
            "50/79  Loss: 1.6357941136640661  Acc: 61.35110294117647\n",
            "Train Accuracy:   99.288 %    Test Accuracy:   61.88 %\n",
            "\n",
            "Epoch: 171\n",
            "0/391  Loss: 0.05635897070169449  Acc: 100.0\n",
            "50/391  Loss: 0.05610569248742917  Acc: 99.49448529411765\n",
            "100/391  Loss: 0.05677825111047466  Acc: 99.49721534653466\n",
            "150/391  Loss: 0.057193918276997595  Acc: 99.47744205298014\n",
            "200/391  Loss: 0.05791654125486144  Acc: 99.48694029850746\n",
            "250/391  Loss: 0.058186130434927714  Acc: 99.47709163346613\n",
            "300/391  Loss: 0.05879539957399978  Acc: 99.46532392026577\n",
            "350/391  Loss: 0.05967899055838415  Acc: 99.42574786324786\n",
            "0/79  Loss: 1.5027434825897217  Acc: 65.625\n",
            "50/79  Loss: 1.622901759895624  Acc: 61.39705882352941\n",
            "Train Accuracy:   99.42 %    Test Accuracy:   61.88 %\n",
            "\n",
            "Epoch: 172\n",
            "0/391  Loss: 0.052231453359127045  Acc: 100.0\n",
            "50/391  Loss: 0.05723484211108264  Acc: 99.38725490196079\n",
            "100/391  Loss: 0.05720808093279305  Acc: 99.4121287128713\n",
            "150/391  Loss: 0.05618987322495078  Acc: 99.47226821192054\n",
            "200/391  Loss: 0.05668667727034187  Acc: 99.45584577114428\n",
            "250/391  Loss: 0.05689611049198297  Acc: 99.44907868525897\n",
            "300/391  Loss: 0.0573503443927959  Acc: 99.43936877076412\n",
            "350/391  Loss: 0.05748219584828598  Acc: 99.42352207977208\n",
            "0/79  Loss: 1.4400259256362915  Acc: 67.96875\n",
            "50/79  Loss: 1.6331598828820622  Acc: 61.70343137254902\n",
            "Train Accuracy:   99.434 %    Test Accuracy:   61.88 %\n",
            "\n",
            "Epoch: 173\n",
            "0/391  Loss: 0.0702526643872261  Acc: 99.21875\n",
            "50/391  Loss: 0.05019825441288013  Acc: 99.5404411764706\n",
            "100/391  Loss: 0.050624501845329115  Acc: 99.52815594059406\n",
            "150/391  Loss: 0.05008998545293777  Acc: 99.53435430463576\n",
            "200/391  Loss: 0.050492209136782595  Acc: 99.53746890547264\n",
            "250/391  Loss: 0.05122540028862269  Acc: 99.52377988047809\n",
            "300/391  Loss: 0.05174266659589701  Acc: 99.5016611295681\n",
            "350/391  Loss: 0.05235031487000973  Acc: 99.47916666666667\n",
            "0/79  Loss: 1.4240055084228516  Acc: 66.40625\n",
            "50/79  Loss: 1.6349551911447562  Acc: 61.963848039215684\n",
            "Saving..\n",
            "Train Accuracy:   99.46 %    Test Accuracy:   62.26 %\n",
            "\n",
            "Epoch: 174\n",
            "0/391  Loss: 0.048462286591529846  Acc: 100.0\n",
            "50/391  Loss: 0.048918550761014805  Acc: 99.5404411764706\n",
            "100/391  Loss: 0.04784960015723021  Acc: 99.58230198019803\n",
            "150/391  Loss: 0.04825548194842228  Acc: 99.54470198675497\n",
            "200/391  Loss: 0.048485945975083614  Acc: 99.54135572139303\n",
            "250/391  Loss: 0.0489224958868022  Acc: 99.54868027888446\n",
            "300/391  Loss: 0.04961402256945241  Acc: 99.55616694352159\n",
            "350/391  Loss: 0.049605261433625494  Acc: 99.56152065527066\n",
            "0/79  Loss: 1.5559895038604736  Acc: 64.0625\n",
            "50/79  Loss: 1.629136328603707  Acc: 61.35110294117647\n",
            "Train Accuracy:   99.55 %    Test Accuracy:   62.26 %\n",
            "\n",
            "Epoch: 175\n",
            "0/391  Loss: 0.04188787192106247  Acc: 100.0\n",
            "50/391  Loss: 0.04470494783976499  Acc: 99.60171568627452\n",
            "100/391  Loss: 0.04431065347156312  Acc: 99.62097772277228\n",
            "150/391  Loss: 0.04523443854999858  Acc: 99.59126655629139\n",
            "200/391  Loss: 0.04560502043077305  Acc: 99.6229788557214\n",
            "250/391  Loss: 0.04626759476366034  Acc: 99.60159362549801\n",
            "300/391  Loss: 0.046490572535268886  Acc: 99.60548172757476\n",
            "350/391  Loss: 0.046913386553398564  Acc: 99.60826210826211\n",
            "0/79  Loss: 1.512476921081543  Acc: 65.625\n",
            "50/79  Loss: 1.6099701535468007  Acc: 61.963848039215684\n",
            "Train Accuracy:   99.608 %    Test Accuracy:   62.26 %\n",
            "\n",
            "Epoch: 176\n",
            "0/391  Loss: 0.030545055866241455  Acc: 99.21875\n",
            "50/391  Loss: 0.043667299198169335  Acc: 99.55575980392157\n",
            "100/391  Loss: 0.043419417086066586  Acc: 99.62871287128714\n",
            "150/391  Loss: 0.04368387931180711  Acc: 99.66887417218543\n",
            "200/391  Loss: 0.04390349588472748  Acc: 99.66573383084577\n",
            "250/391  Loss: 0.04444032063729972  Acc: 99.65139442231076\n",
            "300/391  Loss: 0.0442867488708607  Acc: 99.66258305647841\n",
            "350/391  Loss: 0.04487355323004247  Acc: 99.64610042735043\n",
            "0/79  Loss: 1.5491328239440918  Acc: 64.84375\n",
            "50/79  Loss: 1.604561116181168  Acc: 62.07107843137255\n",
            "Train Accuracy:   99.644 %    Test Accuracy:   62.26 %\n",
            "\n",
            "Epoch: 177\n",
            "0/391  Loss: 0.04839968681335449  Acc: 100.0\n",
            "50/391  Loss: 0.04045004930858519  Acc: 99.80085784313725\n",
            "100/391  Loss: 0.03943222220802661  Acc: 99.79888613861387\n",
            "150/391  Loss: 0.03994919254833105  Acc: 99.76200331125828\n",
            "200/391  Loss: 0.04066778217159694  Acc: 99.74347014925372\n",
            "250/391  Loss: 0.040933561017848105  Acc: 99.74477091633466\n",
            "300/391  Loss: 0.041162837661827524  Acc: 99.73266196013289\n",
            "350/391  Loss: 0.04150029233632944  Acc: 99.71064814814815\n",
            "0/79  Loss: 1.4595896005630493  Acc: 67.96875\n",
            "50/79  Loss: 1.6089325722526102  Acc: 62.16299019607843\n",
            "Saving..\n",
            "Train Accuracy:   99.7 %    Test Accuracy:   62.35 %\n",
            "\n",
            "Epoch: 178\n",
            "0/391  Loss: 0.02983725443482399  Acc: 100.0\n",
            "50/391  Loss: 0.03788582549667826  Acc: 99.83149509803921\n",
            "100/391  Loss: 0.03804583836457517  Acc: 99.82209158415841\n",
            "150/391  Loss: 0.038463040729922966  Acc: 99.79304635761589\n",
            "200/391  Loss: 0.03857268965733585  Acc: 99.77456467661692\n",
            "250/391  Loss: 0.03870996793516841  Acc: 99.77900896414343\n",
            "300/391  Loss: 0.038532410887537207  Acc: 99.79495431893687\n",
            "350/391  Loss: 0.0386965103086583  Acc: 99.77964743589743\n",
            "0/79  Loss: 1.4465641975402832  Acc: 64.84375\n",
            "50/79  Loss: 1.6137430621128457  Acc: 62.11703431372549\n",
            "Train Accuracy:   99.78 %    Test Accuracy:   62.35 %\n",
            "\n",
            "Epoch: 179\n",
            "0/391  Loss: 0.04934503138065338  Acc: 99.21875\n",
            "50/391  Loss: 0.03325137685911328  Acc: 99.83149509803921\n",
            "100/391  Loss: 0.03470643587631754  Acc: 99.82209158415841\n",
            "150/391  Loss: 0.034298792247049854  Acc: 99.85513245033113\n",
            "200/391  Loss: 0.03529581625885631  Acc: 99.8017723880597\n",
            "250/391  Loss: 0.036114544277469  Acc: 99.78212151394422\n",
            "300/391  Loss: 0.036402325916329886  Acc: 99.77419019933555\n",
            "350/391  Loss: 0.03682596183400548  Acc: 99.76406695156695\n",
            "0/79  Loss: 1.557082176208496  Acc: 64.84375\n",
            "50/79  Loss: 1.6039004138871735  Acc: 62.17830882352941\n",
            "Saving..\n",
            "Train Accuracy:   99.78 %    Test Accuracy:   62.48 %\n",
            "\n",
            "Epoch: 180\n",
            "0/391  Loss: 0.03095601685345173  Acc: 100.0\n",
            "50/391  Loss: 0.034601651278196595  Acc: 99.87745098039215\n",
            "100/391  Loss: 0.03468675258578641  Acc: 99.82982673267327\n",
            "150/391  Loss: 0.035013183963713265  Acc: 99.78787251655629\n",
            "200/391  Loss: 0.03550519120522696  Acc: 99.75901741293532\n",
            "250/391  Loss: 0.03564384435693106  Acc: 99.76033366533865\n",
            "300/391  Loss: 0.03548307197657336  Acc: 99.76899916943522\n",
            "350/391  Loss: 0.03603145604332288  Acc: 99.7707443019943\n",
            "0/79  Loss: 1.4671250581741333  Acc: 64.84375\n",
            "50/79  Loss: 1.6074270290486954  Acc: 62.408088235294116\n",
            "Saving..\n",
            "Train Accuracy:   99.78 %    Test Accuracy:   62.57 %\n",
            "\n",
            "Epoch: 181\n",
            "0/391  Loss: 0.03125527873635292  Acc: 100.0\n",
            "50/391  Loss: 0.03345947438741431  Acc: 99.83149509803921\n",
            "100/391  Loss: 0.033191595484714696  Acc: 99.82209158415841\n",
            "150/391  Loss: 0.032852435176143585  Acc: 99.82926324503312\n",
            "200/391  Loss: 0.03305773102488387  Acc: 99.8212064676617\n",
            "250/391  Loss: 0.03337367506052156  Acc: 99.81635956175299\n",
            "300/391  Loss: 0.0338022754871073  Acc: 99.81312292358804\n",
            "350/391  Loss: 0.033888797310215456  Acc: 99.81971153846153\n",
            "0/79  Loss: 1.5611625909805298  Acc: 64.0625\n",
            "50/79  Loss: 1.6013050219591927  Acc: 62.5765931372549\n",
            "Saving..\n",
            "Train Accuracy:   99.82 %    Test Accuracy:   62.64 %\n",
            "\n",
            "Epoch: 182\n",
            "0/391  Loss: 0.02417486160993576  Acc: 100.0\n",
            "50/391  Loss: 0.03180035601790045  Acc: 99.75490196078431\n",
            "100/391  Loss: 0.03191579601699763  Acc: 99.78341584158416\n",
            "150/391  Loss: 0.032291115822874944  Acc: 99.78269867549669\n",
            "200/391  Loss: 0.032712801975842136  Acc: 99.78233830845771\n",
            "250/391  Loss: 0.03252141412154612  Acc: 99.79768426294821\n",
            "300/391  Loss: 0.03275198283360844  Acc: 99.78976328903654\n",
            "350/391  Loss: 0.032946444405746934  Acc: 99.784099002849\n",
            "0/79  Loss: 1.5236608982086182  Acc: 65.625\n",
            "50/79  Loss: 1.6011099090763168  Acc: 62.56127450980392\n",
            "Saving..\n",
            "Train Accuracy:   99.82 %    Test Accuracy:   62.83 %\n",
            "\n",
            "Epoch: 183\n",
            "0/391  Loss: 0.028753554448485374  Acc: 100.0\n",
            "50/391  Loss: 0.029563742488914847  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.029751753858705557  Acc: 99.89170792079207\n",
            "150/391  Loss: 0.02953718402823865  Acc: 99.89134933774834\n",
            "200/391  Loss: 0.029317255590611428  Acc: 99.89894278606965\n",
            "250/391  Loss: 0.029898670861920513  Acc: 99.88794820717132\n",
            "300/391  Loss: 0.030271853787508914  Acc: 99.87541528239203\n",
            "350/391  Loss: 0.030479057015389457  Acc: 99.87535612535612\n",
            "0/79  Loss: 1.5328776836395264  Acc: 63.28125\n",
            "50/79  Loss: 1.591344209278331  Acc: 62.607230392156865\n",
            "Train Accuracy:   99.87 %    Test Accuracy:   62.83 %\n",
            "\n",
            "Epoch: 184\n",
            "0/391  Loss: 0.043978627771139145  Acc: 99.21875\n",
            "50/391  Loss: 0.029935728393348994  Acc: 99.87745098039215\n",
            "100/391  Loss: 0.02988220929251154  Acc: 99.88397277227723\n",
            "150/391  Loss: 0.03000151039724121  Acc: 99.86548013245033\n",
            "200/391  Loss: 0.030270068248298334  Acc: 99.85618781094527\n",
            "250/391  Loss: 0.03026046399472363  Acc: 99.85993525896414\n",
            "300/391  Loss: 0.030251899434012235  Acc: 99.84426910299004\n",
            "350/391  Loss: 0.030262219395881196  Acc: 99.83084045584046\n",
            "0/79  Loss: 1.542880892753601  Acc: 65.625\n",
            "50/79  Loss: 1.591947588266111  Acc: 62.622549019607845\n",
            "Saving..\n",
            "Train Accuracy:   99.87 %    Test Accuracy:   62.85 %\n",
            "\n",
            "Epoch: 185\n",
            "0/391  Loss: 0.023575248196721077  Acc: 100.0\n",
            "50/391  Loss: 0.028021031698467685  Acc: 99.87745098039215\n",
            "100/391  Loss: 0.028285537486766824  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.02817163932205036  Acc: 99.88617549668874\n",
            "200/391  Loss: 0.028623442531595777  Acc: 99.87950870646766\n",
            "250/391  Loss: 0.02860161792709533  Acc: 99.88483565737052\n",
            "300/391  Loss: 0.028687949239762124  Acc: 99.87541528239203\n",
            "350/391  Loss: 0.02879215897381985  Acc: 99.87090455840456\n",
            "0/79  Loss: 1.5142090320587158  Acc: 64.84375\n",
            "50/79  Loss: 1.590275759790458  Acc: 62.377450980392155\n",
            "Train Accuracy:   99.87 %    Test Accuracy:   62.85 %\n",
            "\n",
            "Epoch: 186\n",
            "0/391  Loss: 0.025256777182221413  Acc: 100.0\n",
            "50/391  Loss: 0.029484261996021457  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.028587497245188397  Acc: 99.89170792079207\n",
            "150/391  Loss: 0.028166073066065247  Acc: 99.89652317880795\n",
            "200/391  Loss: 0.027968812404913987  Acc: 99.90671641791045\n",
            "250/391  Loss: 0.028187157656715447  Acc: 99.9003984063745\n",
            "300/391  Loss: 0.028063478526903742  Acc: 99.89617940199335\n",
            "350/391  Loss: 0.02824784656906994  Acc: 99.8909366096866\n",
            "0/79  Loss: 1.5119948387145996  Acc: 64.0625\n",
            "50/79  Loss: 1.5955510700450224  Acc: 62.193627450980394\n",
            "Train Accuracy:   99.898 %    Test Accuracy:   62.85 %\n",
            "\n",
            "Epoch: 187\n",
            "0/391  Loss: 0.023386528715491295  Acc: 100.0\n",
            "50/391  Loss: 0.025190499191190683  Acc: 99.86213235294117\n",
            "100/391  Loss: 0.026202147374722626  Acc: 99.86076732673267\n",
            "150/391  Loss: 0.026327609084546566  Acc: 99.87582781456953\n",
            "200/391  Loss: 0.026519355540214784  Acc: 99.86396144278606\n",
            "250/391  Loss: 0.026469308389462085  Acc: 99.87549800796813\n",
            "300/391  Loss: 0.02650820311047508  Acc: 99.88839285714286\n",
            "350/391  Loss: 0.026689470658062868  Acc: 99.88648504273505\n",
            "0/79  Loss: 1.506020426750183  Acc: 64.0625\n",
            "50/79  Loss: 1.5968348395590688  Acc: 62.5\n",
            "Train Accuracy:   99.898 %    Test Accuracy:   62.85 %\n",
            "\n",
            "Epoch: 188\n",
            "0/391  Loss: 0.02289160154759884  Acc: 100.0\n",
            "50/391  Loss: 0.024858890028268684  Acc: 99.96936274509804\n",
            "100/391  Loss: 0.025978154318388735  Acc: 99.93038366336634\n",
            "150/391  Loss: 0.026102377046269692  Acc: 99.91204470198676\n",
            "200/391  Loss: 0.02581108625589022  Acc: 99.91060323383084\n",
            "250/391  Loss: 0.025746877279236496  Acc: 99.90973605577689\n",
            "300/391  Loss: 0.025906797872850072  Acc: 99.90656146179403\n",
            "350/391  Loss: 0.026164987501277034  Acc: 99.90429131054131\n",
            "0/79  Loss: 1.5036487579345703  Acc: 65.625\n",
            "50/79  Loss: 1.5856455377503937  Acc: 62.34681372549019\n",
            "Train Accuracy:   99.898 %    Test Accuracy:   62.85 %\n",
            "\n",
            "Epoch: 189\n",
            "0/391  Loss: 0.03234470635652542  Acc: 99.21875\n",
            "50/391  Loss: 0.02471518859851594  Acc: 99.96936274509804\n",
            "100/391  Loss: 0.025412244943551498  Acc: 99.93038366336634\n",
            "150/391  Loss: 0.025361713342713994  Acc: 99.94826158940397\n",
            "200/391  Loss: 0.025428815110035204  Acc: 99.93781094527363\n",
            "250/391  Loss: 0.025834503027042783  Acc: 99.92529880478088\n",
            "300/391  Loss: 0.025695223321030504  Acc: 99.92732558139535\n",
            "350/391  Loss: 0.025793761060045144  Acc: 99.92654914529915\n",
            "0/79  Loss: 1.4694254398345947  Acc: 67.96875\n",
            "50/79  Loss: 1.5745668995614146  Acc: 62.5\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   62.85 %\n",
            "\n",
            "Epoch: 190\n",
            "0/391  Loss: 0.020543038845062256  Acc: 100.0\n",
            "50/391  Loss: 0.02615344907869311  Acc: 99.86213235294117\n",
            "100/391  Loss: 0.026202382722703536  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.025599300768843154  Acc: 99.91204470198676\n",
            "200/391  Loss: 0.025686815839761228  Acc: 99.91837686567165\n",
            "250/391  Loss: 0.025762785289184267  Acc: 99.91907370517929\n",
            "300/391  Loss: 0.025721361319166282  Acc: 99.91175249169436\n",
            "350/391  Loss: 0.025793234743390157  Acc: 99.90429131054131\n",
            "0/79  Loss: 1.4800249338150024  Acc: 64.84375\n",
            "50/79  Loss: 1.5889788071314495  Acc: 62.69914215686274\n",
            "Saving..\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   62.88 %\n",
            "\n",
            "Epoch: 191\n",
            "0/391  Loss: 0.02244103141129017  Acc: 100.0\n",
            "50/391  Loss: 0.024605154005043647  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.024793190070160544  Acc: 99.89170792079207\n",
            "150/391  Loss: 0.024920883495977382  Acc: 99.90687086092716\n",
            "200/391  Loss: 0.024792614624608512  Acc: 99.90671641791045\n",
            "250/391  Loss: 0.024995844792648853  Acc: 99.9066235059761\n",
            "300/391  Loss: 0.02505551152970902  Acc: 99.90396594684385\n",
            "350/391  Loss: 0.025107337220272107  Acc: 99.90651709401709\n",
            "0/79  Loss: 1.5056498050689697  Acc: 66.40625\n",
            "50/79  Loss: 1.5905244490679573  Acc: 62.2702205882353\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   62.88 %\n",
            "\n",
            "Epoch: 192\n",
            "0/391  Loss: 0.018577460199594498  Acc: 100.0\n",
            "50/391  Loss: 0.024092399296076858  Acc: 99.9234068627451\n",
            "100/391  Loss: 0.024183549069900914  Acc: 99.94585396039604\n",
            "150/391  Loss: 0.02424157336230902  Acc: 99.94308774834437\n",
            "200/391  Loss: 0.024176473826614777  Acc: 99.93781094527363\n",
            "250/391  Loss: 0.023993189439207196  Acc: 99.92529880478088\n",
            "300/391  Loss: 0.023906461383986513  Acc: 99.92213455149502\n",
            "350/391  Loss: 0.023916095840158285  Acc: 99.92432336182337\n",
            "0/79  Loss: 1.480798363685608  Acc: 66.40625\n",
            "50/79  Loss: 1.5867378851946663  Acc: 62.622549019607845\n",
            "Saving..\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   62.97 %\n",
            "\n",
            "Epoch: 193\n",
            "0/391  Loss: 0.026856273412704468  Acc: 100.0\n",
            "50/391  Loss: 0.02584665663102094  Acc: 99.86213235294117\n",
            "100/391  Loss: 0.02464874918655594  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.024641060375219938  Acc: 99.91204470198676\n",
            "200/391  Loss: 0.024502482849048143  Acc: 99.91060323383084\n",
            "250/391  Loss: 0.02407648399160559  Acc: 99.91596115537848\n",
            "300/391  Loss: 0.02431457063981465  Acc: 99.91953903654485\n",
            "350/391  Loss: 0.024136745819339046  Acc: 99.9198717948718\n",
            "0/79  Loss: 1.4639439582824707  Acc: 66.40625\n",
            "50/79  Loss: 1.5842851377000995  Acc: 62.775735294117645\n",
            "Saving..\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 194\n",
            "0/391  Loss: 0.019047986716032028  Acc: 100.0\n",
            "50/391  Loss: 0.02442569765901449  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.023978544758761874  Acc: 99.91491336633663\n",
            "150/391  Loss: 0.024191197285440977  Acc: 99.90687086092716\n",
            "200/391  Loss: 0.02383786192006288  Acc: 99.91060323383084\n",
            "250/391  Loss: 0.023949786011441297  Acc: 99.91596115537848\n",
            "300/391  Loss: 0.023872001740178  Acc: 99.91434800664452\n",
            "350/391  Loss: 0.023949266042549724  Acc: 99.90874287749288\n",
            "0/79  Loss: 1.507261037826538  Acc: 66.40625\n",
            "50/79  Loss: 1.5879268926732681  Acc: 62.637867647058826\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 195\n",
            "0/391  Loss: 0.019407091662287712  Acc: 100.0\n",
            "50/391  Loss: 0.023812147240866634  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.02340797517485548  Acc: 99.93038366336634\n",
            "150/391  Loss: 0.023520615476103412  Acc: 99.91721854304636\n",
            "200/391  Loss: 0.02343176027287298  Acc: 99.91449004975124\n",
            "250/391  Loss: 0.023409952572825184  Acc: 99.9128486055777\n",
            "300/391  Loss: 0.02337760861616495  Acc: 99.91694352159469\n",
            "350/391  Loss: 0.023569082536142944  Acc: 99.90874287749288\n",
            "0/79  Loss: 1.4984874725341797  Acc: 67.96875\n",
            "50/79  Loss: 1.5870356606502158  Acc: 62.66850490196079\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 196\n",
            "0/391  Loss: 0.02872239425778389  Acc: 100.0\n",
            "50/391  Loss: 0.02353968075020056  Acc: 99.95404411764706\n",
            "100/391  Loss: 0.022953807717502706  Acc: 99.95358910891089\n",
            "150/391  Loss: 0.023288359483148877  Acc: 99.92756622516556\n",
            "200/391  Loss: 0.02337514468241687  Acc: 99.91837686567165\n",
            "250/391  Loss: 0.02343633644416394  Acc: 99.91907370517929\n",
            "300/391  Loss: 0.023434900876716323  Acc: 99.92992109634551\n",
            "350/391  Loss: 0.023486800372409515  Acc: 99.92432336182337\n",
            "0/79  Loss: 1.5197982788085938  Acc: 65.625\n",
            "50/79  Loss: 1.585286724801157  Acc: 62.82169117647059\n",
            "Train Accuracy:   99.924 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 197\n",
            "0/391  Loss: 0.028302086517214775  Acc: 100.0\n",
            "50/391  Loss: 0.02355321403592825  Acc: 99.9234068627451\n",
            "100/391  Loss: 0.023296621812525953  Acc: 99.93811881188118\n",
            "150/391  Loss: 0.023185225666210747  Acc: 99.94826158940397\n",
            "200/391  Loss: 0.023418977444259383  Acc: 99.94558457711443\n",
            "250/391  Loss: 0.023477398593765332  Acc: 99.94708665338645\n",
            "300/391  Loss: 0.023400473015450955  Acc: 99.95068521594685\n",
            "350/391  Loss: 0.023566801374263403  Acc: 99.93990384615384\n",
            "0/79  Loss: 1.500993013381958  Acc: 66.40625\n",
            "50/79  Loss: 1.5805851711946375  Acc: 62.54595588235294\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 198\n",
            "0/391  Loss: 0.03533129766583443  Acc: 99.21875\n",
            "50/391  Loss: 0.02365388348698616  Acc: 99.86213235294117\n",
            "100/391  Loss: 0.02403497805652937  Acc: 99.85303217821782\n",
            "150/391  Loss: 0.023651362347435083  Acc: 99.86548013245033\n",
            "200/391  Loss: 0.023114986231189166  Acc: 99.88728233830845\n",
            "250/391  Loss: 0.02322585900421755  Acc: 99.89106075697211\n",
            "300/391  Loss: 0.02328156387476727  Acc: 99.89617940199335\n",
            "350/391  Loss: 0.02334258833533933  Acc: 99.90206552706553\n",
            "0/79  Loss: 1.4993001222610474  Acc: 66.40625\n",
            "50/79  Loss: 1.5766936236736822  Acc: 62.68382352941177\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 199\n",
            "0/391  Loss: 0.019726376980543137  Acc: 100.0\n",
            "50/391  Loss: 0.02370326798043999  Acc: 99.95404411764706\n",
            "100/391  Loss: 0.023586179120558323  Acc: 99.93811881188118\n",
            "150/391  Loss: 0.02357509166668385  Acc: 99.93274006622516\n",
            "200/391  Loss: 0.023459802279758513  Acc: 99.93392412935323\n",
            "250/391  Loss: 0.02325026506179595  Acc: 99.93463645418326\n",
            "300/391  Loss: 0.02336622338952416  Acc: 99.93251661129568\n",
            "350/391  Loss: 0.023339259032255564  Acc: 99.93100071225071\n",
            "0/79  Loss: 1.512749433517456  Acc: 65.625\n",
            "50/79  Loss: 1.5844557285308838  Acc: 62.5\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 200\n",
            "0/391  Loss: 0.01663089729845524  Acc: 100.0\n",
            "50/391  Loss: 0.023467549871580274  Acc: 99.96936274509804\n",
            "100/391  Loss: 0.023505802413835975  Acc: 99.93811881188118\n",
            "150/391  Loss: 0.023193533877712606  Acc: 99.93274006622516\n",
            "200/391  Loss: 0.02319849430310044  Acc: 99.93003731343283\n",
            "250/391  Loss: 0.023096331614125297  Acc: 99.92841135458167\n",
            "300/391  Loss: 0.023035053491740922  Acc: 99.92473006644518\n",
            "350/391  Loss: 0.02315302529608422  Acc: 99.92654914529915\n",
            "0/79  Loss: 1.4937745332717896  Acc: 65.625\n",
            "50/79  Loss: 1.5878314574559529  Acc: 62.745098039215684\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 201\n",
            "0/391  Loss: 0.02395184151828289  Acc: 100.0\n",
            "50/391  Loss: 0.02163671997978407  Acc: 99.93872549019608\n",
            "100/391  Loss: 0.02229667352604689  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.022554025935958948  Acc: 99.91721854304636\n",
            "200/391  Loss: 0.022776814895816407  Acc: 99.91060323383084\n",
            "250/391  Loss: 0.0231542314228368  Acc: 99.9066235059761\n",
            "300/391  Loss: 0.02296816872624662  Acc: 99.91694352159469\n",
            "350/391  Loss: 0.023010755164755717  Acc: 99.91764601139602\n",
            "0/79  Loss: 1.5081102848052979  Acc: 65.625\n",
            "50/79  Loss: 1.58947739647884  Acc: 62.5765931372549\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.03 %\n",
            "\n",
            "Epoch: 202\n",
            "0/391  Loss: 0.02402176335453987  Acc: 100.0\n",
            "50/391  Loss: 0.022333833735947516  Acc: 99.93872549019608\n",
            "100/391  Loss: 0.02348184897241616  Acc: 99.90717821782178\n",
            "150/391  Loss: 0.02341627940806924  Acc: 99.89652317880795\n",
            "200/391  Loss: 0.02332286188721805  Acc: 99.89894278606965\n",
            "250/391  Loss: 0.02312148723795832  Acc: 99.9066235059761\n",
            "300/391  Loss: 0.023280558315572946  Acc: 99.90656146179403\n",
            "350/391  Loss: 0.023145388124611473  Acc: 99.91542022792022\n",
            "0/79  Loss: 1.4826970100402832  Acc: 67.1875\n",
            "50/79  Loss: 1.5823598469004911  Acc: 63.036151960784316\n",
            "Saving..\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 203\n",
            "0/391  Loss: 0.02041076309978962  Acc: 100.0\n",
            "50/391  Loss: 0.024690893990005933  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.023683697224991158  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.02336010042676665  Acc: 99.91204470198676\n",
            "200/391  Loss: 0.02377004322331788  Acc: 99.90671641791045\n",
            "250/391  Loss: 0.023856022328405505  Acc: 99.8972858565737\n",
            "300/391  Loss: 0.023981871773361963  Acc: 99.90656146179403\n",
            "350/391  Loss: 0.023921726122923048  Acc: 99.90429131054131\n",
            "0/79  Loss: 1.520202398300171  Acc: 67.1875\n",
            "50/79  Loss: 1.5786492450564515  Acc: 62.622549019607845\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 204\n",
            "0/391  Loss: 0.0295330248773098  Acc: 100.0\n",
            "50/391  Loss: 0.024088221547358176  Acc: 99.86213235294117\n",
            "100/391  Loss: 0.02352729539322381  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.02292803165484343  Acc: 99.91204470198676\n",
            "200/391  Loss: 0.022773583637392937  Acc: 99.91449004975124\n",
            "250/391  Loss: 0.0232270808883041  Acc: 99.9035109561753\n",
            "300/391  Loss: 0.02308701523316659  Acc: 99.91434800664452\n",
            "350/391  Loss: 0.023239490557515043  Acc: 99.91542022792022\n",
            "0/79  Loss: 1.5013059377670288  Acc: 65.625\n",
            "50/79  Loss: 1.590525608436734  Acc: 62.377450980392155\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 205\n",
            "0/391  Loss: 0.020620983093976974  Acc: 100.0\n",
            "50/391  Loss: 0.024080719871848236  Acc: 99.93872549019608\n",
            "100/391  Loss: 0.024064030313845908  Acc: 99.91491336633663\n",
            "150/391  Loss: 0.02384997961043522  Acc: 99.91721854304636\n",
            "200/391  Loss: 0.024129714209827322  Acc: 99.92226368159204\n",
            "250/391  Loss: 0.024098253204112508  Acc: 99.91907370517929\n",
            "300/391  Loss: 0.024097701766685988  Acc: 99.91694352159469\n",
            "350/391  Loss: 0.024032977919102225  Acc: 99.91542022792022\n",
            "0/79  Loss: 1.5202476978302002  Acc: 66.40625\n",
            "50/79  Loss: 1.5790323135899562  Acc: 62.760416666666664\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 206\n",
            "0/391  Loss: 0.02151539735496044  Acc: 100.0\n",
            "50/391  Loss: 0.02271865549332955  Acc: 99.96936274509804\n",
            "100/391  Loss: 0.022799336858610117  Acc: 99.94585396039604\n",
            "150/391  Loss: 0.023147937934228917  Acc: 99.92756622516556\n",
            "200/391  Loss: 0.02285201652948536  Acc: 99.94169776119404\n",
            "250/391  Loss: 0.02324035938502664  Acc: 99.92529880478088\n",
            "300/391  Loss: 0.02326653992751013  Acc: 99.92732558139535\n",
            "350/391  Loss: 0.023274507803412583  Acc: 99.92877492877493\n",
            "0/79  Loss: 1.5023460388183594  Acc: 64.84375\n",
            "50/79  Loss: 1.5791005854513132  Acc: 62.69914215686274\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 207\n",
            "0/391  Loss: 0.021924123167991638  Acc: 100.0\n",
            "50/391  Loss: 0.023690975088553102  Acc: 99.93872549019608\n",
            "100/391  Loss: 0.023763796720321816  Acc: 99.89944306930693\n",
            "150/391  Loss: 0.023178758149016772  Acc: 99.92239238410596\n",
            "200/391  Loss: 0.022991044525594556  Acc: 99.93003731343283\n",
            "250/391  Loss: 0.023159692275423215  Acc: 99.93152390438247\n",
            "300/391  Loss: 0.02324815456480085  Acc: 99.92732558139535\n",
            "350/391  Loss: 0.023399257220518895  Acc: 99.92209757834758\n",
            "0/79  Loss: 1.4608306884765625  Acc: 67.1875\n",
            "50/79  Loss: 1.585324902160495  Acc: 62.377450980392155\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 208\n",
            "0/391  Loss: 0.020016001537442207  Acc: 100.0\n",
            "50/391  Loss: 0.023122254886901845  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.023486397468881443  Acc: 99.90717821782178\n",
            "150/391  Loss: 0.023275865229549785  Acc: 99.92239238410596\n",
            "200/391  Loss: 0.02333496573997374  Acc: 99.91837686567165\n",
            "250/391  Loss: 0.023735371377482356  Acc: 99.9066235059761\n",
            "300/391  Loss: 0.023700619305165305  Acc: 99.90656146179403\n",
            "350/391  Loss: 0.023661938283666757  Acc: 99.91542022792022\n",
            "0/79  Loss: 1.529276728630066  Acc: 67.1875\n",
            "50/79  Loss: 1.5901681488635493  Acc: 62.65318627450981\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 209\n",
            "0/391  Loss: 0.018397744745016098  Acc: 100.0\n",
            "50/391  Loss: 0.023919190517535396  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.023437575243487215  Acc: 99.93811881188118\n",
            "150/391  Loss: 0.02319130694974732  Acc: 99.94826158940397\n",
            "200/391  Loss: 0.023341553540557475  Acc: 99.94947139303483\n",
            "250/391  Loss: 0.02350583651744987  Acc: 99.93774900398407\n",
            "300/391  Loss: 0.023429477395508377  Acc: 99.93511212624584\n",
            "350/391  Loss: 0.023473934952350083  Acc: 99.9332264957265\n",
            "0/79  Loss: 1.4859262704849243  Acc: 65.625\n",
            "50/79  Loss: 1.579574652746612  Acc: 62.48468137254902\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 210\n",
            "0/391  Loss: 0.01229057926684618  Acc: 100.0\n",
            "50/391  Loss: 0.024456580626029595  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.02409110064295554  Acc: 99.90717821782178\n",
            "150/391  Loss: 0.02355033935152537  Acc: 99.92239238410596\n",
            "200/391  Loss: 0.023365562341514216  Acc: 99.92226368159204\n",
            "250/391  Loss: 0.023529930492915005  Acc: 99.9066235059761\n",
            "300/391  Loss: 0.023450333532255355  Acc: 99.90396594684385\n",
            "350/391  Loss: 0.0234848467335954  Acc: 99.90206552706553\n",
            "0/79  Loss: 1.4982829093933105  Acc: 67.96875\n",
            "50/79  Loss: 1.58700739402397  Acc: 62.46936274509804\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 211\n",
            "0/391  Loss: 0.023572396486997604  Acc: 100.0\n",
            "50/391  Loss: 0.023861875337566816  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.023229600197755464  Acc: 99.91491336633663\n",
            "150/391  Loss: 0.023003354767300435  Acc: 99.92756622516556\n",
            "200/391  Loss: 0.02351604697679109  Acc: 99.92615049751244\n",
            "250/391  Loss: 0.023572093087333844  Acc: 99.91907370517929\n",
            "300/391  Loss: 0.023708868360276834  Acc: 99.90915697674419\n",
            "350/391  Loss: 0.024075391230059317  Acc: 99.90429131054131\n",
            "0/79  Loss: 1.5132758617401123  Acc: 64.84375\n",
            "50/79  Loss: 1.5778375887403302  Acc: 62.69914215686274\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 212\n",
            "0/391  Loss: 0.0198777187615633  Acc: 100.0\n",
            "50/391  Loss: 0.023845666672523115  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.02321215544707409  Acc: 99.93038366336634\n",
            "150/391  Loss: 0.02366862916665164  Acc: 99.92239238410596\n",
            "200/391  Loss: 0.023764808356428323  Acc: 99.91060323383084\n",
            "250/391  Loss: 0.023447894016317875  Acc: 99.91596115537848\n",
            "300/391  Loss: 0.023753859198642927  Acc: 99.91953903654485\n",
            "350/391  Loss: 0.02369056917737607  Acc: 99.92209757834758\n",
            "0/79  Loss: 1.5133132934570312  Acc: 67.96875\n",
            "50/79  Loss: 1.5913875476986754  Acc: 62.5\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 213\n",
            "0/391  Loss: 0.018623946234583855  Acc: 100.0\n",
            "50/391  Loss: 0.022390177181246234  Acc: 99.95404411764706\n",
            "100/391  Loss: 0.023253668219515  Acc: 99.93038366336634\n",
            "150/391  Loss: 0.022846559216792614  Acc: 99.94308774834437\n",
            "200/391  Loss: 0.022741203339983574  Acc: 99.93003731343283\n",
            "250/391  Loss: 0.02283985372187488  Acc: 99.93463645418326\n",
            "300/391  Loss: 0.0232301450974967  Acc: 99.92992109634551\n",
            "350/391  Loss: 0.023344753440098037  Acc: 99.92432336182337\n",
            "0/79  Loss: 1.5040562152862549  Acc: 66.40625\n",
            "50/79  Loss: 1.5837160068399765  Acc: 62.7297794117647\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 214\n",
            "0/391  Loss: 0.027108659967780113  Acc: 100.0\n",
            "50/391  Loss: 0.02289324650066156  Acc: 99.93872549019608\n",
            "100/391  Loss: 0.022896838973801913  Acc: 99.92264851485149\n",
            "150/391  Loss: 0.023406819027188597  Acc: 99.92239238410596\n",
            "200/391  Loss: 0.023305981582151122  Acc: 99.93003731343283\n",
            "250/391  Loss: 0.0233870849414057  Acc: 99.93463645418326\n",
            "300/391  Loss: 0.023370772126777427  Acc: 99.94030315614619\n",
            "350/391  Loss: 0.023657904407195215  Acc: 99.92654914529915\n",
            "0/79  Loss: 1.492788314819336  Acc: 65.625\n",
            "50/79  Loss: 1.586062513145746  Acc: 62.392769607843135\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 215\n",
            "0/391  Loss: 0.027470890432596207  Acc: 100.0\n",
            "50/391  Loss: 0.024430724737398764  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.024608519629086598  Acc: 99.87623762376238\n",
            "150/391  Loss: 0.024506563259078965  Acc: 99.89134933774834\n",
            "200/391  Loss: 0.02428479871680191  Acc: 99.90671641791045\n",
            "250/391  Loss: 0.02415401368324025  Acc: 99.9035109561753\n",
            "300/391  Loss: 0.02410962047098681  Acc: 99.91175249169436\n",
            "350/391  Loss: 0.02411227327827205  Acc: 99.91319444444444\n",
            "0/79  Loss: 1.4966065883636475  Acc: 65.625\n",
            "50/79  Loss: 1.5814119530659096  Acc: 62.54595588235294\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 216\n",
            "0/391  Loss: 0.027871329337358475  Acc: 100.0\n",
            "50/391  Loss: 0.02351673940817515  Acc: 99.9234068627451\n",
            "100/391  Loss: 0.02415841821544241  Acc: 99.91491336633663\n",
            "150/391  Loss: 0.02448600840242888  Acc: 99.91721854304636\n",
            "200/391  Loss: 0.024396857459895054  Acc: 99.92226368159204\n",
            "250/391  Loss: 0.024824666668219395  Acc: 99.90973605577689\n",
            "300/391  Loss: 0.0250664433043611  Acc: 99.89358388704319\n",
            "350/391  Loss: 0.02535069584740363  Acc: 99.8909366096866\n",
            "0/79  Loss: 1.5077693462371826  Acc: 64.84375\n",
            "50/79  Loss: 1.5854907105950748  Acc: 62.45404411764706\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 217\n",
            "0/391  Loss: 0.017764335498213768  Acc: 100.0\n",
            "50/391  Loss: 0.023865546976380488  Acc: 99.9234068627451\n",
            "100/391  Loss: 0.024276313335072287  Acc: 99.91491336633663\n",
            "150/391  Loss: 0.02460921471260044  Acc: 99.92239238410596\n",
            "200/391  Loss: 0.024571588462153775  Acc: 99.92615049751244\n",
            "250/391  Loss: 0.024793365402555324  Acc: 99.92529880478088\n",
            "300/391  Loss: 0.02504609714252905  Acc: 99.91694352159469\n",
            "350/391  Loss: 0.02513597406542454  Acc: 99.92209757834758\n",
            "0/79  Loss: 1.484861135482788  Acc: 65.625\n",
            "50/79  Loss: 1.5939688425438077  Acc: 62.82169117647059\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 218\n",
            "0/391  Loss: 0.020005403086543083  Acc: 100.0\n",
            "50/391  Loss: 0.02512603099731838  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.025091047481735154  Acc: 99.89170792079207\n",
            "150/391  Loss: 0.0259134593043501  Acc: 99.88617549668874\n",
            "200/391  Loss: 0.02612717386650209  Acc: 99.88339552238806\n",
            "250/391  Loss: 0.02591108152294064  Acc: 99.9035109561753\n",
            "300/391  Loss: 0.02604530948180198  Acc: 99.90137043189368\n",
            "350/391  Loss: 0.02612780547963503  Acc: 99.89761396011396\n",
            "0/79  Loss: 1.491241455078125  Acc: 64.0625\n",
            "50/79  Loss: 1.5868392201030956  Acc: 62.7297794117647\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 219\n",
            "0/391  Loss: 0.021841323003172874  Acc: 100.0\n",
            "50/391  Loss: 0.026611891337761692  Acc: 99.83149509803921\n",
            "100/391  Loss: 0.02695099295734769  Acc: 99.84529702970298\n",
            "150/391  Loss: 0.026895123577078447  Acc: 99.87065397350993\n",
            "200/391  Loss: 0.027719318866729736  Acc: 99.86007462686567\n",
            "250/391  Loss: 0.027720837345517488  Acc: 99.86304780876495\n",
            "300/391  Loss: 0.02794383965630666  Acc: 99.8546511627907\n",
            "350/391  Loss: 0.027851408156446922  Acc: 99.85754985754986\n",
            "0/79  Loss: 1.5146862268447876  Acc: 64.0625\n",
            "50/79  Loss: 1.5844974354201673  Acc: 62.5\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 220\n",
            "0/391  Loss: 0.0378020815551281  Acc: 99.21875\n",
            "50/391  Loss: 0.02723687920061981  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.0275565421042761  Acc: 99.88397277227723\n",
            "150/391  Loss: 0.027689157830958336  Acc: 99.89652317880795\n",
            "200/391  Loss: 0.027596880501108385  Acc: 99.89894278606965\n",
            "250/391  Loss: 0.02756050853912099  Acc: 99.9003984063745\n",
            "300/391  Loss: 0.027920188673112876  Acc: 99.90137043189368\n",
            "350/391  Loss: 0.028259132593403177  Acc: 99.90206552706553\n",
            "0/79  Loss: 1.5034887790679932  Acc: 64.84375\n",
            "50/79  Loss: 1.5903612164890064  Acc: 62.5765931372549\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 221\n",
            "0/391  Loss: 0.02442057617008686  Acc: 100.0\n",
            "50/391  Loss: 0.02839003769936515  Acc: 99.83149509803921\n",
            "100/391  Loss: 0.027906706169395164  Acc: 99.82982673267327\n",
            "150/391  Loss: 0.028252429149995578  Acc: 99.82408940397352\n",
            "200/391  Loss: 0.028570452593822977  Acc: 99.83286691542288\n",
            "250/391  Loss: 0.028561605622925132  Acc: 99.84748505976096\n",
            "300/391  Loss: 0.028656714648453895  Acc: 99.8468646179402\n",
            "350/391  Loss: 0.028943652626753195  Acc: 99.85977564102564\n",
            "0/79  Loss: 1.5521609783172607  Acc: 64.0625\n",
            "50/79  Loss: 1.5964432393803316  Acc: 62.43872549019608\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 222\n",
            "0/391  Loss: 0.02416348084807396  Acc: 100.0\n",
            "50/391  Loss: 0.027458862894598174  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.02876161883520608  Acc: 99.86076732673267\n",
            "150/391  Loss: 0.029193984677657388  Acc: 99.84478476821192\n",
            "200/391  Loss: 0.029637683803836506  Acc: 99.86396144278606\n",
            "250/391  Loss: 0.02980110879646117  Acc: 99.86616035856574\n",
            "300/391  Loss: 0.030145135865952088  Acc: 99.86762873754152\n",
            "350/391  Loss: 0.030596039706358203  Acc: 99.8530982905983\n",
            "0/79  Loss: 1.472975730895996  Acc: 66.40625\n",
            "50/79  Loss: 1.5895737456340415  Acc: 62.34681372549019\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 223\n",
            "0/391  Loss: 0.02921871654689312  Acc: 100.0\n",
            "50/391  Loss: 0.028889696387683645  Acc: 99.89276960784314\n",
            "100/391  Loss: 0.02865972362532474  Acc: 99.92264851485149\n",
            "150/391  Loss: 0.02937208382499139  Acc: 99.91721854304636\n",
            "200/391  Loss: 0.03035954991123866  Acc: 99.88728233830845\n",
            "250/391  Loss: 0.031067950354213735  Acc: 99.86304780876495\n",
            "300/391  Loss: 0.031401818587237416  Acc: 99.85984219269103\n",
            "350/391  Loss: 0.03170466016012209  Acc: 99.85532407407408\n",
            "0/79  Loss: 1.5197665691375732  Acc: 65.625\n",
            "50/79  Loss: 1.6089727177339441  Acc: 61.73406862745098\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n",
            "\n",
            "Epoch: 224\n",
            "0/391  Loss: 0.021079959347844124  Acc: 100.0\n",
            "50/391  Loss: 0.031768158005148754  Acc: 99.90808823529412\n",
            "100/391  Loss: 0.03256171336858579  Acc: 99.8066212871287\n",
            "150/391  Loss: 0.03312075055444872  Acc: 99.8137417218543\n",
            "200/391  Loss: 0.03299833934253721  Acc: 99.8212064676617\n",
            "250/391  Loss: 0.033564201723116326  Acc: 99.81324701195219\n",
            "300/391  Loss: 0.03424794644587658  Acc: 99.79495431893687\n",
            "350/391  Loss: 0.03450761351235572  Acc: 99.79077635327636\n",
            "0/79  Loss: 1.528548240661621  Acc: 62.5\n",
            "50/79  Loss: 1.5966583817612892  Acc: 61.994485294117645\n",
            "Train Accuracy:   99.938 %    Test Accuracy:   63.17 %\n"
          ]
        }
      ],
      "metadata": {
        "id": "-3YceOT-gbYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae355c08-785b-44ac-83b5-22d700e74c51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "torch.save(net.state_dict(), 'ckpt_final.pth')"
      ],
      "outputs": [],
      "metadata": {
        "id": "nMHFkqjcDD39"
      }
    }
  ]
}